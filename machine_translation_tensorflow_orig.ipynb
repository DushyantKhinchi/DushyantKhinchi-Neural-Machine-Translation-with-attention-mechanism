{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "machine_translation_tensorflow_orig.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DushyantKhinchi/DushyantKhinchi-Neural-Machine-Translation-with-attention-mechanism/blob/master/machine_translation_tensorflow_orig.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En0l2TgqVWTH",
        "colab_type": "text"
      },
      "source": [
        "# Neural machine translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "su00atkqqzjZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "6d0c1245-f4ed-41a0-aa2a-cb9fd53d23ff"
      },
      "source": [
        "!pip install -q tensorflow==2.0.0-beta1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 87.9MB 55kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.1MB 38.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 501kB 35.9MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IF3LjvXqdJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import unicodedata\n",
        "import string\n",
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUoGvxNddFHh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "a09cb2c6-3b08-45f7-ed02-7537d6615ddd"
      },
      "source": [
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44g54wKWqdJp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81da7ae2-a8a4-4a8a-b668-fb58e4d6b777"
      },
      "source": [
        "keras = tf.keras\n",
        "print(tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEed-KleNdUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !wget https://www.manythings.org/anki/fra-eng.zip\n",
        "# !unzip  fra-eng.zip"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHgfCZXVqdJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vMM4YwkwtKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"gdrive/My Drive/Machine Translation\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mYtoNDOmrC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "language=\"fra.txt\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npJpxCKh6uU6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "419f3971-1e18-4615-9c22-e17468a5b14f"
      },
      "source": [
        "#counting number of lines\n",
        "\n",
        "file = open(language, \"r\")\n",
        "number_of_lines = 0\n",
        "\n",
        "for line in file:\n",
        "  line = line.strip(\"\\n\")\n",
        "  number_of_lines += 1\n",
        "  \n",
        "file.close()\n",
        "\n",
        "print(\"lines:\", number_of_lines)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lines: 175623\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqyPjc3iwobG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "492212b3-d649-4b4d-94fa-3aede9aea2c4"
      },
      "source": [
        "with open(language,'r') as f:\n",
        "        lines = f.readlines()\n",
        "for i in range(0,10):\n",
        "        print(lines[i])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go.\tVa !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)\n",
            "\n",
            "Hi.\tSalut !\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)\n",
            "\n",
            "Hi.\tSalut.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux)\n",
            "\n",
            "Run!\tCours !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906331 (sacredceltic)\n",
            "\n",
            "Run!\tCourez !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906332 (sacredceltic)\n",
            "\n",
            "Who?\tQui ?\tCC-BY 2.0 (France) Attribution: tatoeba.org #2083030 (CK) & #4366796 (gillux)\n",
            "\n",
            "Wow!\tÇa alors !\tCC-BY 2.0 (France) Attribution: tatoeba.org #52027 (Zifre) & #374631 (zmoo)\n",
            "\n",
            "Fire!\tAu feu !\tCC-BY 2.0 (France) Attribution: tatoeba.org #1829639 (Spamster) & #4627939 (sacredceltic)\n",
            "\n",
            "Help!\tÀ l'aide !\tCC-BY 2.0 (France) Attribution: tatoeba.org #435084 (lukaszpp) & #128430 (sysko)\n",
            "\n",
            "Jump.\tSaute.\tCC-BY 2.0 (France) Attribution: tatoeba.org #631038 (Shishir) & #2416938 (Phoenix)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJav2p1yzNJC",
        "colab_type": "text"
      },
      "source": [
        "This class below will map words in a particular language to the index.\n",
        "This class has three dictionaries data structures.\n",
        "\n",
        "word2int: to map each word to a unique index integer\n",
        "int2word: to map index integer to the word\n",
        "word2count: to map a word to its total count in the corpus \n",
        "\n",
        "We have three functions in our class\n",
        "\n",
        "\n",
        "1)  ***addWord()*** function just adds a word as a key to the word2int dictionary with its corresponding index value. The opposite is done for the int2word dictionary. It also keep track of how many times we’ve come across a word when parsing our corpus to populate the class dictionaries and if we’ve already come across  a particular word, we desist from adding it to the word2int and int2word dictionaries and instead keep track of how many times it appears in our corpus using the word2count dictionary.\n",
        "\n",
        "\n",
        "\n",
        "2) ***addSentence()*** does is to simply iterate through each sentence and for each sentence, it splits the sentence into words and implements the addWord function on each word in each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSK6zPRUqdJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lang(object):\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2int = {}#map words to integers\n",
        "        self.word2count = {} #map words to their count in the corpus/ how many times a word is repeated in the corpus?\n",
        "        self.int2word = {0 : \"SOS\", 1 : \"EOS\"} # map integers to tokens. just the opposite of word2int but has some initial values. EOS means End of Sentence token \n",
        "                                               # used to indicate the end of a sentence. Every sentence is going to have an EOS token. SOS means Start of Sentence token and is used to indicate the start of a sentence.)\n",
        "        self.n_words = 2\n",
        "        \n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2int:\n",
        "            self.word2int[word] = self.n_words \n",
        "            self.word2count[word] = 1 \n",
        "            self.int2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "            \n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "    \n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(\" \"):\n",
        "            self.addWord(word)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsXg0mRxWQ5s",
        "colab_type": "text"
      },
      "source": [
        "Corpus consists of French words which may have some characters like ‘Ç’. For simplicity sake, we convert them into their normal corresponding ASCII characters(Ç → C). \n",
        "\n",
        "Also we create white spaces between words and punctuation attached to these words. (hello’s → hello s). This is to ensure that the presence of a punctuation does not create two words for a particular word (For example, different integers would be assign to “they’re” and \"they are\" although they have the same maeaning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UujikttqdJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicodeToAscii(s):\n",
        "    return \"\".join(c for c in unicodedata.normalize(\"NFD\", s) \\\n",
        "                   if unicodedata.category(c) != \"Mn\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb0sLusAqdKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    \n",
        "    s = re.sub(r\"([!.?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z?.!]+\", \" \", s)  #re.sub() function is used to replace occurrences of a particular sub-string with another sub-string.\n",
        "    return s\n",
        "\n",
        "    #When the ^ character appears outside of [] matches the beginning of the line (or string). \n",
        "    #When the ^ character appears inside the [], it matches any character not appearing inside the []."
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRVyyGNUcoBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# re.sub(r'(\\b[a-z]+) \\1', r'\\1', 'cat in the the hat')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bbmjKl-fUQo",
        "colab_type": "text"
      },
      "source": [
        "Two helper functions are combined under ***load_dataset()*** to load the dataset as a list of list containing the pair of sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbvFe0iqqdKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset():\n",
        "    with open(language,'r') as f:\n",
        "        lines = f.readlines()\n",
        "    \n",
        "    pairs = [[normalizeString(pair) for pair in \n",
        "              line.strip().split('\\t')] for line in lines]\n",
        "    return pairs"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26fM5LRUqdKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pairs = load_dataset()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufcihjlpf_-D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "ce50f686-e5a5-4ccd-f73e-6d5e490da606"
      },
      "source": [
        "pairs[0:10] #list of list containing the pair of sentences"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['go .', 'va !', 'cc by . france attribution tatoeba .org cm wittydev '],\n",
              " ['hi .', 'salut !', 'cc by . france attribution tatoeba .org cm aiji '],\n",
              " ['hi .', 'salut .', 'cc by . france attribution tatoeba .org cm gillux '],\n",
              " ['run !',\n",
              "  'cours !',\n",
              "  'cc by . france attribution tatoeba .org papabear sacredceltic '],\n",
              " ['run !',\n",
              "  'courez !',\n",
              "  'cc by . france attribution tatoeba .org papabear sacredceltic '],\n",
              " ['who ?', 'qui ?', 'cc by . france attribution tatoeba .org ck gillux '],\n",
              " ['wow !',\n",
              "  'ca alors !',\n",
              "  'cc by . france attribution tatoeba .org zifre zmoo '],\n",
              " ['fire !',\n",
              "  'au feu !',\n",
              "  'cc by . france attribution tatoeba .org spamster sacredceltic '],\n",
              " ['help !',\n",
              "  'a l aide !',\n",
              "  'cc by . france attribution tatoeba .org lukaszpp sysko '],\n",
              " ['jump .',\n",
              "  'saute .',\n",
              "  'cc by . france attribution tatoeba .org shishir phoenix ']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyWBlfA8jbPP",
        "colab_type": "text"
      },
      "source": [
        "To reduce the training time, we are going to filter out our dataset to remove sentences with more than ten words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnmGqtDOqdKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Iterates through a sentence, breaks it into words and maps the word to its corresponding integer\n",
        "value using the word2int dictionary which we implemented in the language class\n",
        "\n",
        "\"\"\"\n",
        "MAX_LENGTH = 10\n",
        "def sentencetoIndexes(sentence, lang):\n",
        "    indexes = [lang.word2int[word] for word in sentence.split()]\n",
        "    indexes.append(EOS_token)\n",
        "    return indexes\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split()) < MAX_LENGTH and \\\n",
        "len(p[1].split()) < MAX_LENGTH\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "pairs = filterPairs(pairs)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-yqR45pwMea",
        "colab_type": "text"
      },
      "source": [
        "To populate the word2int dictionary in each Language class with words and assign a corresponding  integer to each word in a new function. Also since we are going to batch our dataset, we are going to apply padding to sentences with words less than the maximum length we proposed. Let’s see how to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlAGPZShqdKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_lang(lang1, lang2, max_length=10):\n",
        "    input_lang = Lang(lang1)\n",
        "    output_lang = Lang(lang2)\n",
        "    input_seq = []\n",
        "    output_seq = []\n",
        "    \n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[1])  #french\n",
        "        output_lang.addSentence(pair[0]) #english\n",
        "    for pair in pairs:\n",
        "        input_seq.append(sentencetoIndexes(pair[1], input_lang))\n",
        "        output_seq.append(sentencetoIndexes(pair[0], output_lang))\n",
        "    return keras.preprocessing.sequence.pad_sequences(input_seq, maxlen=max_length, padding='post',\n",
        "                                                      truncating='post'), \\\n",
        "keras.preprocessing.sequence.pad_sequences(output_seq, padding='post', truncating='post'), input_lang, output_lang"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9MAxjU-qdKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor, output_tensor, input_lang, output_lang = build_lang('fr', 'en')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ8Pcm1Ls8Cc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8cc60e48-e59c-4bca-aea2-b1915bacd8f0"
      },
      "source": [
        "input_lang.name"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'fr'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPSDwq71ujqZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "97b8166f-a641-4219-fc14-b01d671e878d"
      },
      "source": [
        "print(\"input_tensor at index 10: {}\".format(input_tensor[10]))\n",
        "print(\"output_tensor at index 10: {}\".format(output_tensor[10]))\n",
        "print(\"corresponding integer value for 'nous' {}\".format(input_lang.word2int['nous']))\n",
        "print(\"corresponding integer value for 'she' {}\".format(output_lang.word2int['she']))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_tensor at index 10: [10 18  3  1  0  0  0  0  0  0]\n",
            "output_tensor at index 10: [13  6  1  0  0  0  0  0  0  0]\n",
            "corresponding integer value for 'nous' 98\n",
            "corresponding integer value for 'she' 194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp2j_RaPQpZv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "83be64a1-072e-4df0-be1e-e17e25b09d45"
      },
      "source": [
        "output_tensor.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(123935, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qii6d6iKqdKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating batches of size 16\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "BUFFER_SIZE = len(input_tensor)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor, output_tensor)).shuffle(BUFFER_SIZE) #maintains a buffer of size \n",
        "dataset = dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7IzgHTAVJh6",
        "colab_type": "text"
      },
      "source": [
        "# Seq2Seq model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9tu3s7KV1rd",
        "colab_type": "text"
      },
      "source": [
        "There are several architectures of Recurrent Neural Networks, each suited for a particular group of tasks. For examples, many-to-one architecture for task such as sentiment analysis and one-to-many for music generation but we are going to employ the many-to-many architecture which is suited for tasks such as chat-bots and Neural Machine Translation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QC26fd2YLQ4",
        "colab_type": "text"
      },
      "source": [
        "The class of RNN used here is called Sequence to Sequence model. This network is a combination of an encoder and a decoder. One to take in the input sentence and the other to translate into another language in the case of machine translation. This architecture can effectively handle the tasks where inputs and outputs have different lengths. Encoder feed fixed length vector to the decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i10ZBFOXez6f",
        "colab_type": "text"
      },
      "source": [
        "The only information that the decoder receives from the encoder is the hidden state. The hidden state is a fixed size vector into which the encoder squeezes every bit of information about the input sentence and passes it on to the decoder to generate the output sentence. This might work fairly well for shorter sentences but this fixed size vector tends to become a bottleneck for longer sentences. This is where attention mechanism becomes a crucial part of our translation network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj1DRDcp3LUb",
        "colab_type": "text"
      },
      "source": [
        "Encoder unit is made up of two layers: the Embedding layer which converts each token into a dense representation and a Recurrent Network layer(here Gate Recurrent Unit Network has been used).\n",
        "\n",
        "The two very important parameters in the GRU implementation: **return_sequences** and **return_state**. return_sequences ensures that the GRU outputs the hidden state of each time step which We need to access information about each word in the input sequence during the computation of the attention weights and the context vector. Additionally, Return state returns the hidden state of the last time step. We need this tensor to be used as initial hidden state for the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41nSXuflqdKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(keras.models.Model):\n",
        "    def __init__(self, vocab_size, num_hidden=256, num_embedding=256, batch_size=16):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.num_hidden = num_hidden\n",
        "        self.num_embedding = num_embedding\n",
        "        self.embedding = keras.layers.Embedding(vocab_size, num_embedding)\n",
        "        self.gru = keras.layers.GRU(num_hidden, return_sequences=True,\n",
        "                                    recurrent_initializer='glorot_uniform',\n",
        "                                   return_state=True)\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        embedded = self.embedding(x)\n",
        "        rnn_out, hidden = self.gru(embedded, initial_state=hidden)\n",
        "        return rnn_out, hidden\n",
        "    def init_hidden(self):\n",
        "        return tf.zeros(shape=(self.batch_size, self.num_hidden))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ9lsXnV7cvK",
        "colab_type": "text"
      },
      "source": [
        "In the call function where forward propagation of our network is implemented. we first pass the input tensor through an embedding layer and then through a GRU layer. This returns the RNN output(rnn_out) of shape (batch_size, max_sequence length, hidden_size) and hidden state of shape (batch_size, hidden_size)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qm8zLgoqdKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs, outputs = next(iter(dataset))\n",
        "hidden = tf.zeros((16, 256))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5AHvxMiqdKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(input_lang.n_words)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCF-S8fBqdKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e_outputs, e_hidden = encoder(inputs, hidden)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqGefguVqdKv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "70be079f-bee8-4bb2-9b20-01bae8e25bff"
      },
      "source": [
        "e_hidden"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(16, 256), dtype=float32, numpy=\n",
              "array([[ 0.01616649,  0.02700939, -0.02309283, ..., -0.01543358,\n",
              "         0.03011385, -0.01570803],\n",
              "       [ 0.0153998 ,  0.02441876, -0.02244034, ..., -0.01532582,\n",
              "         0.02675886, -0.0155929 ],\n",
              "       [-0.00043042,  0.01773172, -0.02228476, ..., -0.00324049,\n",
              "         0.01360346, -0.01082011],\n",
              "       ...,\n",
              "       [-0.00293469, -0.00411293, -0.01346144, ...,  0.01455526,\n",
              "        -0.01193631, -0.00232897],\n",
              "       [ 0.01566966,  0.02625025, -0.02388918, ..., -0.01532003,\n",
              "         0.02904152, -0.0162377 ],\n",
              "       [ 0.01711785,  0.02590249, -0.02316064, ..., -0.01666605,\n",
              "         0.02970154, -0.01609926]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGGzlvzHqXFT",
        "colab_type": "text"
      },
      "source": [
        "Decoder layer = Attention layer + fully connected layer\n",
        "\n",
        "Attention layer is designed to return the context vector and the attention weights. In the attention layer The **call** function where forward propagation takes place takes in two parameters; **encoder_out** which represents all the hidden states at each timestep in the encoder and **hidden** which represents the hidden state of the decoder before the current timestep where we are generating the correct word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21bfPvySsBpw",
        "colab_type": "text"
      },
      "source": [
        "We add a dimension of size 1 to represent the timestep hence shape of **hidden** becomes (batch_size, time_step, hidden_size). Score variable stores attention weights which is a function of the hidden state immediately before that timestep(hidden) in the decoder and all the hidden states (encoder_out) of the encoder. Then we've applied softmax to score across the max_length dimension which is at axis=1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bJHJsq1qdK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(keras.models.Model):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "    \n",
        "        self.W1 = keras.layers.Dense(units)\n",
        "        self.W2 = keras.layers.Dense(units)\n",
        "        self.V = keras.layers.Dense(1)\n",
        "        \n",
        "    def call(self, encoder_out, hidden):\n",
        "        #shape of encoder_out : batch_size, seq_length, hidden_dim \n",
        "        #shape of encoder_hidden : batch_size, hidden_dim \n",
        "        \n",
        "        hidden = tf.expand_dims(hidden, axis=1) #out: (16, 1, 1024)\n",
        "        \n",
        "        score = self.V(tf.nn.tanh(self.W1(encoder_out) + \\\n",
        "                                  self.W2(hidden))) #out: (16, 10, 1)\n",
        "\n",
        "        \n",
        "        attn_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        context =  attn_weights * encoder_out  #out: ((16,10,1) * (16,10,1024))=16, 10, 1024\n",
        "        context = tf.reduce_sum(context, axis=1) #out: 16, 1024\n",
        "        return context, attn_weights\n",
        "        "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwvMXJToz_qv",
        "colab_type": "text"
      },
      "source": [
        "The end product of the softmax function gives us the weights which we multiply with all the hidden states from the encoder. A hidden state at a particular timestep with a bigger weight value means more attention is being paid on the word at that timestep.\n",
        "\n",
        "reduce_sum will produce the context vector. After multiplying each hidden_state with its corresponding weight, we combine all resultant values through a summation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gRC60cCqdK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attn = BahdanauAttention(256)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHU6rIRzqdLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context, attn_weights = attn(e_outputs, e_hidden)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2XacZYkqdLK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf9c4f9c-f314-43e0-f7fe-fb80c1c00d3b"
      },
      "source": [
        "attn_weights.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([16, 10, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4BnFHyDk1nx",
        "colab_type": "text"
      },
      "source": [
        "Under the Decoder class\n",
        "\n",
        "x: tensor of a single word\n",
        "x is passed through an embedding layer that maps the single integer token into a dense 256 dimensional vector. That vector is later concatenated with the context vector generated by attention layer. The resultant tensor becomes the input for the Gated Recurrent Network for a single timestep.\n",
        "\n",
        "Finally we pass the output of the GRU through a fully connected layer which outputs a vector of size (batch_size, number of english words).\n",
        "\n",
        "We also return hidden state to be fed into the next timestep and the attention weights for later visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsLImrCwqdLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(keras.models.Model):\n",
        "    def __init__(self, vocab_size, dec_dim=256, embedding_dim=256):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.attn = BahdanauAttention(dec_dim)\n",
        "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = keras.layers.GRU(dec_dim, recurrent_initializer='glorot_uniform',\n",
        "                                   return_sequences=True, return_state=True)\n",
        "        self.fc = keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x, enc_hidden, enc_out):\n",
        "        x = self.embedding(x)\n",
        "        context, attn_weights = self.attn(enc_out, enc_hidden)\n",
        "        x = tf.concat((tf.expand_dims(context, 1), x), -1)\n",
        "        r_out, hidden = self.gru(x, initial_state=enc_hidden)\n",
        "        out = tf.reshape(r_out,shape=(-1, r_out.shape[2]))\n",
        "        return self.fc(out), hidden, attn_weights"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItYHAqWJqdLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder = Decoder(output_lang.n_words)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnhNtfb9qdLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor, output_tensor = next(iter(dataset))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7Y7QsykqdLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.expand_dims(output_tensor[:,1], -1)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At4TcH8NoX6c",
        "colab_type": "text"
      },
      "source": [
        "Next step is to build a training pipeline. \n",
        "\n",
        "Starting with our loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaWOV6fdpod-",
        "colab_type": "text"
      },
      "source": [
        "keras’s sparse categorical cross entropy module has been used since we have a large number of categories(number of english words). We create a mask that asserts that the padding tokens are not included in calculating the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DE7fnvAqdLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(real, pred):\n",
        "    criterion = keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                           reduction='none')\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0)) #mask asserts that the padding tokens are not included in calculating the loss.\n",
        "    _loss = criterion(real, pred)\n",
        "    mask = tf.cast(mask, dtype=_loss.dtype)\n",
        "    _loss *= mask\n",
        "    return tf.reduce_mean(_loss)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMc7zD6WqdLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.optimizers.Adam()"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM9heXPLrSFm",
        "colab_type": "text"
      },
      "source": [
        "**Building a training pipeline**\n",
        "\n",
        "\n",
        "The following snippet implements a single training step. In which we pass the input_tensor which represent the input sentence through the forward propagation pipeline of the Encoder. This return the enc_output(hidden_state of all timesteps) and enc_hidden(last hidden_state).\n",
        "\n",
        "In the decoding part, we use a technique called teacher forcing where instead of using the predicted word as input for the next timestep, we use the actual word.\n",
        "\n",
        "\n",
        "At the start of decoding, we feed the Start Of Sentence token as input and maximize the probability of the decoder predicting the first word in the output sequence as it output. We then take the actual first word and feed it into the second timestep and maximize the probability of the decoder predicting the second word in the output sequence as it output. This continues sequentially until we reach the End of Sentence token<EOS>. We accumulate all the losses, derive the gradients and train both networks end-to-end with the gradients. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiAmynEsqdLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(input_tensor, target_tensor, enc_hidden):\n",
        "    loss = 0.0\n",
        "    with tf.GradientTape() as tape:\n",
        "    \n",
        "        batch_size = input_tensor.shape[0]\n",
        "        enc_output, enc_hidden = encoder(input_tensor, enc_hidden)\n",
        "\n",
        "        SOS_tensor = np.array([SOS_token])\n",
        "        dec_input = tf.squeeze(tf.expand_dims([SOS_tensor]*batch_size, 1), -1)\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        for tx in range(target_tensor.shape[1]-1):\n",
        "          \n",
        "            dec_out, dec_hidden, _ = decoder(dec_input, dec_hidden,\n",
        "                                            enc_output)\n",
        "            loss += loss_fn(target_tensor[:, tx], dec_out)\n",
        "            dec_input = tf.expand_dims(target_tensor[:, tx], 1)\n",
        "\n",
        "    batch_loss = loss / target_tensor.shape[1]\n",
        "    t_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, t_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, t_variables))\n",
        "    return batch_loss\n",
        "    "
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kj6diJMqdL0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c59546e-cca5-4f17-bda6-41e5a4ad2a09"
      },
      "source": [
        "hidden = tf.zeros(shape=(16, 256))\n",
        "loss = train_step(input_tensor, output_tensor, hidden)\n",
        "print(loss)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(7.031624, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw5Sib0zvN15",
        "colab_type": "text"
      },
      "source": [
        "**checkpoint** is a helper function to save our model at certain points during our training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk_OG391qdL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def checkpoint(model, name=None):\n",
        "    if name is not None:\n",
        "        model.save_weights('{}.h5'.format(name))\n",
        "    else:\n",
        "        raise NotImplementedError"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuH_Zd23w1-S",
        "colab_type": "text"
      },
      "source": [
        "The final part of our training pipeline is a training loop. \n",
        "In each epoch, we iterate through our dataset and call the train_step function on each batch of the dataset. If-else statements just to log our training statistics on screen. Let’s see how."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZEbkwx9qdL-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "84c5eb07-1bb7-4c64-ed17-dda8f7fdedcb"
      },
      "source": [
        "EPOCHS = 10\n",
        "log_every = 50\n",
        "steps_per_epoch = len(pairs) // BATCH_SIZE\n",
        "loss_list = []\n",
        "\n",
        "for e in range(1, EPOCHS):\n",
        "    \n",
        "    total_loss = 0.0\n",
        "    enc_hidden = encoder.init_hidden()\n",
        "    \n",
        "    for idx, (input_tensor, target_tensor) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(input_tensor, target_tensor, hidden)\n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        if idx % log_every == 0:\n",
        "            loss_list.append(batch_loss)\n",
        "            print(\"Epochs: {} batch_loss: {:.4f}\".format(e, batch_loss))\n",
        "            checkpoint(encoder, 'encoder')\n",
        "            checkpoint(decoder, 'decoder')\n",
        "      "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epochs: 1 batch_loss: 3.5530\n",
            "Epochs: 1 batch_loss: 3.3279\n",
            "Epochs: 1 batch_loss: 3.6336\n",
            "Epochs: 1 batch_loss: 3.2904\n",
            "Epochs: 1 batch_loss: 3.6759\n",
            "Epochs: 1 batch_loss: 3.6160\n",
            "Epochs: 1 batch_loss: 3.3390\n",
            "Epochs: 1 batch_loss: 3.2499\n",
            "Epochs: 1 batch_loss: 3.3726\n",
            "Epochs: 1 batch_loss: 3.4554\n",
            "Epochs: 1 batch_loss: 3.3669\n",
            "Epochs: 1 batch_loss: 3.1826\n",
            "Epochs: 1 batch_loss: 3.6763\n",
            "Epochs: 1 batch_loss: 2.9055\n",
            "Epochs: 1 batch_loss: 3.2865\n",
            "Epochs: 1 batch_loss: 3.2335\n",
            "Epochs: 1 batch_loss: 3.2278\n",
            "Epochs: 1 batch_loss: 3.0268\n",
            "Epochs: 1 batch_loss: 2.9376\n",
            "Epochs: 1 batch_loss: 3.1822\n",
            "Epochs: 1 batch_loss: 2.9226\n",
            "Epochs: 1 batch_loss: 2.5447\n",
            "Epochs: 1 batch_loss: 3.2972\n",
            "Epochs: 1 batch_loss: 2.8368\n",
            "Epochs: 1 batch_loss: 2.9931\n",
            "Epochs: 1 batch_loss: 2.7049\n",
            "Epochs: 1 batch_loss: 2.5418\n",
            "Epochs: 1 batch_loss: 2.3800\n",
            "Epochs: 1 batch_loss: 2.5157\n",
            "Epochs: 1 batch_loss: 2.5143\n",
            "Epochs: 1 batch_loss: 2.3435\n",
            "Epochs: 1 batch_loss: 2.6450\n",
            "Epochs: 1 batch_loss: 2.2757\n",
            "Epochs: 1 batch_loss: 2.2078\n",
            "Epochs: 1 batch_loss: 2.6522\n",
            "Epochs: 1 batch_loss: 2.3309\n",
            "Epochs: 1 batch_loss: 2.4312\n",
            "Epochs: 1 batch_loss: 2.2920\n",
            "Epochs: 1 batch_loss: 2.3456\n",
            "Epochs: 1 batch_loss: 2.1210\n",
            "Epochs: 1 batch_loss: 1.9493\n",
            "Epochs: 1 batch_loss: 2.2051\n",
            "Epochs: 1 batch_loss: 2.0120\n",
            "Epochs: 1 batch_loss: 1.9355\n",
            "Epochs: 1 batch_loss: 1.7262\n",
            "Epochs: 1 batch_loss: 1.9986\n",
            "Epochs: 1 batch_loss: 2.1409\n",
            "Epochs: 1 batch_loss: 1.9724\n",
            "Epochs: 1 batch_loss: 1.9650\n",
            "Epochs: 1 batch_loss: 2.0994\n",
            "Epochs: 1 batch_loss: 2.2106\n",
            "Epochs: 1 batch_loss: 2.0471\n",
            "Epochs: 1 batch_loss: 1.9735\n",
            "Epochs: 1 batch_loss: 1.9289\n",
            "Epochs: 1 batch_loss: 2.1653\n",
            "Epochs: 1 batch_loss: 1.8901\n",
            "Epochs: 1 batch_loss: 2.1072\n",
            "Epochs: 1 batch_loss: 2.2026\n",
            "Epochs: 1 batch_loss: 1.6502\n",
            "Epochs: 1 batch_loss: 1.7704\n",
            "Epochs: 1 batch_loss: 1.9885\n",
            "Epochs: 1 batch_loss: 2.0627\n",
            "Epochs: 1 batch_loss: 1.6174\n",
            "Epochs: 1 batch_loss: 1.7157\n",
            "Epochs: 1 batch_loss: 1.6745\n",
            "Epochs: 1 batch_loss: 1.9455\n",
            "Epochs: 1 batch_loss: 1.8605\n",
            "Epochs: 1 batch_loss: 2.0968\n",
            "Epochs: 1 batch_loss: 1.6642\n",
            "Epochs: 1 batch_loss: 2.2718\n",
            "Epochs: 1 batch_loss: 1.7878\n",
            "Epochs: 1 batch_loss: 1.5427\n",
            "Epochs: 1 batch_loss: 1.4516\n",
            "Epochs: 1 batch_loss: 1.5162\n",
            "Epochs: 1 batch_loss: 1.5553\n",
            "Epochs: 1 batch_loss: 1.5236\n",
            "Epochs: 1 batch_loss: 1.4035\n",
            "Epochs: 1 batch_loss: 1.5308\n",
            "Epochs: 1 batch_loss: 1.5287\n",
            "Epochs: 1 batch_loss: 1.5709\n",
            "Epochs: 1 batch_loss: 1.2170\n",
            "Epochs: 1 batch_loss: 2.1616\n",
            "Epochs: 1 batch_loss: 1.3853\n",
            "Epochs: 1 batch_loss: 1.5819\n",
            "Epochs: 1 batch_loss: 1.9943\n",
            "Epochs: 1 batch_loss: 1.5265\n",
            "Epochs: 1 batch_loss: 1.3963\n",
            "Epochs: 1 batch_loss: 1.4579\n",
            "Epochs: 1 batch_loss: 1.1502\n",
            "Epochs: 1 batch_loss: 1.4292\n",
            "Epochs: 1 batch_loss: 1.6526\n",
            "Epochs: 1 batch_loss: 0.9309\n",
            "Epochs: 1 batch_loss: 1.1753\n",
            "Epochs: 1 batch_loss: 1.5976\n",
            "Epochs: 1 batch_loss: 1.3197\n",
            "Epochs: 1 batch_loss: 1.1447\n",
            "Epochs: 1 batch_loss: 1.0726\n",
            "Epochs: 1 batch_loss: 1.3522\n",
            "Epochs: 1 batch_loss: 1.4687\n",
            "Epochs: 1 batch_loss: 0.8380\n",
            "Epochs: 1 batch_loss: 1.0521\n",
            "Epochs: 1 batch_loss: 1.3065\n",
            "Epochs: 1 batch_loss: 0.9445\n",
            "Epochs: 1 batch_loss: 1.1744\n",
            "Epochs: 1 batch_loss: 1.5056\n",
            "Epochs: 1 batch_loss: 1.2688\n",
            "Epochs: 1 batch_loss: 1.1637\n",
            "Epochs: 1 batch_loss: 1.5419\n",
            "Epochs: 1 batch_loss: 1.0951\n",
            "Epochs: 1 batch_loss: 1.1559\n",
            "Epochs: 1 batch_loss: 1.3623\n",
            "Epochs: 1 batch_loss: 1.2169\n",
            "Epochs: 1 batch_loss: 1.5707\n",
            "Epochs: 1 batch_loss: 1.2425\n",
            "Epochs: 1 batch_loss: 1.2210\n",
            "Epochs: 1 batch_loss: 1.1586\n",
            "Epochs: 1 batch_loss: 1.1711\n",
            "Epochs: 1 batch_loss: 1.1724\n",
            "Epochs: 1 batch_loss: 0.8718\n",
            "Epochs: 1 batch_loss: 1.0204\n",
            "Epochs: 1 batch_loss: 0.9770\n",
            "Epochs: 1 batch_loss: 1.6134\n",
            "Epochs: 1 batch_loss: 1.0419\n",
            "Epochs: 1 batch_loss: 1.2718\n",
            "Epochs: 1 batch_loss: 1.3110\n",
            "Epochs: 1 batch_loss: 1.1367\n",
            "Epochs: 1 batch_loss: 0.9512\n",
            "Epochs: 1 batch_loss: 0.7101\n",
            "Epochs: 1 batch_loss: 1.2385\n",
            "Epochs: 1 batch_loss: 1.6266\n",
            "Epochs: 1 batch_loss: 1.0806\n",
            "Epochs: 1 batch_loss: 0.9164\n",
            "Epochs: 1 batch_loss: 1.3228\n",
            "Epochs: 1 batch_loss: 0.8197\n",
            "Epochs: 1 batch_loss: 1.2295\n",
            "Epochs: 1 batch_loss: 1.0946\n",
            "Epochs: 1 batch_loss: 1.2742\n",
            "Epochs: 1 batch_loss: 0.8271\n",
            "Epochs: 1 batch_loss: 0.9429\n",
            "Epochs: 1 batch_loss: 0.9679\n",
            "Epochs: 1 batch_loss: 1.3129\n",
            "Epochs: 1 batch_loss: 1.1750\n",
            "Epochs: 1 batch_loss: 1.1644\n",
            "Epochs: 1 batch_loss: 1.1485\n",
            "Epochs: 1 batch_loss: 1.0059\n",
            "Epochs: 1 batch_loss: 1.1913\n",
            "Epochs: 1 batch_loss: 0.9772\n",
            "Epochs: 1 batch_loss: 0.6889\n",
            "Epochs: 1 batch_loss: 0.9120\n",
            "Epochs: 1 batch_loss: 1.2290\n",
            "Epochs: 1 batch_loss: 1.0487\n",
            "Epochs: 1 batch_loss: 1.0652\n",
            "Epochs: 1 batch_loss: 0.9524\n",
            "Epochs: 1 batch_loss: 1.1536\n",
            "Epochs: 1 batch_loss: 0.7397\n",
            "Epochs: 2 batch_loss: 0.9408\n",
            "Epochs: 2 batch_loss: 0.9615\n",
            "Epochs: 2 batch_loss: 0.9654\n",
            "Epochs: 2 batch_loss: 1.3307\n",
            "Epochs: 2 batch_loss: 0.6820\n",
            "Epochs: 2 batch_loss: 1.1630\n",
            "Epochs: 2 batch_loss: 1.1858\n",
            "Epochs: 2 batch_loss: 1.0148\n",
            "Epochs: 2 batch_loss: 1.0756\n",
            "Epochs: 2 batch_loss: 0.7828\n",
            "Epochs: 2 batch_loss: 1.2404\n",
            "Epochs: 2 batch_loss: 0.8590\n",
            "Epochs: 2 batch_loss: 1.0104\n",
            "Epochs: 2 batch_loss: 0.9776\n",
            "Epochs: 2 batch_loss: 0.9455\n",
            "Epochs: 2 batch_loss: 0.6772\n",
            "Epochs: 2 batch_loss: 0.7772\n",
            "Epochs: 2 batch_loss: 1.1879\n",
            "Epochs: 2 batch_loss: 0.9882\n",
            "Epochs: 2 batch_loss: 1.1140\n",
            "Epochs: 2 batch_loss: 0.8131\n",
            "Epochs: 2 batch_loss: 1.0104\n",
            "Epochs: 2 batch_loss: 1.1524\n",
            "Epochs: 2 batch_loss: 1.1138\n",
            "Epochs: 2 batch_loss: 0.7268\n",
            "Epochs: 2 batch_loss: 0.9680\n",
            "Epochs: 2 batch_loss: 1.3818\n",
            "Epochs: 2 batch_loss: 0.8597\n",
            "Epochs: 2 batch_loss: 0.7159\n",
            "Epochs: 2 batch_loss: 0.7645\n",
            "Epochs: 2 batch_loss: 0.6333\n",
            "Epochs: 2 batch_loss: 1.2464\n",
            "Epochs: 2 batch_loss: 0.9627\n",
            "Epochs: 2 batch_loss: 0.5609\n",
            "Epochs: 2 batch_loss: 0.8893\n",
            "Epochs: 2 batch_loss: 1.1362\n",
            "Epochs: 2 batch_loss: 0.7596\n",
            "Epochs: 2 batch_loss: 0.9226\n",
            "Epochs: 2 batch_loss: 0.8808\n",
            "Epochs: 2 batch_loss: 0.7155\n",
            "Epochs: 2 batch_loss: 1.0361\n",
            "Epochs: 2 batch_loss: 0.8435\n",
            "Epochs: 2 batch_loss: 1.1238\n",
            "Epochs: 2 batch_loss: 0.8564\n",
            "Epochs: 2 batch_loss: 0.6443\n",
            "Epochs: 2 batch_loss: 0.6890\n",
            "Epochs: 2 batch_loss: 0.8538\n",
            "Epochs: 2 batch_loss: 0.5065\n",
            "Epochs: 2 batch_loss: 0.9452\n",
            "Epochs: 2 batch_loss: 1.1056\n",
            "Epochs: 2 batch_loss: 0.5663\n",
            "Epochs: 2 batch_loss: 0.7896\n",
            "Epochs: 2 batch_loss: 0.7489\n",
            "Epochs: 2 batch_loss: 0.8701\n",
            "Epochs: 2 batch_loss: 0.9640\n",
            "Epochs: 2 batch_loss: 0.6401\n",
            "Epochs: 2 batch_loss: 0.9483\n",
            "Epochs: 2 batch_loss: 0.7275\n",
            "Epochs: 2 batch_loss: 0.5592\n",
            "Epochs: 2 batch_loss: 1.0626\n",
            "Epochs: 2 batch_loss: 0.8837\n",
            "Epochs: 2 batch_loss: 1.0070\n",
            "Epochs: 2 batch_loss: 0.8194\n",
            "Epochs: 2 batch_loss: 0.6548\n",
            "Epochs: 2 batch_loss: 1.2083\n",
            "Epochs: 2 batch_loss: 0.8101\n",
            "Epochs: 2 batch_loss: 0.9751\n",
            "Epochs: 2 batch_loss: 0.7221\n",
            "Epochs: 2 batch_loss: 0.7242\n",
            "Epochs: 2 batch_loss: 0.6889\n",
            "Epochs: 2 batch_loss: 0.9171\n",
            "Epochs: 2 batch_loss: 0.9366\n",
            "Epochs: 2 batch_loss: 0.9358\n",
            "Epochs: 2 batch_loss: 0.9973\n",
            "Epochs: 2 batch_loss: 0.8782\n",
            "Epochs: 2 batch_loss: 0.6382\n",
            "Epochs: 2 batch_loss: 0.7353\n",
            "Epochs: 2 batch_loss: 0.4239\n",
            "Epochs: 2 batch_loss: 0.9216\n",
            "Epochs: 2 batch_loss: 0.6341\n",
            "Epochs: 2 batch_loss: 0.6767\n",
            "Epochs: 2 batch_loss: 0.5060\n",
            "Epochs: 2 batch_loss: 0.7879\n",
            "Epochs: 2 batch_loss: 0.9455\n",
            "Epochs: 2 batch_loss: 0.7380\n",
            "Epochs: 2 batch_loss: 0.4523\n",
            "Epochs: 2 batch_loss: 1.0525\n",
            "Epochs: 2 batch_loss: 0.5449\n",
            "Epochs: 2 batch_loss: 0.5653\n",
            "Epochs: 2 batch_loss: 0.8307\n",
            "Epochs: 2 batch_loss: 0.9551\n",
            "Epochs: 2 batch_loss: 0.9613\n",
            "Epochs: 2 batch_loss: 1.0288\n",
            "Epochs: 2 batch_loss: 0.7699\n",
            "Epochs: 2 batch_loss: 0.7253\n",
            "Epochs: 2 batch_loss: 0.8063\n",
            "Epochs: 2 batch_loss: 0.9371\n",
            "Epochs: 2 batch_loss: 0.5876\n",
            "Epochs: 2 batch_loss: 1.2833\n",
            "Epochs: 2 batch_loss: 0.8283\n",
            "Epochs: 2 batch_loss: 0.8308\n",
            "Epochs: 2 batch_loss: 0.8580\n",
            "Epochs: 2 batch_loss: 0.8382\n",
            "Epochs: 2 batch_loss: 0.9147\n",
            "Epochs: 2 batch_loss: 1.1597\n",
            "Epochs: 2 batch_loss: 0.8226\n",
            "Epochs: 2 batch_loss: 0.6476\n",
            "Epochs: 2 batch_loss: 0.7978\n",
            "Epochs: 2 batch_loss: 0.8894\n",
            "Epochs: 2 batch_loss: 0.8867\n",
            "Epochs: 2 batch_loss: 0.6282\n",
            "Epochs: 2 batch_loss: 0.8082\n",
            "Epochs: 2 batch_loss: 1.0623\n",
            "Epochs: 2 batch_loss: 0.8394\n",
            "Epochs: 2 batch_loss: 0.6889\n",
            "Epochs: 2 batch_loss: 0.7172\n",
            "Epochs: 2 batch_loss: 1.0503\n",
            "Epochs: 2 batch_loss: 0.7969\n",
            "Epochs: 2 batch_loss: 0.7445\n",
            "Epochs: 2 batch_loss: 1.0262\n",
            "Epochs: 2 batch_loss: 0.6088\n",
            "Epochs: 2 batch_loss: 0.9172\n",
            "Epochs: 2 batch_loss: 0.8037\n",
            "Epochs: 2 batch_loss: 0.6029\n",
            "Epochs: 2 batch_loss: 0.5583\n",
            "Epochs: 2 batch_loss: 0.7699\n",
            "Epochs: 2 batch_loss: 1.0872\n",
            "Epochs: 2 batch_loss: 0.7137\n",
            "Epochs: 2 batch_loss: 0.8152\n",
            "Epochs: 2 batch_loss: 0.8005\n",
            "Epochs: 2 batch_loss: 0.7653\n",
            "Epochs: 2 batch_loss: 1.1707\n",
            "Epochs: 2 batch_loss: 0.9877\n",
            "Epochs: 2 batch_loss: 0.7643\n",
            "Epochs: 2 batch_loss: 0.7641\n",
            "Epochs: 2 batch_loss: 0.6873\n",
            "Epochs: 2 batch_loss: 0.6624\n",
            "Epochs: 2 batch_loss: 0.9207\n",
            "Epochs: 2 batch_loss: 0.6402\n",
            "Epochs: 2 batch_loss: 0.5937\n",
            "Epochs: 2 batch_loss: 0.6224\n",
            "Epochs: 2 batch_loss: 1.0320\n",
            "Epochs: 2 batch_loss: 0.4786\n",
            "Epochs: 2 batch_loss: 1.0190\n",
            "Epochs: 2 batch_loss: 0.7663\n",
            "Epochs: 2 batch_loss: 0.9163\n",
            "Epochs: 2 batch_loss: 0.7181\n",
            "Epochs: 2 batch_loss: 0.6803\n",
            "Epochs: 2 batch_loss: 0.4589\n",
            "Epochs: 2 batch_loss: 0.6003\n",
            "Epochs: 2 batch_loss: 0.6104\n",
            "Epochs: 2 batch_loss: 0.8708\n",
            "Epochs: 2 batch_loss: 0.9475\n",
            "Epochs: 2 batch_loss: 0.8240\n",
            "Epochs: 2 batch_loss: 0.6645\n",
            "Epochs: 3 batch_loss: 0.5626\n",
            "Epochs: 3 batch_loss: 0.5782\n",
            "Epochs: 3 batch_loss: 0.6652\n",
            "Epochs: 3 batch_loss: 0.5841\n",
            "Epochs: 3 batch_loss: 0.5226\n",
            "Epochs: 3 batch_loss: 0.6677\n",
            "Epochs: 3 batch_loss: 0.8055\n",
            "Epochs: 3 batch_loss: 0.8990\n",
            "Epochs: 3 batch_loss: 0.3739\n",
            "Epochs: 3 batch_loss: 0.6158\n",
            "Epochs: 3 batch_loss: 0.5355\n",
            "Epochs: 3 batch_loss: 0.3755\n",
            "Epochs: 3 batch_loss: 0.5361\n",
            "Epochs: 3 batch_loss: 0.7236\n",
            "Epochs: 3 batch_loss: 0.6595\n",
            "Epochs: 3 batch_loss: 0.8826\n",
            "Epochs: 3 batch_loss: 0.4635\n",
            "Epochs: 3 batch_loss: 0.6282\n",
            "Epochs: 3 batch_loss: 0.5557\n",
            "Epochs: 3 batch_loss: 0.3504\n",
            "Epochs: 3 batch_loss: 0.4357\n",
            "Epochs: 3 batch_loss: 0.7602\n",
            "Epochs: 3 batch_loss: 1.0503\n",
            "Epochs: 3 batch_loss: 0.6368\n",
            "Epochs: 3 batch_loss: 0.8302\n",
            "Epochs: 3 batch_loss: 0.7537\n",
            "Epochs: 3 batch_loss: 0.3475\n",
            "Epochs: 3 batch_loss: 0.3617\n",
            "Epochs: 3 batch_loss: 0.6027\n",
            "Epochs: 3 batch_loss: 0.4986\n",
            "Epochs: 3 batch_loss: 0.4402\n",
            "Epochs: 3 batch_loss: 0.5556\n",
            "Epochs: 3 batch_loss: 0.3941\n",
            "Epochs: 3 batch_loss: 0.5822\n",
            "Epochs: 3 batch_loss: 0.8061\n",
            "Epochs: 3 batch_loss: 0.5826\n",
            "Epochs: 3 batch_loss: 0.5099\n",
            "Epochs: 3 batch_loss: 0.4538\n",
            "Epochs: 3 batch_loss: 0.5220\n",
            "Epochs: 3 batch_loss: 0.2607\n",
            "Epochs: 3 batch_loss: 0.4770\n",
            "Epochs: 3 batch_loss: 0.6459\n",
            "Epochs: 3 batch_loss: 0.5299\n",
            "Epochs: 3 batch_loss: 0.3897\n",
            "Epochs: 3 batch_loss: 0.7521\n",
            "Epochs: 3 batch_loss: 0.4651\n",
            "Epochs: 3 batch_loss: 0.7265\n",
            "Epochs: 3 batch_loss: 0.8999\n",
            "Epochs: 3 batch_loss: 0.4340\n",
            "Epochs: 3 batch_loss: 0.5537\n",
            "Epochs: 3 batch_loss: 0.6813\n",
            "Epochs: 3 batch_loss: 0.7135\n",
            "Epochs: 3 batch_loss: 1.0058\n",
            "Epochs: 3 batch_loss: 0.5894\n",
            "Epochs: 3 batch_loss: 0.6142\n",
            "Epochs: 3 batch_loss: 0.2311\n",
            "Epochs: 3 batch_loss: 0.5060\n",
            "Epochs: 3 batch_loss: 0.3065\n",
            "Epochs: 3 batch_loss: 0.4742\n",
            "Epochs: 3 batch_loss: 0.5256\n",
            "Epochs: 3 batch_loss: 0.7080\n",
            "Epochs: 3 batch_loss: 0.5290\n",
            "Epochs: 3 batch_loss: 0.4869\n",
            "Epochs: 3 batch_loss: 0.5513\n",
            "Epochs: 3 batch_loss: 0.5651\n",
            "Epochs: 3 batch_loss: 0.5537\n",
            "Epochs: 3 batch_loss: 0.4662\n",
            "Epochs: 3 batch_loss: 0.6030\n",
            "Epochs: 3 batch_loss: 0.6673\n",
            "Epochs: 3 batch_loss: 0.6470\n",
            "Epochs: 3 batch_loss: 0.4884\n",
            "Epochs: 3 batch_loss: 0.5811\n",
            "Epochs: 3 batch_loss: 0.4815\n",
            "Epochs: 3 batch_loss: 0.4031\n",
            "Epochs: 3 batch_loss: 0.6002\n",
            "Epochs: 3 batch_loss: 0.3906\n",
            "Epochs: 3 batch_loss: 0.4956\n",
            "Epochs: 3 batch_loss: 0.7726\n",
            "Epochs: 3 batch_loss: 0.5816\n",
            "Epochs: 3 batch_loss: 0.6338\n",
            "Epochs: 3 batch_loss: 0.8151\n",
            "Epochs: 3 batch_loss: 0.6108\n",
            "Epochs: 3 batch_loss: 0.4597\n",
            "Epochs: 3 batch_loss: 0.6836\n",
            "Epochs: 3 batch_loss: 0.6074\n",
            "Epochs: 3 batch_loss: 0.5323\n",
            "Epochs: 3 batch_loss: 0.5715\n",
            "Epochs: 3 batch_loss: 0.5613\n",
            "Epochs: 3 batch_loss: 0.4686\n",
            "Epochs: 3 batch_loss: 0.3334\n",
            "Epochs: 3 batch_loss: 0.4922\n",
            "Epochs: 3 batch_loss: 0.4725\n",
            "Epochs: 3 batch_loss: 0.6205\n",
            "Epochs: 3 batch_loss: 0.4744\n",
            "Epochs: 3 batch_loss: 0.5699\n",
            "Epochs: 3 batch_loss: 0.3918\n",
            "Epochs: 3 batch_loss: 0.4957\n",
            "Epochs: 3 batch_loss: 0.7742\n",
            "Epochs: 3 batch_loss: 0.6308\n",
            "Epochs: 3 batch_loss: 0.7134\n",
            "Epochs: 3 batch_loss: 0.4955\n",
            "Epochs: 3 batch_loss: 0.3105\n",
            "Epochs: 3 batch_loss: 0.8309\n",
            "Epochs: 3 batch_loss: 0.8051\n",
            "Epochs: 3 batch_loss: 0.7018\n",
            "Epochs: 3 batch_loss: 0.3865\n",
            "Epochs: 3 batch_loss: 0.3887\n",
            "Epochs: 3 batch_loss: 0.5311\n",
            "Epochs: 3 batch_loss: 0.6842\n",
            "Epochs: 3 batch_loss: 0.4991\n",
            "Epochs: 3 batch_loss: 0.8164\n",
            "Epochs: 3 batch_loss: 0.3985\n",
            "Epochs: 3 batch_loss: 0.4326\n",
            "Epochs: 3 batch_loss: 0.7911\n",
            "Epochs: 3 batch_loss: 0.4414\n",
            "Epochs: 3 batch_loss: 0.3369\n",
            "Epochs: 3 batch_loss: 0.9079\n",
            "Epochs: 3 batch_loss: 0.3860\n",
            "Epochs: 3 batch_loss: 0.3990\n",
            "Epochs: 3 batch_loss: 0.5347\n",
            "Epochs: 3 batch_loss: 0.6342\n",
            "Epochs: 3 batch_loss: 0.5555\n",
            "Epochs: 3 batch_loss: 0.5643\n",
            "Epochs: 3 batch_loss: 0.4736\n",
            "Epochs: 3 batch_loss: 0.3512\n",
            "Epochs: 3 batch_loss: 0.3676\n",
            "Epochs: 3 batch_loss: 0.4020\n",
            "Epochs: 3 batch_loss: 0.7001\n",
            "Epochs: 3 batch_loss: 0.3380\n",
            "Epochs: 3 batch_loss: 0.4488\n",
            "Epochs: 3 batch_loss: 0.2020\n",
            "Epochs: 3 batch_loss: 0.6815\n",
            "Epochs: 3 batch_loss: 0.3874\n",
            "Epochs: 3 batch_loss: 0.3336\n",
            "Epochs: 3 batch_loss: 0.5939\n",
            "Epochs: 3 batch_loss: 0.8023\n",
            "Epochs: 3 batch_loss: 0.6793\n",
            "Epochs: 3 batch_loss: 0.4056\n",
            "Epochs: 3 batch_loss: 0.5126\n",
            "Epochs: 3 batch_loss: 0.6246\n",
            "Epochs: 3 batch_loss: 0.8265\n",
            "Epochs: 3 batch_loss: 0.5763\n",
            "Epochs: 3 batch_loss: 0.5897\n",
            "Epochs: 3 batch_loss: 0.5180\n",
            "Epochs: 3 batch_loss: 0.4695\n",
            "Epochs: 3 batch_loss: 0.3716\n",
            "Epochs: 3 batch_loss: 0.2792\n",
            "Epochs: 3 batch_loss: 0.6259\n",
            "Epochs: 3 batch_loss: 0.3854\n",
            "Epochs: 3 batch_loss: 0.4967\n",
            "Epochs: 3 batch_loss: 0.7682\n",
            "Epochs: 3 batch_loss: 0.8988\n",
            "Epochs: 3 batch_loss: 0.4178\n",
            "Epochs: 3 batch_loss: 0.6525\n",
            "Epochs: 3 batch_loss: 0.5643\n",
            "Epochs: 4 batch_loss: 0.4894\n",
            "Epochs: 4 batch_loss: 0.4321\n",
            "Epochs: 4 batch_loss: 0.6440\n",
            "Epochs: 4 batch_loss: 0.4386\n",
            "Epochs: 4 batch_loss: 0.3798\n",
            "Epochs: 4 batch_loss: 0.3855\n",
            "Epochs: 4 batch_loss: 0.2109\n",
            "Epochs: 4 batch_loss: 0.2221\n",
            "Epochs: 4 batch_loss: 0.4745\n",
            "Epochs: 4 batch_loss: 0.3100\n",
            "Epochs: 4 batch_loss: 0.3335\n",
            "Epochs: 4 batch_loss: 0.3674\n",
            "Epochs: 4 batch_loss: 0.5987\n",
            "Epochs: 4 batch_loss: 0.6815\n",
            "Epochs: 4 batch_loss: 0.4148\n",
            "Epochs: 4 batch_loss: 0.2555\n",
            "Epochs: 4 batch_loss: 0.4287\n",
            "Epochs: 4 batch_loss: 0.4525\n",
            "Epochs: 4 batch_loss: 0.3347\n",
            "Epochs: 4 batch_loss: 0.3975\n",
            "Epochs: 4 batch_loss: 0.5609\n",
            "Epochs: 4 batch_loss: 0.3869\n",
            "Epochs: 4 batch_loss: 0.2074\n",
            "Epochs: 4 batch_loss: 0.5338\n",
            "Epochs: 4 batch_loss: 0.4678\n",
            "Epochs: 4 batch_loss: 0.5746\n",
            "Epochs: 4 batch_loss: 0.6257\n",
            "Epochs: 4 batch_loss: 0.4178\n",
            "Epochs: 4 batch_loss: 0.6298\n",
            "Epochs: 4 batch_loss: 0.5845\n",
            "Epochs: 4 batch_loss: 0.4375\n",
            "Epochs: 4 batch_loss: 0.3788\n",
            "Epochs: 4 batch_loss: 0.4411\n",
            "Epochs: 4 batch_loss: 0.3310\n",
            "Epochs: 4 batch_loss: 0.3062\n",
            "Epochs: 4 batch_loss: 0.4032\n",
            "Epochs: 4 batch_loss: 0.2807\n",
            "Epochs: 4 batch_loss: 0.5376\n",
            "Epochs: 4 batch_loss: 0.3656\n",
            "Epochs: 4 batch_loss: 0.4328\n",
            "Epochs: 4 batch_loss: 0.5093\n",
            "Epochs: 4 batch_loss: 0.3862\n",
            "Epochs: 4 batch_loss: 0.4300\n",
            "Epochs: 4 batch_loss: 0.6910\n",
            "Epochs: 4 batch_loss: 0.3046\n",
            "Epochs: 4 batch_loss: 0.3633\n",
            "Epochs: 4 batch_loss: 0.3924\n",
            "Epochs: 4 batch_loss: 0.2569\n",
            "Epochs: 4 batch_loss: 0.4572\n",
            "Epochs: 4 batch_loss: 0.6799\n",
            "Epochs: 4 batch_loss: 0.3802\n",
            "Epochs: 4 batch_loss: 0.2244\n",
            "Epochs: 4 batch_loss: 0.4301\n",
            "Epochs: 4 batch_loss: 0.3494\n",
            "Epochs: 4 batch_loss: 0.3636\n",
            "Epochs: 4 batch_loss: 0.3307\n",
            "Epochs: 4 batch_loss: 0.4294\n",
            "Epochs: 4 batch_loss: 0.6741\n",
            "Epochs: 4 batch_loss: 0.5271\n",
            "Epochs: 4 batch_loss: 0.4689\n",
            "Epochs: 4 batch_loss: 0.6878\n",
            "Epochs: 4 batch_loss: 0.5375\n",
            "Epochs: 4 batch_loss: 0.5123\n",
            "Epochs: 4 batch_loss: 0.3862\n",
            "Epochs: 4 batch_loss: 0.2902\n",
            "Epochs: 4 batch_loss: 0.4880\n",
            "Epochs: 4 batch_loss: 0.5754\n",
            "Epochs: 4 batch_loss: 0.3565\n",
            "Epochs: 4 batch_loss: 0.4411\n",
            "Epochs: 4 batch_loss: 0.4895\n",
            "Epochs: 4 batch_loss: 0.2699\n",
            "Epochs: 4 batch_loss: 0.3681\n",
            "Epochs: 4 batch_loss: 0.3378\n",
            "Epochs: 4 batch_loss: 0.5416\n",
            "Epochs: 4 batch_loss: 0.3783\n",
            "Epochs: 4 batch_loss: 0.3703\n",
            "Epochs: 4 batch_loss: 0.4401\n",
            "Epochs: 4 batch_loss: 0.4740\n",
            "Epochs: 4 batch_loss: 0.4830\n",
            "Epochs: 4 batch_loss: 0.3687\n",
            "Epochs: 4 batch_loss: 0.4944\n",
            "Epochs: 4 batch_loss: 0.5220\n",
            "Epochs: 4 batch_loss: 0.5887\n",
            "Epochs: 4 batch_loss: 0.4637\n",
            "Epochs: 4 batch_loss: 0.3625\n",
            "Epochs: 4 batch_loss: 0.3824\n",
            "Epochs: 4 batch_loss: 0.2851\n",
            "Epochs: 4 batch_loss: 0.4106\n",
            "Epochs: 4 batch_loss: 0.4641\n",
            "Epochs: 4 batch_loss: 0.3176\n",
            "Epochs: 4 batch_loss: 0.6474\n",
            "Epochs: 4 batch_loss: 0.2122\n",
            "Epochs: 4 batch_loss: 0.3209\n",
            "Epochs: 4 batch_loss: 0.3849\n",
            "Epochs: 4 batch_loss: 0.4868\n",
            "Epochs: 4 batch_loss: 0.3599\n",
            "Epochs: 4 batch_loss: 0.5407\n",
            "Epochs: 4 batch_loss: 0.1843\n",
            "Epochs: 4 batch_loss: 0.3991\n",
            "Epochs: 4 batch_loss: 0.4856\n",
            "Epochs: 4 batch_loss: 0.5703\n",
            "Epochs: 4 batch_loss: 0.3288\n",
            "Epochs: 4 batch_loss: 0.3317\n",
            "Epochs: 4 batch_loss: 0.5002\n",
            "Epochs: 4 batch_loss: 0.3170\n",
            "Epochs: 4 batch_loss: 1.1261\n",
            "Epochs: 4 batch_loss: 0.4026\n",
            "Epochs: 4 batch_loss: 0.7871\n",
            "Epochs: 4 batch_loss: 0.4677\n",
            "Epochs: 4 batch_loss: 0.3636\n",
            "Epochs: 4 batch_loss: 0.3729\n",
            "Epochs: 4 batch_loss: 0.5520\n",
            "Epochs: 4 batch_loss: 0.4211\n",
            "Epochs: 4 batch_loss: 0.5892\n",
            "Epochs: 4 batch_loss: 0.2278\n",
            "Epochs: 4 batch_loss: 0.3709\n",
            "Epochs: 4 batch_loss: 0.4556\n",
            "Epochs: 4 batch_loss: 0.4668\n",
            "Epochs: 4 batch_loss: 0.5741\n",
            "Epochs: 4 batch_loss: 0.3005\n",
            "Epochs: 4 batch_loss: 0.3287\n",
            "Epochs: 4 batch_loss: 0.5616\n",
            "Epochs: 4 batch_loss: 0.3168\n",
            "Epochs: 4 batch_loss: 0.3450\n",
            "Epochs: 4 batch_loss: 0.2908\n",
            "Epochs: 4 batch_loss: 0.4430\n",
            "Epochs: 4 batch_loss: 0.2331\n",
            "Epochs: 4 batch_loss: 0.7023\n",
            "Epochs: 4 batch_loss: 0.3994\n",
            "Epochs: 4 batch_loss: 0.7613\n",
            "Epochs: 4 batch_loss: 0.6833\n",
            "Epochs: 4 batch_loss: 0.2840\n",
            "Epochs: 4 batch_loss: 0.4302\n",
            "Epochs: 4 batch_loss: 0.6164\n",
            "Epochs: 4 batch_loss: 0.3610\n",
            "Epochs: 4 batch_loss: 0.6199\n",
            "Epochs: 4 batch_loss: 0.6666\n",
            "Epochs: 4 batch_loss: 0.5208\n",
            "Epochs: 4 batch_loss: 0.2793\n",
            "Epochs: 4 batch_loss: 0.3245\n",
            "Epochs: 4 batch_loss: 0.3584\n",
            "Epochs: 4 batch_loss: 0.6582\n",
            "Epochs: 4 batch_loss: 0.4211\n",
            "Epochs: 4 batch_loss: 0.3161\n",
            "Epochs: 4 batch_loss: 0.3728\n",
            "Epochs: 4 batch_loss: 0.3871\n",
            "Epochs: 4 batch_loss: 0.3471\n",
            "Epochs: 4 batch_loss: 0.3382\n",
            "Epochs: 4 batch_loss: 0.5366\n",
            "Epochs: 4 batch_loss: 0.7043\n",
            "Epochs: 4 batch_loss: 0.5514\n",
            "Epochs: 4 batch_loss: 0.4225\n",
            "Epochs: 4 batch_loss: 0.7867\n",
            "Epochs: 4 batch_loss: 0.3020\n",
            "Epochs: 4 batch_loss: 0.3781\n",
            "Epochs: 5 batch_loss: 0.3160\n",
            "Epochs: 5 batch_loss: 0.3643\n",
            "Epochs: 5 batch_loss: 0.2947\n",
            "Epochs: 5 batch_loss: 0.3849\n",
            "Epochs: 5 batch_loss: 0.3486\n",
            "Epochs: 5 batch_loss: 0.2934\n",
            "Epochs: 5 batch_loss: 0.2622\n",
            "Epochs: 5 batch_loss: 0.3141\n",
            "Epochs: 5 batch_loss: 0.2477\n",
            "Epochs: 5 batch_loss: 0.4455\n",
            "Epochs: 5 batch_loss: 0.1977\n",
            "Epochs: 5 batch_loss: 0.3956\n",
            "Epochs: 5 batch_loss: 0.2743\n",
            "Epochs: 5 batch_loss: 0.2169\n",
            "Epochs: 5 batch_loss: 0.3320\n",
            "Epochs: 5 batch_loss: 0.2714\n",
            "Epochs: 5 batch_loss: 0.4215\n",
            "Epochs: 5 batch_loss: 0.4313\n",
            "Epochs: 5 batch_loss: 0.3012\n",
            "Epochs: 5 batch_loss: 0.3566\n",
            "Epochs: 5 batch_loss: 0.3017\n",
            "Epochs: 5 batch_loss: 0.2226\n",
            "Epochs: 5 batch_loss: 0.3913\n",
            "Epochs: 5 batch_loss: 0.2631\n",
            "Epochs: 5 batch_loss: 0.3692\n",
            "Epochs: 5 batch_loss: 0.3501\n",
            "Epochs: 5 batch_loss: 0.2815\n",
            "Epochs: 5 batch_loss: 0.4860\n",
            "Epochs: 5 batch_loss: 0.4038\n",
            "Epochs: 5 batch_loss: 0.3664\n",
            "Epochs: 5 batch_loss: 0.3488\n",
            "Epochs: 5 batch_loss: 0.2778\n",
            "Epochs: 5 batch_loss: 0.2532\n",
            "Epochs: 5 batch_loss: 0.2000\n",
            "Epochs: 5 batch_loss: 0.3153\n",
            "Epochs: 5 batch_loss: 0.3045\n",
            "Epochs: 5 batch_loss: 0.3194\n",
            "Epochs: 5 batch_loss: 0.4925\n",
            "Epochs: 5 batch_loss: 0.3543\n",
            "Epochs: 5 batch_loss: 0.4092\n",
            "Epochs: 5 batch_loss: 0.6053\n",
            "Epochs: 5 batch_loss: 0.5823\n",
            "Epochs: 5 batch_loss: 0.3611\n",
            "Epochs: 5 batch_loss: 0.2976\n",
            "Epochs: 5 batch_loss: 0.2919\n",
            "Epochs: 5 batch_loss: 0.3349\n",
            "Epochs: 5 batch_loss: 0.2325\n",
            "Epochs: 5 batch_loss: 0.3448\n",
            "Epochs: 5 batch_loss: 0.3814\n",
            "Epochs: 5 batch_loss: 0.6088\n",
            "Epochs: 5 batch_loss: 0.3928\n",
            "Epochs: 5 batch_loss: 0.2857\n",
            "Epochs: 5 batch_loss: 0.3718\n",
            "Epochs: 5 batch_loss: 0.4460\n",
            "Epochs: 5 batch_loss: 0.3751\n",
            "Epochs: 5 batch_loss: 0.2890\n",
            "Epochs: 5 batch_loss: 0.2146\n",
            "Epochs: 5 batch_loss: 0.1868\n",
            "Epochs: 5 batch_loss: 0.3382\n",
            "Epochs: 5 batch_loss: 0.3453\n",
            "Epochs: 5 batch_loss: 0.6634\n",
            "Epochs: 5 batch_loss: 0.3861\n",
            "Epochs: 5 batch_loss: 0.4034\n",
            "Epochs: 5 batch_loss: 0.2480\n",
            "Epochs: 5 batch_loss: 0.3889\n",
            "Epochs: 5 batch_loss: 0.6792\n",
            "Epochs: 5 batch_loss: 0.2642\n",
            "Epochs: 5 batch_loss: 0.3531\n",
            "Epochs: 5 batch_loss: 0.5054\n",
            "Epochs: 5 batch_loss: 0.3395\n",
            "Epochs: 5 batch_loss: 0.2640\n",
            "Epochs: 5 batch_loss: 0.2673\n",
            "Epochs: 5 batch_loss: 0.5308\n",
            "Epochs: 5 batch_loss: 0.3251\n",
            "Epochs: 5 batch_loss: 0.4455\n",
            "Epochs: 5 batch_loss: 0.3734\n",
            "Epochs: 5 batch_loss: 0.4342\n",
            "Epochs: 5 batch_loss: 0.4006\n",
            "Epochs: 5 batch_loss: 0.3948\n",
            "Epochs: 5 batch_loss: 0.4502\n",
            "Epochs: 5 batch_loss: 0.6583\n",
            "Epochs: 5 batch_loss: 0.4009\n",
            "Epochs: 5 batch_loss: 0.3007\n",
            "Epochs: 5 batch_loss: 0.4943\n",
            "Epochs: 5 batch_loss: 0.2531\n",
            "Epochs: 5 batch_loss: 0.4934\n",
            "Epochs: 5 batch_loss: 0.4532\n",
            "Epochs: 5 batch_loss: 0.2840\n",
            "Epochs: 5 batch_loss: 0.2992\n",
            "Epochs: 5 batch_loss: 0.4143\n",
            "Epochs: 5 batch_loss: 0.3369\n",
            "Epochs: 5 batch_loss: 0.2532\n",
            "Epochs: 5 batch_loss: 0.5944\n",
            "Epochs: 5 batch_loss: 0.5961\n",
            "Epochs: 5 batch_loss: 0.5484\n",
            "Epochs: 5 batch_loss: 0.2144\n",
            "Epochs: 5 batch_loss: 0.4866\n",
            "Epochs: 5 batch_loss: 0.4486\n",
            "Epochs: 5 batch_loss: 0.4364\n",
            "Epochs: 5 batch_loss: 0.3342\n",
            "Epochs: 5 batch_loss: 0.5645\n",
            "Epochs: 5 batch_loss: 0.4340\n",
            "Epochs: 5 batch_loss: 0.4273\n",
            "Epochs: 5 batch_loss: 0.3385\n",
            "Epochs: 5 batch_loss: 0.6848\n",
            "Epochs: 5 batch_loss: 0.4857\n",
            "Epochs: 5 batch_loss: 0.3927\n",
            "Epochs: 5 batch_loss: 0.3603\n",
            "Epochs: 5 batch_loss: 0.2741\n",
            "Epochs: 5 batch_loss: 0.3746\n",
            "Epochs: 5 batch_loss: 0.2603\n",
            "Epochs: 5 batch_loss: 0.5607\n",
            "Epochs: 5 batch_loss: 0.3737\n",
            "Epochs: 5 batch_loss: 0.3296\n",
            "Epochs: 5 batch_loss: 0.3204\n",
            "Epochs: 5 batch_loss: 0.4297\n",
            "Epochs: 5 batch_loss: 0.5777\n",
            "Epochs: 5 batch_loss: 0.3407\n",
            "Epochs: 5 batch_loss: 0.5059\n",
            "Epochs: 5 batch_loss: 0.4046\n",
            "Epochs: 5 batch_loss: 0.3414\n",
            "Epochs: 5 batch_loss: 0.3448\n",
            "Epochs: 5 batch_loss: 0.3622\n",
            "Epochs: 5 batch_loss: 0.4347\n",
            "Epochs: 5 batch_loss: 0.3430\n",
            "Epochs: 5 batch_loss: 0.2472\n",
            "Epochs: 5 batch_loss: 0.2589\n",
            "Epochs: 5 batch_loss: 0.2791\n",
            "Epochs: 5 batch_loss: 0.3474\n",
            "Epochs: 5 batch_loss: 0.2799\n",
            "Epochs: 5 batch_loss: 0.3208\n",
            "Epochs: 5 batch_loss: 0.3557\n",
            "Epochs: 5 batch_loss: 0.4410\n",
            "Epochs: 5 batch_loss: 0.5535\n",
            "Epochs: 5 batch_loss: 0.2982\n",
            "Epochs: 5 batch_loss: 0.3889\n",
            "Epochs: 5 batch_loss: 0.4501\n",
            "Epochs: 5 batch_loss: 0.4024\n",
            "Epochs: 5 batch_loss: 0.2643\n",
            "Epochs: 5 batch_loss: 0.4684\n",
            "Epochs: 5 batch_loss: 0.3742\n",
            "Epochs: 5 batch_loss: 0.2264\n",
            "Epochs: 5 batch_loss: 0.2422\n",
            "Epochs: 5 batch_loss: 0.3224\n",
            "Epochs: 5 batch_loss: 0.2746\n",
            "Epochs: 5 batch_loss: 0.5334\n",
            "Epochs: 5 batch_loss: 0.2724\n",
            "Epochs: 5 batch_loss: 0.4783\n",
            "Epochs: 5 batch_loss: 0.2353\n",
            "Epochs: 5 batch_loss: 0.3317\n",
            "Epochs: 5 batch_loss: 0.3454\n",
            "Epochs: 5 batch_loss: 0.4131\n",
            "Epochs: 5 batch_loss: 0.3111\n",
            "Epochs: 5 batch_loss: 0.4751\n",
            "Epochs: 5 batch_loss: 0.4365\n",
            "Epochs: 6 batch_loss: 0.2067\n",
            "Epochs: 6 batch_loss: 0.2699\n",
            "Epochs: 6 batch_loss: 0.4249\n",
            "Epochs: 6 batch_loss: 0.2094\n",
            "Epochs: 6 batch_loss: 0.1937\n",
            "Epochs: 6 batch_loss: 0.2069\n",
            "Epochs: 6 batch_loss: 0.1164\n",
            "Epochs: 6 batch_loss: 0.4389\n",
            "Epochs: 6 batch_loss: 0.3390\n",
            "Epochs: 6 batch_loss: 0.2064\n",
            "Epochs: 6 batch_loss: 0.1632\n",
            "Epochs: 6 batch_loss: 0.2999\n",
            "Epochs: 6 batch_loss: 0.3330\n",
            "Epochs: 6 batch_loss: 0.2620\n",
            "Epochs: 6 batch_loss: 0.1785\n",
            "Epochs: 6 batch_loss: 0.3125\n",
            "Epochs: 6 batch_loss: 0.1713\n",
            "Epochs: 6 batch_loss: 0.3309\n",
            "Epochs: 6 batch_loss: 0.1756\n",
            "Epochs: 6 batch_loss: 0.4037\n",
            "Epochs: 6 batch_loss: 0.2974\n",
            "Epochs: 6 batch_loss: 0.3418\n",
            "Epochs: 6 batch_loss: 0.2814\n",
            "Epochs: 6 batch_loss: 0.2678\n",
            "Epochs: 6 batch_loss: 0.2630\n",
            "Epochs: 6 batch_loss: 0.3956\n",
            "Epochs: 6 batch_loss: 0.1674\n",
            "Epochs: 6 batch_loss: 0.3585\n",
            "Epochs: 6 batch_loss: 0.3267\n",
            "Epochs: 6 batch_loss: 0.2318\n",
            "Epochs: 6 batch_loss: 0.3330\n",
            "Epochs: 6 batch_loss: 0.3424\n",
            "Epochs: 6 batch_loss: 0.1847\n",
            "Epochs: 6 batch_loss: 0.4045\n",
            "Epochs: 6 batch_loss: 0.2108\n",
            "Epochs: 6 batch_loss: 0.4722\n",
            "Epochs: 6 batch_loss: 0.3074\n",
            "Epochs: 6 batch_loss: 0.3522\n",
            "Epochs: 6 batch_loss: 0.4378\n",
            "Epochs: 6 batch_loss: 0.2997\n",
            "Epochs: 6 batch_loss: 0.2149\n",
            "Epochs: 6 batch_loss: 0.1879\n",
            "Epochs: 6 batch_loss: 0.2151\n",
            "Epochs: 6 batch_loss: 0.1486\n",
            "Epochs: 6 batch_loss: 0.4609\n",
            "Epochs: 6 batch_loss: 0.1535\n",
            "Epochs: 6 batch_loss: 0.2580\n",
            "Epochs: 6 batch_loss: 0.2726\n",
            "Epochs: 6 batch_loss: 0.2652\n",
            "Epochs: 6 batch_loss: 0.4358\n",
            "Epochs: 6 batch_loss: 0.1889\n",
            "Epochs: 6 batch_loss: 0.2400\n",
            "Epochs: 6 batch_loss: 0.3382\n",
            "Epochs: 6 batch_loss: 0.3009\n",
            "Epochs: 6 batch_loss: 0.2485\n",
            "Epochs: 6 batch_loss: 0.2913\n",
            "Epochs: 6 batch_loss: 0.2400\n",
            "Epochs: 6 batch_loss: 0.2248\n",
            "Epochs: 6 batch_loss: 0.2013\n",
            "Epochs: 6 batch_loss: 0.3850\n",
            "Epochs: 6 batch_loss: 0.2664\n",
            "Epochs: 6 batch_loss: 0.2671\n",
            "Epochs: 6 batch_loss: 0.2874\n",
            "Epochs: 6 batch_loss: 0.1874\n",
            "Epochs: 6 batch_loss: 0.2723\n",
            "Epochs: 6 batch_loss: 0.2001\n",
            "Epochs: 6 batch_loss: 0.1625\n",
            "Epochs: 6 batch_loss: 0.2859\n",
            "Epochs: 6 batch_loss: 0.2730\n",
            "Epochs: 6 batch_loss: 0.2711\n",
            "Epochs: 6 batch_loss: 0.5074\n",
            "Epochs: 6 batch_loss: 0.2181\n",
            "Epochs: 6 batch_loss: 0.3597\n",
            "Epochs: 6 batch_loss: 0.1619\n",
            "Epochs: 6 batch_loss: 0.3003\n",
            "Epochs: 6 batch_loss: 0.1922\n",
            "Epochs: 6 batch_loss: 0.2296\n",
            "Epochs: 6 batch_loss: 0.3415\n",
            "Epochs: 6 batch_loss: 0.2457\n",
            "Epochs: 6 batch_loss: 0.3255\n",
            "Epochs: 6 batch_loss: 0.1884\n",
            "Epochs: 6 batch_loss: 0.3359\n",
            "Epochs: 6 batch_loss: 0.4876\n",
            "Epochs: 6 batch_loss: 0.2412\n",
            "Epochs: 6 batch_loss: 0.2612\n",
            "Epochs: 6 batch_loss: 0.6302\n",
            "Epochs: 6 batch_loss: 0.1929\n",
            "Epochs: 6 batch_loss: 0.4085\n",
            "Epochs: 6 batch_loss: 0.3303\n",
            "Epochs: 6 batch_loss: 0.2878\n",
            "Epochs: 6 batch_loss: 0.2274\n",
            "Epochs: 6 batch_loss: 0.2713\n",
            "Epochs: 6 batch_loss: 0.3709\n",
            "Epochs: 6 batch_loss: 0.4626\n",
            "Epochs: 6 batch_loss: 0.3226\n",
            "Epochs: 6 batch_loss: 0.5041\n",
            "Epochs: 6 batch_loss: 0.3029\n",
            "Epochs: 6 batch_loss: 0.3841\n",
            "Epochs: 6 batch_loss: 0.4647\n",
            "Epochs: 6 batch_loss: 0.2992\n",
            "Epochs: 6 batch_loss: 0.3771\n",
            "Epochs: 6 batch_loss: 0.3356\n",
            "Epochs: 6 batch_loss: 0.2427\n",
            "Epochs: 6 batch_loss: 0.3291\n",
            "Epochs: 6 batch_loss: 0.1764\n",
            "Epochs: 6 batch_loss: 0.2142\n",
            "Epochs: 6 batch_loss: 0.2732\n",
            "Epochs: 6 batch_loss: 0.1840\n",
            "Epochs: 6 batch_loss: 0.1553\n",
            "Epochs: 6 batch_loss: 0.3780\n",
            "Epochs: 6 batch_loss: 0.3220\n",
            "Epochs: 6 batch_loss: 0.2323\n",
            "Epochs: 6 batch_loss: 0.3920\n",
            "Epochs: 6 batch_loss: 0.3048\n",
            "Epochs: 6 batch_loss: 0.2418\n",
            "Epochs: 6 batch_loss: 0.3832\n",
            "Epochs: 6 batch_loss: 0.1882\n",
            "Epochs: 6 batch_loss: 0.1260\n",
            "Epochs: 6 batch_loss: 0.4097\n",
            "Epochs: 6 batch_loss: 0.5391\n",
            "Epochs: 6 batch_loss: 0.4356\n",
            "Epochs: 6 batch_loss: 0.3247\n",
            "Epochs: 6 batch_loss: 0.1544\n",
            "Epochs: 6 batch_loss: 0.2407\n",
            "Epochs: 6 batch_loss: 0.2510\n",
            "Epochs: 6 batch_loss: 0.2922\n",
            "Epochs: 6 batch_loss: 0.2825\n",
            "Epochs: 6 batch_loss: 0.2577\n",
            "Epochs: 6 batch_loss: 0.3551\n",
            "Epochs: 6 batch_loss: 0.1933\n",
            "Epochs: 6 batch_loss: 0.2918\n",
            "Epochs: 6 batch_loss: 0.1775\n",
            "Epochs: 6 batch_loss: 0.4615\n",
            "Epochs: 6 batch_loss: 0.2072\n",
            "Epochs: 6 batch_loss: 0.3213\n",
            "Epochs: 6 batch_loss: 0.3653\n",
            "Epochs: 6 batch_loss: 0.3598\n",
            "Epochs: 6 batch_loss: 0.2508\n",
            "Epochs: 6 batch_loss: 0.3510\n",
            "Epochs: 6 batch_loss: 0.2127\n",
            "Epochs: 6 batch_loss: 0.5036\n",
            "Epochs: 6 batch_loss: 0.3421\n",
            "Epochs: 6 batch_loss: 0.3924\n",
            "Epochs: 6 batch_loss: 0.3626\n",
            "Epochs: 6 batch_loss: 0.3320\n",
            "Epochs: 6 batch_loss: 0.1893\n",
            "Epochs: 6 batch_loss: 0.3865\n",
            "Epochs: 6 batch_loss: 0.3126\n",
            "Epochs: 6 batch_loss: 0.2521\n",
            "Epochs: 6 batch_loss: 0.3329\n",
            "Epochs: 6 batch_loss: 0.1561\n",
            "Epochs: 6 batch_loss: 0.1837\n",
            "Epochs: 6 batch_loss: 0.3620\n",
            "Epochs: 6 batch_loss: 0.3338\n",
            "Epochs: 6 batch_loss: 0.4513\n",
            "Epochs: 7 batch_loss: 0.1954\n",
            "Epochs: 7 batch_loss: 0.2246\n",
            "Epochs: 7 batch_loss: 0.2466\n",
            "Epochs: 7 batch_loss: 0.2580\n",
            "Epochs: 7 batch_loss: 0.1679\n",
            "Epochs: 7 batch_loss: 0.3000\n",
            "Epochs: 7 batch_loss: 0.1829\n",
            "Epochs: 7 batch_loss: 0.3238\n",
            "Epochs: 7 batch_loss: 0.3632\n",
            "Epochs: 7 batch_loss: 0.3489\n",
            "Epochs: 7 batch_loss: 0.1904\n",
            "Epochs: 7 batch_loss: 0.2664\n",
            "Epochs: 7 batch_loss: 0.3008\n",
            "Epochs: 7 batch_loss: 0.1942\n",
            "Epochs: 7 batch_loss: 0.3700\n",
            "Epochs: 7 batch_loss: 0.1496\n",
            "Epochs: 7 batch_loss: 0.2730\n",
            "Epochs: 7 batch_loss: 0.1955\n",
            "Epochs: 7 batch_loss: 0.2198\n",
            "Epochs: 7 batch_loss: 0.1172\n",
            "Epochs: 7 batch_loss: 0.1482\n",
            "Epochs: 7 batch_loss: 0.1230\n",
            "Epochs: 7 batch_loss: 0.2228\n",
            "Epochs: 7 batch_loss: 0.3152\n",
            "Epochs: 7 batch_loss: 0.3251\n",
            "Epochs: 7 batch_loss: 0.2992\n",
            "Epochs: 7 batch_loss: 0.2749\n",
            "Epochs: 7 batch_loss: 0.2840\n",
            "Epochs: 7 batch_loss: 0.1328\n",
            "Epochs: 7 batch_loss: 0.2561\n",
            "Epochs: 7 batch_loss: 0.1938\n",
            "Epochs: 7 batch_loss: 0.1852\n",
            "Epochs: 7 batch_loss: 0.1270\n",
            "Epochs: 7 batch_loss: 0.2071\n",
            "Epochs: 7 batch_loss: 0.3491\n",
            "Epochs: 7 batch_loss: 0.2675\n",
            "Epochs: 7 batch_loss: 0.3803\n",
            "Epochs: 7 batch_loss: 0.2242\n",
            "Epochs: 7 batch_loss: 0.1305\n",
            "Epochs: 7 batch_loss: 0.3086\n",
            "Epochs: 7 batch_loss: 0.3951\n",
            "Epochs: 7 batch_loss: 0.1581\n",
            "Epochs: 7 batch_loss: 0.1780\n",
            "Epochs: 7 batch_loss: 0.2735\n",
            "Epochs: 7 batch_loss: 0.1928\n",
            "Epochs: 7 batch_loss: 0.2299\n",
            "Epochs: 7 batch_loss: 0.4230\n",
            "Epochs: 7 batch_loss: 0.2913\n",
            "Epochs: 7 batch_loss: 0.5394\n",
            "Epochs: 7 batch_loss: 0.1107\n",
            "Epochs: 7 batch_loss: 0.2061\n",
            "Epochs: 7 batch_loss: 0.1425\n",
            "Epochs: 7 batch_loss: 0.2973\n",
            "Epochs: 7 batch_loss: 0.2029\n",
            "Epochs: 7 batch_loss: 0.1780\n",
            "Epochs: 7 batch_loss: 0.1824\n",
            "Epochs: 7 batch_loss: 0.1898\n",
            "Epochs: 7 batch_loss: 0.1843\n",
            "Epochs: 7 batch_loss: 0.2302\n",
            "Epochs: 7 batch_loss: 0.3388\n",
            "Epochs: 7 batch_loss: 0.2575\n",
            "Epochs: 7 batch_loss: 0.1932\n",
            "Epochs: 7 batch_loss: 0.2514\n",
            "Epochs: 7 batch_loss: 0.1716\n",
            "Epochs: 7 batch_loss: 0.1676\n",
            "Epochs: 7 batch_loss: 0.2424\n",
            "Epochs: 7 batch_loss: 0.3620\n",
            "Epochs: 7 batch_loss: 0.2067\n",
            "Epochs: 7 batch_loss: 0.2006\n",
            "Epochs: 7 batch_loss: 0.2210\n",
            "Epochs: 7 batch_loss: 0.1526\n",
            "Epochs: 7 batch_loss: 0.1483\n",
            "Epochs: 7 batch_loss: 0.2297\n",
            "Epochs: 7 batch_loss: 0.2013\n",
            "Epochs: 7 batch_loss: 0.4299\n",
            "Epochs: 7 batch_loss: 0.1618\n",
            "Epochs: 7 batch_loss: 0.2136\n",
            "Epochs: 7 batch_loss: 0.4039\n",
            "Epochs: 7 batch_loss: 0.3759\n",
            "Epochs: 7 batch_loss: 0.1744\n",
            "Epochs: 7 batch_loss: 0.1881\n",
            "Epochs: 7 batch_loss: 0.2399\n",
            "Epochs: 7 batch_loss: 0.1653\n",
            "Epochs: 7 batch_loss: 0.3484\n",
            "Epochs: 7 batch_loss: 0.3098\n",
            "Epochs: 7 batch_loss: 0.2902\n",
            "Epochs: 7 batch_loss: 0.2901\n",
            "Epochs: 7 batch_loss: 0.2527\n",
            "Epochs: 7 batch_loss: 0.2431\n",
            "Epochs: 7 batch_loss: 0.3834\n",
            "Epochs: 7 batch_loss: 0.2443\n",
            "Epochs: 7 batch_loss: 0.2970\n",
            "Epochs: 7 batch_loss: 0.4512\n",
            "Epochs: 7 batch_loss: 0.3261\n",
            "Epochs: 7 batch_loss: 0.2983\n",
            "Epochs: 7 batch_loss: 0.3865\n",
            "Epochs: 7 batch_loss: 0.2597\n",
            "Epochs: 7 batch_loss: 0.2747\n",
            "Epochs: 7 batch_loss: 0.4104\n",
            "Epochs: 7 batch_loss: 0.3085\n",
            "Epochs: 7 batch_loss: 0.3860\n",
            "Epochs: 7 batch_loss: 0.1693\n",
            "Epochs: 7 batch_loss: 0.1547\n",
            "Epochs: 7 batch_loss: 0.3047\n",
            "Epochs: 7 batch_loss: 0.2161\n",
            "Epochs: 7 batch_loss: 0.2916\n",
            "Epochs: 7 batch_loss: 0.4981\n",
            "Epochs: 7 batch_loss: 0.2872\n",
            "Epochs: 7 batch_loss: 0.3095\n",
            "Epochs: 7 batch_loss: 0.1858\n",
            "Epochs: 7 batch_loss: 0.2476\n",
            "Epochs: 7 batch_loss: 0.3593\n",
            "Epochs: 7 batch_loss: 0.2831\n",
            "Epochs: 7 batch_loss: 0.3244\n",
            "Epochs: 7 batch_loss: 0.2835\n",
            "Epochs: 7 batch_loss: 0.3254\n",
            "Epochs: 7 batch_loss: 0.2769\n",
            "Epochs: 7 batch_loss: 0.3607\n",
            "Epochs: 7 batch_loss: 0.3258\n",
            "Epochs: 7 batch_loss: 0.4590\n",
            "Epochs: 7 batch_loss: 0.1789\n",
            "Epochs: 7 batch_loss: 0.1543\n",
            "Epochs: 7 batch_loss: 0.2112\n",
            "Epochs: 7 batch_loss: 0.1753\n",
            "Epochs: 7 batch_loss: 0.2082\n",
            "Epochs: 7 batch_loss: 0.3223\n",
            "Epochs: 7 batch_loss: 0.2001\n",
            "Epochs: 7 batch_loss: 0.1933\n",
            "Epochs: 7 batch_loss: 0.1779\n",
            "Epochs: 7 batch_loss: 0.1932\n",
            "Epochs: 7 batch_loss: 0.2687\n",
            "Epochs: 7 batch_loss: 0.4876\n",
            "Epochs: 7 batch_loss: 0.2748\n",
            "Epochs: 7 batch_loss: 0.4023\n",
            "Epochs: 7 batch_loss: 0.2299\n",
            "Epochs: 7 batch_loss: 0.2734\n",
            "Epochs: 7 batch_loss: 0.3485\n",
            "Epochs: 7 batch_loss: 0.4343\n",
            "Epochs: 7 batch_loss: 0.3958\n",
            "Epochs: 7 batch_loss: 0.5086\n",
            "Epochs: 7 batch_loss: 0.2014\n",
            "Epochs: 7 batch_loss: 0.3901\n",
            "Epochs: 7 batch_loss: 0.2516\n",
            "Epochs: 7 batch_loss: 0.2647\n",
            "Epochs: 7 batch_loss: 0.4765\n",
            "Epochs: 7 batch_loss: 0.2814\n",
            "Epochs: 7 batch_loss: 0.3039\n",
            "Epochs: 7 batch_loss: 0.1465\n",
            "Epochs: 7 batch_loss: 0.3571\n",
            "Epochs: 7 batch_loss: 0.3656\n",
            "Epochs: 7 batch_loss: 0.2676\n",
            "Epochs: 7 batch_loss: 0.2527\n",
            "Epochs: 7 batch_loss: 0.4397\n",
            "Epochs: 7 batch_loss: 0.3153\n",
            "Epochs: 7 batch_loss: 0.3560\n",
            "Epochs: 8 batch_loss: 0.3430\n",
            "Epochs: 8 batch_loss: 0.1433\n",
            "Epochs: 8 batch_loss: 0.1604\n",
            "Epochs: 8 batch_loss: 0.2731\n",
            "Epochs: 8 batch_loss: 0.2106\n",
            "Epochs: 8 batch_loss: 0.1726\n",
            "Epochs: 8 batch_loss: 0.3118\n",
            "Epochs: 8 batch_loss: 0.2035\n",
            "Epochs: 8 batch_loss: 0.2012\n",
            "Epochs: 8 batch_loss: 0.2218\n",
            "Epochs: 8 batch_loss: 0.2540\n",
            "Epochs: 8 batch_loss: 0.1195\n",
            "Epochs: 8 batch_loss: 0.2218\n",
            "Epochs: 8 batch_loss: 0.1377\n",
            "Epochs: 8 batch_loss: 0.1897\n",
            "Epochs: 8 batch_loss: 0.1145\n",
            "Epochs: 8 batch_loss: 0.1656\n",
            "Epochs: 8 batch_loss: 0.2177\n",
            "Epochs: 8 batch_loss: 0.1628\n",
            "Epochs: 8 batch_loss: 0.2515\n",
            "Epochs: 8 batch_loss: 0.2729\n",
            "Epochs: 8 batch_loss: 0.1848\n",
            "Epochs: 8 batch_loss: 0.1343\n",
            "Epochs: 8 batch_loss: 0.1215\n",
            "Epochs: 8 batch_loss: 0.2249\n",
            "Epochs: 8 batch_loss: 0.1326\n",
            "Epochs: 8 batch_loss: 0.2647\n",
            "Epochs: 8 batch_loss: 0.2086\n",
            "Epochs: 8 batch_loss: 0.2179\n",
            "Epochs: 8 batch_loss: 0.2388\n",
            "Epochs: 8 batch_loss: 0.1733\n",
            "Epochs: 8 batch_loss: 0.2236\n",
            "Epochs: 8 batch_loss: 0.2264\n",
            "Epochs: 8 batch_loss: 0.1922\n",
            "Epochs: 8 batch_loss: 0.1532\n",
            "Epochs: 8 batch_loss: 0.3018\n",
            "Epochs: 8 batch_loss: 0.1794\n",
            "Epochs: 8 batch_loss: 0.2165\n",
            "Epochs: 8 batch_loss: 0.1606\n",
            "Epochs: 8 batch_loss: 0.3447\n",
            "Epochs: 8 batch_loss: 0.1846\n",
            "Epochs: 8 batch_loss: 0.3349\n",
            "Epochs: 8 batch_loss: 0.3638\n",
            "Epochs: 8 batch_loss: 0.2872\n",
            "Epochs: 8 batch_loss: 0.2179\n",
            "Epochs: 8 batch_loss: 0.2101\n",
            "Epochs: 8 batch_loss: 0.1153\n",
            "Epochs: 8 batch_loss: 0.2210\n",
            "Epochs: 8 batch_loss: 0.2392\n",
            "Epochs: 8 batch_loss: 0.2719\n",
            "Epochs: 8 batch_loss: 0.2791\n",
            "Epochs: 8 batch_loss: 0.1935\n",
            "Epochs: 8 batch_loss: 0.1678\n",
            "Epochs: 8 batch_loss: 0.2889\n",
            "Epochs: 8 batch_loss: 0.1821\n",
            "Epochs: 8 batch_loss: 0.3098\n",
            "Epochs: 8 batch_loss: 0.2052\n",
            "Epochs: 8 batch_loss: 0.1443\n",
            "Epochs: 8 batch_loss: 0.2084\n",
            "Epochs: 8 batch_loss: 0.1969\n",
            "Epochs: 8 batch_loss: 0.2405\n",
            "Epochs: 8 batch_loss: 0.2311\n",
            "Epochs: 8 batch_loss: 0.1544\n",
            "Epochs: 8 batch_loss: 0.2744\n",
            "Epochs: 8 batch_loss: 0.4427\n",
            "Epochs: 8 batch_loss: 0.1494\n",
            "Epochs: 8 batch_loss: 0.1787\n",
            "Epochs: 8 batch_loss: 0.2172\n",
            "Epochs: 8 batch_loss: 0.3498\n",
            "Epochs: 8 batch_loss: 0.3218\n",
            "Epochs: 8 batch_loss: 0.2375\n",
            "Epochs: 8 batch_loss: 0.1845\n",
            "Epochs: 8 batch_loss: 0.1860\n",
            "Epochs: 8 batch_loss: 0.1168\n",
            "Epochs: 8 batch_loss: 0.2918\n",
            "Epochs: 8 batch_loss: 0.3958\n",
            "Epochs: 8 batch_loss: 0.1962\n",
            "Epochs: 8 batch_loss: 0.2535\n",
            "Epochs: 8 batch_loss: 0.2723\n",
            "Epochs: 8 batch_loss: 0.2024\n",
            "Epochs: 8 batch_loss: 0.1015\n",
            "Epochs: 8 batch_loss: 0.2073\n",
            "Epochs: 8 batch_loss: 0.2865\n",
            "Epochs: 8 batch_loss: 0.2097\n",
            "Epochs: 8 batch_loss: 0.2018\n",
            "Epochs: 8 batch_loss: 0.1583\n",
            "Epochs: 8 batch_loss: 0.2672\n",
            "Epochs: 8 batch_loss: 0.2969\n",
            "Epochs: 8 batch_loss: 0.2240\n",
            "Epochs: 8 batch_loss: 0.4648\n",
            "Epochs: 8 batch_loss: 0.2576\n",
            "Epochs: 8 batch_loss: 0.1731\n",
            "Epochs: 8 batch_loss: 0.1798\n",
            "Epochs: 8 batch_loss: 0.3632\n",
            "Epochs: 8 batch_loss: 0.1713\n",
            "Epochs: 8 batch_loss: 0.1351\n",
            "Epochs: 8 batch_loss: 0.1435\n",
            "Epochs: 8 batch_loss: 0.1424\n",
            "Epochs: 8 batch_loss: 0.1905\n",
            "Epochs: 8 batch_loss: 0.2585\n",
            "Epochs: 8 batch_loss: 0.3349\n",
            "Epochs: 8 batch_loss: 0.1643\n",
            "Epochs: 8 batch_loss: 0.3249\n",
            "Epochs: 8 batch_loss: 0.2747\n",
            "Epochs: 8 batch_loss: 0.1088\n",
            "Epochs: 8 batch_loss: 0.1393\n",
            "Epochs: 8 batch_loss: 0.2371\n",
            "Epochs: 8 batch_loss: 0.2372\n",
            "Epochs: 8 batch_loss: 0.3824\n",
            "Epochs: 8 batch_loss: 0.1373\n",
            "Epochs: 8 batch_loss: 0.2051\n",
            "Epochs: 8 batch_loss: 0.1788\n",
            "Epochs: 8 batch_loss: 0.2344\n",
            "Epochs: 8 batch_loss: 0.2800\n",
            "Epochs: 8 batch_loss: 0.3993\n",
            "Epochs: 8 batch_loss: 0.2657\n",
            "Epochs: 8 batch_loss: 0.2857\n",
            "Epochs: 8 batch_loss: 0.3316\n",
            "Epochs: 8 batch_loss: 0.1737\n",
            "Epochs: 8 batch_loss: 0.1403\n",
            "Epochs: 8 batch_loss: 0.2748\n",
            "Epochs: 8 batch_loss: 0.3149\n",
            "Epochs: 8 batch_loss: 0.2294\n",
            "Epochs: 8 batch_loss: 0.3837\n",
            "Epochs: 8 batch_loss: 0.1446\n",
            "Epochs: 8 batch_loss: 0.1863\n",
            "Epochs: 8 batch_loss: 0.2299\n",
            "Epochs: 8 batch_loss: 0.3066\n",
            "Epochs: 8 batch_loss: 0.3073\n",
            "Epochs: 8 batch_loss: 0.3025\n",
            "Epochs: 8 batch_loss: 0.1861\n",
            "Epochs: 8 batch_loss: 0.2951\n",
            "Epochs: 8 batch_loss: 0.2430\n",
            "Epochs: 8 batch_loss: 0.1938\n",
            "Epochs: 8 batch_loss: 0.2560\n",
            "Epochs: 8 batch_loss: 0.2371\n",
            "Epochs: 8 batch_loss: 0.2450\n",
            "Epochs: 8 batch_loss: 0.3954\n",
            "Epochs: 8 batch_loss: 0.3184\n",
            "Epochs: 8 batch_loss: 0.2288\n",
            "Epochs: 8 batch_loss: 0.5132\n",
            "Epochs: 8 batch_loss: 0.2001\n",
            "Epochs: 8 batch_loss: 0.1848\n",
            "Epochs: 8 batch_loss: 0.3987\n",
            "Epochs: 8 batch_loss: 0.2281\n",
            "Epochs: 8 batch_loss: 0.2777\n",
            "Epochs: 8 batch_loss: 0.2630\n",
            "Epochs: 8 batch_loss: 0.2179\n",
            "Epochs: 8 batch_loss: 0.3285\n",
            "Epochs: 8 batch_loss: 0.3885\n",
            "Epochs: 8 batch_loss: 0.6475\n",
            "Epochs: 8 batch_loss: 0.3096\n",
            "Epochs: 8 batch_loss: 0.3405\n",
            "Epochs: 8 batch_loss: 0.3572\n",
            "Epochs: 8 batch_loss: 0.2492\n",
            "Epochs: 9 batch_loss: 0.1691\n",
            "Epochs: 9 batch_loss: 0.3363\n",
            "Epochs: 9 batch_loss: 0.2308\n",
            "Epochs: 9 batch_loss: 0.2488\n",
            "Epochs: 9 batch_loss: 0.1364\n",
            "Epochs: 9 batch_loss: 0.1874\n",
            "Epochs: 9 batch_loss: 0.2240\n",
            "Epochs: 9 batch_loss: 0.2199\n",
            "Epochs: 9 batch_loss: 0.1123\n",
            "Epochs: 9 batch_loss: 0.1922\n",
            "Epochs: 9 batch_loss: 0.1790\n",
            "Epochs: 9 batch_loss: 0.1889\n",
            "Epochs: 9 batch_loss: 0.1712\n",
            "Epochs: 9 batch_loss: 0.1690\n",
            "Epochs: 9 batch_loss: 0.1422\n",
            "Epochs: 9 batch_loss: 0.2306\n",
            "Epochs: 9 batch_loss: 0.1790\n",
            "Epochs: 9 batch_loss: 0.1336\n",
            "Epochs: 9 batch_loss: 0.2406\n",
            "Epochs: 9 batch_loss: 0.2572\n",
            "Epochs: 9 batch_loss: 0.1009\n",
            "Epochs: 9 batch_loss: 0.2538\n",
            "Epochs: 9 batch_loss: 0.1022\n",
            "Epochs: 9 batch_loss: 0.1758\n",
            "Epochs: 9 batch_loss: 0.1599\n",
            "Epochs: 9 batch_loss: 0.1683\n",
            "Epochs: 9 batch_loss: 0.1578\n",
            "Epochs: 9 batch_loss: 0.1726\n",
            "Epochs: 9 batch_loss: 0.1634\n",
            "Epochs: 9 batch_loss: 0.1417\n",
            "Epochs: 9 batch_loss: 0.2266\n",
            "Epochs: 9 batch_loss: 0.1335\n",
            "Epochs: 9 batch_loss: 0.1582\n",
            "Epochs: 9 batch_loss: 0.2865\n",
            "Epochs: 9 batch_loss: 0.0916\n",
            "Epochs: 9 batch_loss: 0.1550\n",
            "Epochs: 9 batch_loss: 0.2228\n",
            "Epochs: 9 batch_loss: 0.2011\n",
            "Epochs: 9 batch_loss: 0.1475\n",
            "Epochs: 9 batch_loss: 0.1742\n",
            "Epochs: 9 batch_loss: 0.2753\n",
            "Epochs: 9 batch_loss: 0.2907\n",
            "Epochs: 9 batch_loss: 0.1723\n",
            "Epochs: 9 batch_loss: 0.1823\n",
            "Epochs: 9 batch_loss: 0.1757\n",
            "Epochs: 9 batch_loss: 0.2140\n",
            "Epochs: 9 batch_loss: 0.1665\n",
            "Epochs: 9 batch_loss: 0.0725\n",
            "Epochs: 9 batch_loss: 0.2080\n",
            "Epochs: 9 batch_loss: 0.1553\n",
            "Epochs: 9 batch_loss: 0.1422\n",
            "Epochs: 9 batch_loss: 0.2723\n",
            "Epochs: 9 batch_loss: 0.1296\n",
            "Epochs: 9 batch_loss: 0.3722\n",
            "Epochs: 9 batch_loss: 0.1643\n",
            "Epochs: 9 batch_loss: 0.3054\n",
            "Epochs: 9 batch_loss: 0.1595\n",
            "Epochs: 9 batch_loss: 0.3946\n",
            "Epochs: 9 batch_loss: 0.2792\n",
            "Epochs: 9 batch_loss: 0.1554\n",
            "Epochs: 9 batch_loss: 0.1798\n",
            "Epochs: 9 batch_loss: 0.3908\n",
            "Epochs: 9 batch_loss: 0.3472\n",
            "Epochs: 9 batch_loss: 0.1344\n",
            "Epochs: 9 batch_loss: 0.2004\n",
            "Epochs: 9 batch_loss: 0.2127\n",
            "Epochs: 9 batch_loss: 0.4071\n",
            "Epochs: 9 batch_loss: 0.2346\n",
            "Epochs: 9 batch_loss: 0.1521\n",
            "Epochs: 9 batch_loss: 0.1832\n",
            "Epochs: 9 batch_loss: 0.3229\n",
            "Epochs: 9 batch_loss: 0.1735\n",
            "Epochs: 9 batch_loss: 0.1573\n",
            "Epochs: 9 batch_loss: 0.2399\n",
            "Epochs: 9 batch_loss: 0.2282\n",
            "Epochs: 9 batch_loss: 0.2336\n",
            "Epochs: 9 batch_loss: 0.2399\n",
            "Epochs: 9 batch_loss: 0.1827\n",
            "Epochs: 9 batch_loss: 0.2109\n",
            "Epochs: 9 batch_loss: 0.2506\n",
            "Epochs: 9 batch_loss: 0.2935\n",
            "Epochs: 9 batch_loss: 0.2744\n",
            "Epochs: 9 batch_loss: 0.2560\n",
            "Epochs: 9 batch_loss: 0.1376\n",
            "Epochs: 9 batch_loss: 0.2600\n",
            "Epochs: 9 batch_loss: 0.1337\n",
            "Epochs: 9 batch_loss: 0.3205\n",
            "Epochs: 9 batch_loss: 0.1767\n",
            "Epochs: 9 batch_loss: 0.1457\n",
            "Epochs: 9 batch_loss: 0.1764\n",
            "Epochs: 9 batch_loss: 0.2129\n",
            "Epochs: 9 batch_loss: 0.1087\n",
            "Epochs: 9 batch_loss: 0.2956\n",
            "Epochs: 9 batch_loss: 0.2636\n",
            "Epochs: 9 batch_loss: 0.4058\n",
            "Epochs: 9 batch_loss: 0.3141\n",
            "Epochs: 9 batch_loss: 0.2305\n",
            "Epochs: 9 batch_loss: 0.1659\n",
            "Epochs: 9 batch_loss: 0.2040\n",
            "Epochs: 9 batch_loss: 0.2602\n",
            "Epochs: 9 batch_loss: 0.4675\n",
            "Epochs: 9 batch_loss: 0.3561\n",
            "Epochs: 9 batch_loss: 0.1558\n",
            "Epochs: 9 batch_loss: 0.2595\n",
            "Epochs: 9 batch_loss: 0.2442\n",
            "Epochs: 9 batch_loss: 0.1785\n",
            "Epochs: 9 batch_loss: 0.1975\n",
            "Epochs: 9 batch_loss: 0.1969\n",
            "Epochs: 9 batch_loss: 0.2670\n",
            "Epochs: 9 batch_loss: 0.1873\n",
            "Epochs: 9 batch_loss: 0.2223\n",
            "Epochs: 9 batch_loss: 0.1316\n",
            "Epochs: 9 batch_loss: 0.1582\n",
            "Epochs: 9 batch_loss: 0.2149\n",
            "Epochs: 9 batch_loss: 0.1975\n",
            "Epochs: 9 batch_loss: 0.4503\n",
            "Epochs: 9 batch_loss: 0.1815\n",
            "Epochs: 9 batch_loss: 0.2316\n",
            "Epochs: 9 batch_loss: 0.3124\n",
            "Epochs: 9 batch_loss: 0.2262\n",
            "Epochs: 9 batch_loss: 0.2423\n",
            "Epochs: 9 batch_loss: 0.2899\n",
            "Epochs: 9 batch_loss: 0.1544\n",
            "Epochs: 9 batch_loss: 0.1340\n",
            "Epochs: 9 batch_loss: 0.3679\n",
            "Epochs: 9 batch_loss: 0.1839\n",
            "Epochs: 9 batch_loss: 0.4941\n",
            "Epochs: 9 batch_loss: 0.2184\n",
            "Epochs: 9 batch_loss: 0.3280\n",
            "Epochs: 9 batch_loss: 0.2309\n",
            "Epochs: 9 batch_loss: 0.2949\n",
            "Epochs: 9 batch_loss: 0.2104\n",
            "Epochs: 9 batch_loss: 0.1786\n",
            "Epochs: 9 batch_loss: 0.2222\n",
            "Epochs: 9 batch_loss: 0.3640\n",
            "Epochs: 9 batch_loss: 0.3745\n",
            "Epochs: 9 batch_loss: 0.3300\n",
            "Epochs: 9 batch_loss: 0.3141\n",
            "Epochs: 9 batch_loss: 0.2954\n",
            "Epochs: 9 batch_loss: 0.1294\n",
            "Epochs: 9 batch_loss: 0.2067\n",
            "Epochs: 9 batch_loss: 0.3007\n",
            "Epochs: 9 batch_loss: 0.2929\n",
            "Epochs: 9 batch_loss: 0.3072\n",
            "Epochs: 9 batch_loss: 0.2086\n",
            "Epochs: 9 batch_loss: 0.3350\n",
            "Epochs: 9 batch_loss: 0.3998\n",
            "Epochs: 9 batch_loss: 0.2228\n",
            "Epochs: 9 batch_loss: 0.3132\n",
            "Epochs: 9 batch_loss: 0.2981\n",
            "Epochs: 9 batch_loss: 0.3405\n",
            "Epochs: 9 batch_loss: 0.2391\n",
            "Epochs: 9 batch_loss: 0.3130\n",
            "Epochs: 9 batch_loss: 0.2103\n",
            "Epochs: 9 batch_loss: 0.2295\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjVwV_waXH0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to reload wights\n",
        "\n",
        "encoder.load_weights('encoder.h5')\n",
        "decoder.load_weights('decoder.h5')"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfB2kwYLx4Fe",
        "colab_type": "text"
      },
      "source": [
        "In order to perform the translation, we need to write a function much like what we did in the train_step function but instead of feeding in the actual word at a prior time step into the next time step, we feed in the word predicted by our network. This algorithm is known as Greedy search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuswAWMYqdME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence, max_length=10):\n",
        "    result = ''\n",
        "    attention_plot = np.zeros((10,10))\n",
        "    sentence = normalizeString(sentence)\n",
        "    sentence = sentencetoIndexes(sentence, input_lang)\n",
        "    sentence = keras.preprocessing.sequence.pad_sequences([sentence],padding='post',\n",
        "                                                      maxlen=max_length, truncating='post')\n",
        "    \n",
        "    encoder_hidden = hidden = [tf.zeros((1, 256))]\n",
        "    \n",
        "    enc_out, enc_hidden = encoder(sentence, encoder_hidden)\n",
        "    \n",
        "    dec_hidden = enc_hidden\n",
        "    SOS_tensor = np.array([SOS_token])\n",
        "    dec_input = tf.squeeze(tf.expand_dims([SOS_tensor], 1), -1)\n",
        "    \n",
        "    for tx in range(max_length):\n",
        "        dec_out, dec_hidden, attn_weights = decoder(dec_input,\n",
        "                                                   dec_hidden, enc_out)\n",
        "        attn_weights = tf.reshape(attn_weights, (-1, ))\n",
        "        attention_plot[tx] = attn_weights.numpy()\n",
        "        pred = tf.argmax(dec_out, axis=1).numpy()\n",
        "        result += output_lang.int2word[pred[0]] + \" \"\n",
        "        if output_lang.int2word[pred[0]] == \"EOS\":\n",
        "            break\n",
        "        dec_input = tf.expand_dims(pred, axis=1)\n",
        "    return result, attention_plot"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7mmzYUIqdMI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3cd1f721-1950-4749-ffb2-8f0ba64a30c0"
      },
      "source": [
        "sentence = \"Faites-moi savoir quand vous avez terminé avec le nettoyant.\"\n",
        "pred, attn_weights = translate(sentence)\n",
        "print(pred)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i m going to know when you re done . \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNNN56gvpVlN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "2977d466-a453-49e8-fd29-9410f267c17d"
      },
      "source": [
        "french_test = []\n",
        "with open(\"french_test.txt.fr\",\"r\") as text:\n",
        "    Line = text.readline()\n",
        "    \n",
        "\n",
        "    while Line!='':\n",
        "        Line = Line.strip(\"\\n\")\n",
        "        french_test.append(Line)\n",
        "        Line = text.readline()\n",
        "\n",
        "french_test[:10]"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Reprise de la session',\n",
              " 'Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.',\n",
              " 'Comme vous avez pu le constater, le grand \"bogue de l\\'an 2000\" ne s\\'est pas produit. En revanche, les citoyens d\\'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.',\n",
              " 'Vous avez souhaité un débat à ce sujet dans les prochains jours, au cours de cette période de session.',\n",
              " \"En attendant, je souhaiterais, comme un certain nombre de collègues me l'ont demandé, que nous observions une minute de silence pour toutes les victimes, des tempêtes notamment, dans les différents pays de l'Union européenne qui ont été touchés.\",\n",
              " 'Je vous invite à vous lever pour cette minute de silence.',\n",
              " '(Le Parlement, debout, observe une minute de silence)',\n",
              " \"Madame la Présidente, c'est une motion de procédure.\",\n",
              " 'Vous avez probablement appris par la presse et par la télévision que plusieurs attentats à la bombe et crimes ont été perpétrés au Sri Lanka.',\n",
              " \"L'une des personnes qui vient d'être assassinée au Sri Lanka est M. Kumar Ponnambalam, qui avait rendu visite au Parlement européen il y a quelques mois à peine.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6gpplMh1NCL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "5174bab7-7402-4e03-acfc-30209b59e5fb"
      },
      "source": [
        "english_test = []\n",
        "with open(\"eng_test1.txt.en\",\"r\") as text:\n",
        "    Line = text.readline()\n",
        "    \n",
        "\n",
        "    while Line!='':\n",
        "        Line = Line.strip(\"\\n\")\n",
        "        english_test.append(Line)\n",
        "        Line = text.readline()\n",
        "\n",
        "english_test[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Resumption of the session',\n",
              " 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.',\n",
              " \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\",\n",
              " 'You have requested a debate on this subject in the course of the next few days, during this part-session.',\n",
              " \"In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\",\n",
              " \"Please rise, then, for this minute' s silence.\",\n",
              " \"(The House rose and observed a minute' s silence)\",\n",
              " 'Madam President, on a point of order.',\n",
              " 'You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.',\n",
              " 'One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDd_iBiQu_Nl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "0946239d-922e-4503-9bd4-68aa7cca93fa"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4hGB9c3Q9b8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "french_test=[\"Pourquoi ne commencez-vous pas à courir avec moi ?\",\"faisons la fête ce soir parce que c'est dimanche demain .\",\"Restez en contact avec moi jusqu'à ce que vous trouviez un autre endroit où vivre .\",\"Faites-moi savoir quand vous avez terminé avec le nettoyant .\",\"quand tu vas au marche ?\"]\n",
        "english_test=[\"Why don't you start running with me ?\",\"let's party tonight because it's sunday tomorrow .\",\"Keep in touch with me until you find another place to live .\",\"Let me know when you are done with the cleaner .\",\"when will you go to the market .\"]"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYQnEZZKsjNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokenized_english_test = [word_tokenize(i) for i in english_test]"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opLnPhUR3kym",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a25ad31c-0bd6-49e5-ae12-1d7cb7391091"
      },
      "source": [
        "tokenized_english_test[0]"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Why', 'do', \"n't\", 'you', 'start', 'running', 'with', 'me', '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7QnuOP35A3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sentence = \"j'ai besoin de quelqu'un pour m'aider ?\"\n",
        "pred_list=[]\n",
        "for i in french_test:\n",
        "    pred, attn_weights = translate(i)\n",
        "    pred_list.append(pred)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwjosUFG6tWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_pred = [word_tokenize(i) for i in pred_list]"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iNL-HiQMRA-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "2b5d9c50-182c-4a55-aba5-6611e30aec64"
      },
      "source": [
        "#Average Bleu score for a document\n",
        "import nltk\n",
        "\n",
        "total_Bleu_score=0\n",
        "for i in range(len(tokenized_english_test)):\n",
        "  hypothesis = tokenized_pred[i]\n",
        "  reference = tokenized_english_test[i]\n",
        "  BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, weights = (0.5, 0.5))\n",
        "  total_Bleu_score += BLEUscore\n",
        "print(total_Bleu_score/len(tokenized_english_test))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2629825084484026\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efRbcnkXWaA7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fc6a6471-cc59-48f9-e8a4-97d8a0075cba"
      },
      "source": [
        "sentence= \"j'ai besoin de quelqu'un pour m'aider ?\"\n",
        "pred, attn_weights = translate(sentence)\n",
        "print(pred)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i need someone to help me . EOS \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s9cnSvUxNIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    sentence = normalizeString(sentence)\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5HzDV3NTy6A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 858
        },
        "outputId": "751f2f9f-a4c6-4ce5-edec-a2e33a9e67ed"
      },
      "source": [
        "plot_attention(attn_weights, \"j'ai besoin de quelqu'un pour m'aider ?\", \"i need someone to help me .\")"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-0ff8079400bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"j'ai besoin de quelqu'un pour m'aider ?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i need someone to help me .\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-84-e2eb4a2889fa>\u001b[0m in \u001b[0;36mplot_attention\u001b[0;34m(attention, sentence, predicted_sentence)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfontdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'fontsize'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfontdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_yticklabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpredicted_sentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfontdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"str\") to list"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le3t_OqmT0FZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}