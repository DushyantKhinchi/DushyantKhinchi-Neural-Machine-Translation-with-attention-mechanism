{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "machine_translation_tensorflow_orig.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DushyantKhinchi/DushyantKhinchi-Neural-Machine-Translation-with-attention-mechanism/blob/master/machine_translation_tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "En0l2TgqVWTH",
        "colab_type": "text"
      },
      "source": [
        "# Neural machine translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "su00atkqqzjZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "9e56691b-f17e-4d1d-a047-107d441f895f"
      },
      "source": [
        "!pip install -q tensorflow==2.0.0-beta1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 87.9MB 44kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.1MB 48.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 501kB 51.0MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IF3LjvXqdJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import unicodedata\n",
        "import string\n",
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUoGvxNddFHh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "f69a80c5-7164-4e77-d8cd-f47e23e87fad"
      },
      "source": [
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44g54wKWqdJp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a3ceb3f-ae7b-4d11-d72f-eeb395b49bdb"
      },
      "source": [
        "keras = tf.keras\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEed-KleNdUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !wget https://www.manythings.org/anki/fra-eng.zip\n",
        "# !unzip  fra-eng.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHgfCZXVqdJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vMM4YwkwtKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"gdrive/My Drive/Machine Translation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqyPjc3iwobG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "77235fc4-54b6-4754-fb19-7fa5d95d00c6"
      },
      "source": [
        "with open(\"fra.txt\",'r') as f:\n",
        "        lines = f.readlines()\n",
        "for i in range(0,10):\n",
        "        print(lines[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go.\tVa !\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)\n",
            "\n",
            "Hi.\tSalut !\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)\n",
            "\n",
            "Hi.\tSalut.\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux)\n",
            "\n",
            "Run!\tCours !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906331 (sacredceltic)\n",
            "\n",
            "Run!\tCourez !\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #906332 (sacredceltic)\n",
            "\n",
            "Who?\tQui ?\tCC-BY 2.0 (France) Attribution: tatoeba.org #2083030 (CK) & #4366796 (gillux)\n",
            "\n",
            "Wow!\tÇa alors !\tCC-BY 2.0 (France) Attribution: tatoeba.org #52027 (Zifre) & #374631 (zmoo)\n",
            "\n",
            "Fire!\tAu feu !\tCC-BY 2.0 (France) Attribution: tatoeba.org #1829639 (Spamster) & #4627939 (sacredceltic)\n",
            "\n",
            "Help!\tÀ l'aide !\tCC-BY 2.0 (France) Attribution: tatoeba.org #435084 (lukaszpp) & #128430 (sysko)\n",
            "\n",
            "Jump.\tSaute.\tCC-BY 2.0 (France) Attribution: tatoeba.org #631038 (Shishir) & #2416938 (Phoenix)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJav2p1yzNJC",
        "colab_type": "text"
      },
      "source": [
        "This class below will map words in a particular language to the index.\n",
        "This class has three dictionaries data structures.\n",
        "\n",
        "word2int: to map each word to a unique index integer\n",
        "int2word: to map index integer to the word\n",
        "word2count: to map a word to its total count in the corpus \n",
        "\n",
        "We have three functions in our class\n",
        "\n",
        "\n",
        "1)  ***addWord()*** function just adds a word as a key to the word2int dictionary with its corresponding index value. The opposite is done for the int2word dictionary. It also keep track of how many times we’ve come across a word when parsing our corpus to populate the class dictionaries and if we’ve already come across  a particular word, we desist from adding it to the word2int and int2word dictionaries and instead keep track of how many times it appears in our corpus using the word2count dictionary.\n",
        "\n",
        "\n",
        "\n",
        "2) ***addSentence()*** does is to simply iterate through each sentence and for each sentence, it splits the sentence into words and implements the addWord function on each word in each sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSK6zPRUqdJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lang(object):\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2int = {}#map words to integers\n",
        "        self.word2count = {} #map words to their count in the corpus/ how many times a word is repeated in the corpus?\n",
        "        self.int2word = {0 : \"SOS\", 1 : \"EOS\"} # map integers to tokens. just the opposite of word2int but has some initial values. EOS means End of Sentence token \n",
        "                                               # used to indicate the end of a sentence. Every sentence is going to have an EOS token. SOS means Start of Sentence token and is used to indicate the start of a sentence.)\n",
        "        self.n_words = 2\n",
        "        \n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2int:\n",
        "            self.word2int[word] = self.n_words \n",
        "            self.word2count[word] = 1 \n",
        "            self.int2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "            \n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "    \n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(\" \"):\n",
        "            self.addWord(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsXg0mRxWQ5s",
        "colab_type": "text"
      },
      "source": [
        "Corpus consists of French words which may have some characters like ‘Ç’. For simplicity sake, we convert them into their normal corresponding ASCII characters(Ç → C). \n",
        "\n",
        "Also we create white spaces between words and punctuation attached to these words. (hello’s → hello s). This is to ensure that the presence of a punctuation does not create two words for a particular word (For example, different integers would be assign to “they’re” and \"they are\" although they have the same maeaning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UujikttqdJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicodeToAscii(s):\n",
        "    return \"\".join(c for c in unicodedata.normalize(\"NFD\", s) \\\n",
        "                   if unicodedata.category(c) != \"Mn\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb0sLusAqdKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    \n",
        "    s = re.sub(r\"([!.?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z?.!]+\", \" \", s)  #re.sub() function is used to replace occurrences of a particular sub-string with another sub-string.\n",
        "    return s\n",
        "\n",
        "    #When the ^ character appears outside of [] matches the beginning of the line (or string). \n",
        "    #When the ^ character appears inside the [], it matches any character not appearing inside the []."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRVyyGNUcoBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# re.sub(r'(\\b[a-z]+) \\1', r'\\1', 'cat in the the hat')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bbmjKl-fUQo",
        "colab_type": "text"
      },
      "source": [
        "Two helper functions are combined under ***load_dataset()*** to load the dataset as a list of list containing the pair of sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbvFe0iqqdKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset():\n",
        "    with open(\"fra.txt\",'r') as f:\n",
        "        lines = f.readlines()\n",
        "    \n",
        "    pairs = [[normalizeString(pair) for pair in \n",
        "              line.strip().split('\\t')] for line in lines]\n",
        "    return pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26fM5LRUqdKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pairs = load_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufcihjlpf_-D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "outputId": "f77ce723-ea29-4ebf-ea87-99a45dfd5382"
      },
      "source": [
        "pairs[0:10] #list of list containing the pair of sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['go .', 'va !', 'cc by . france attribution tatoeba .org cm wittydev '],\n",
              " ['hi .', 'salut !', 'cc by . france attribution tatoeba .org cm aiji '],\n",
              " ['hi .', 'salut .', 'cc by . france attribution tatoeba .org cm gillux '],\n",
              " ['run !',\n",
              "  'cours !',\n",
              "  'cc by . france attribution tatoeba .org papabear sacredceltic '],\n",
              " ['run !',\n",
              "  'courez !',\n",
              "  'cc by . france attribution tatoeba .org papabear sacredceltic '],\n",
              " ['who ?', 'qui ?', 'cc by . france attribution tatoeba .org ck gillux '],\n",
              " ['wow !',\n",
              "  'ca alors !',\n",
              "  'cc by . france attribution tatoeba .org zifre zmoo '],\n",
              " ['fire !',\n",
              "  'au feu !',\n",
              "  'cc by . france attribution tatoeba .org spamster sacredceltic '],\n",
              " ['help !',\n",
              "  'a l aide !',\n",
              "  'cc by . france attribution tatoeba .org lukaszpp sysko '],\n",
              " ['jump .',\n",
              "  'saute .',\n",
              "  'cc by . france attribution tatoeba .org shishir phoenix ']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyWBlfA8jbPP",
        "colab_type": "text"
      },
      "source": [
        "To reduce the training time, we are going to filter out our dataset to remove sentences with more than ten words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnmGqtDOqdKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Iterates through a sentence, breaks it into words and maps the word to its corresponding integer\n",
        "value using the word2int dictionary which we implemented in the language class\n",
        "\n",
        "\"\"\"\n",
        "MAX_LENGTH = 10\n",
        "def sentencetoIndexes(sentence, lang):\n",
        "    indexes = [lang.word2int[word] for word in sentence.split()]\n",
        "    indexes.append(EOS_token)\n",
        "    return indexes\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split()) < MAX_LENGTH and \\\n",
        "len(p[1].split()) < MAX_LENGTH\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "pairs = filterPairs(pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-yqR45pwMea",
        "colab_type": "text"
      },
      "source": [
        "To populate the word2int dictionary in each Language class with words and assign a corresponding  integer to each word in a new function. Also since we are going to batch our dataset, we are going to apply padding to sentences with words less than the maximum length we proposed. Let’s see how to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlAGPZShqdKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_lang(lang1, lang2, max_length=10):\n",
        "    input_lang = Lang(lang1)\n",
        "    output_lang = Lang(lang2)\n",
        "    input_seq = []\n",
        "    output_seq = []\n",
        "    \n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[1])  #french\n",
        "        output_lang.addSentence(pair[0]) #english\n",
        "    for pair in pairs:\n",
        "        input_seq.append(sentencetoIndexes(pair[1], input_lang))\n",
        "        output_seq.append(sentencetoIndexes(pair[0], output_lang))\n",
        "    return keras.preprocessing.sequence.pad_sequences(input_seq, maxlen=max_length, padding='post',\n",
        "                                                      truncating='post'), \\\n",
        "keras.preprocessing.sequence.pad_sequences(output_seq, padding='post', truncating='post'), input_lang, output_lang"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9MAxjU-qdKY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor, output_tensor, input_lang, output_lang = build_lang('fr', 'en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ8Pcm1Ls8Cc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "119b29fb-c7e4-4046-e844-59b0060204eb"
      },
      "source": [
        "input_lang.name"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fr'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPSDwq71ujqZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "06332784-dcd5-40b2-e970-8c0b454c5722"
      },
      "source": [
        "print(\"input_tensor at index 10: {}\".format(input_tensor[10]))\n",
        "print(\"output_tensor at index 10: {}\".format(output_tensor[10]))\n",
        "print(\"corresponding integer value for 'nous' {}\".format(input_lang.word2int['nous']))\n",
        "print(\"corresponding integer value for 'she' {}\".format(output_lang.word2int['she']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_tensor at index 10: [10 18  3  1  0  0  0  0  0  0]\n",
            "output_tensor at index 10: [13  6  1  0  0  0  0  0  0  0]\n",
            "corresponding integer value for 'nous' 98\n",
            "corresponding integer value for 'she' 194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp2j_RaPQpZv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b212787b-1e35-4908-b20c-161d5aa81473"
      },
      "source": [
        "output_tensor.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(123935, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qii6d6iKqdKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating batches of size 16\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "BUFFER_SIZE = len(input_tensor)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor, output_tensor)).shuffle(BUFFER_SIZE) #maintains a buffer of size \n",
        "dataset = dataset.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7IzgHTAVJh6",
        "colab_type": "text"
      },
      "source": [
        "# Seq2Seq model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9tu3s7KV1rd",
        "colab_type": "text"
      },
      "source": [
        "There are several architectures of Recurrent Neural Networks, each suited for a particular group of tasks. For examples, many-to-one architecture for task such as sentiment analysis and one-to-many for music generation but we are going to employ the many-to-many architecture which is suited for tasks such as chat-bots and Neural Machine Translation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QC26fd2YLQ4",
        "colab_type": "text"
      },
      "source": [
        "The class of RNN used here is called Sequence to Sequence model. This network is a combination of an encoder and a decoder. One to take in the input sentence and the other to translate into another language in the case of machine translation. This architecture can effectively handle the tasks where inputs and outputs have different lengths. Encoder feed fixed length vector to the decoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i10ZBFOXez6f",
        "colab_type": "text"
      },
      "source": [
        "The only information that the decoder receives from the encoder is the hidden state. The hidden state is a fixed size vector into which the encoder squeezes every bit of information about the input sentence and passes it on to the decoder to generate the output sentence. This might work fairly well for shorter sentences but this fixed size vector tends to become a bottleneck for longer sentences. This is where attention mechanism becomes a crucial part of our translation network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj1DRDcp3LUb",
        "colab_type": "text"
      },
      "source": [
        "Encoder unit is made up of two layers: the Embedding layer which converts each token into a dense representation and a Recurrent Network layer(here Gate Recurrent Unit Network has been used).\n",
        "\n",
        "The two very important parameters in the GRU implementation: **return_sequences** and **return_state**. return_sequences ensures that the GRU outputs the hidden state of each time step which We need to access information about each word in the input sequence during the computation of the attention weights and the context vector. Additionally, Return state returns the hidden state of the last time step. We need this tensor to be used as initial hidden state for the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41nSXuflqdKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(keras.models.Model):\n",
        "    def __init__(self, vocab_size, num_hidden=256, num_embedding=256, batch_size=16):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.num_hidden = num_hidden\n",
        "        self.num_embedding = num_embedding\n",
        "        self.embedding = keras.layers.Embedding(vocab_size, num_embedding)\n",
        "        self.gru = keras.layers.GRU(num_hidden, return_sequences=True,\n",
        "                                    recurrent_initializer='glorot_uniform',\n",
        "                                   return_state=True)\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        embedded = self.embedding(x)\n",
        "        rnn_out, hidden = self.gru(embedded, initial_state=hidden)\n",
        "        return rnn_out, hidden\n",
        "    def init_hidden(self):\n",
        "        return tf.zeros(shape=(self.batch_size, self.num_hidden))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ9lsXnV7cvK",
        "colab_type": "text"
      },
      "source": [
        "In the call function where forward propagation of our network is implemented. we first pass the input tensor through an embedding layer and then through a GRU layer. This returns the RNN output(rnn_out) of shape (batch_size, max_sequence length, hidden_size) and hidden state of shape (batch_size, hidden_size)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qm8zLgoqdKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs, outputs = next(iter(dataset))\n",
        "hidden = tf.zeros((16, 256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5AHvxMiqdKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(input_lang.n_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCF-S8fBqdKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "e_outputs, e_hidden = encoder(inputs, hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqGefguVqdKv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "c8aff630-2ce0-4e64-f25b-f38bc8c09fbe"
      },
      "source": [
        "e_hidden"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(16, 256), dtype=float32, numpy=\n",
              "array([[ 2.1840066e-02,  3.8554348e-04, -1.1842080e-03, ...,\n",
              "        -2.9822711e-02,  1.4798867e-02,  3.5127299e-03],\n",
              "       [ 2.2661942e-04,  9.6484497e-03, -8.0569815e-03, ...,\n",
              "        -1.4858230e-02, -6.5956177e-04, -1.0904243e-02],\n",
              "       [ 2.1884400e-02,  1.4553041e-03,  5.0750683e-04, ...,\n",
              "        -3.1709541e-02,  1.4247962e-02,  5.2800681e-03],\n",
              "       ...,\n",
              "       [ 1.9097811e-02,  2.2428753e-03, -5.0719833e-04, ...,\n",
              "        -2.9227473e-02,  8.0375969e-03,  1.2690941e-03],\n",
              "       [ 2.0666702e-02, -2.3495229e-03,  1.1601836e-03, ...,\n",
              "        -3.2917775e-02,  1.1977090e-02,  3.0316033e-03],\n",
              "       [ 2.2430029e-02, -6.7723880e-04, -4.3382006e-05, ...,\n",
              "        -3.4074042e-02,  1.6565433e-02,  7.0250207e-03]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGGzlvzHqXFT",
        "colab_type": "text"
      },
      "source": [
        "Decoder layer = Attention layer + fully connected layer\n",
        "\n",
        "Attention layer is designed to return the context vector and the attention weights. In the attention layer The **call** function where forward propagation takes place takes in two parameters; **encoder_out** which represents all the hidden states at each timestep in the encoder and **hidden** which represents the hidden state of the decoder before the current timestep where we are generating the correct word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21bfPvySsBpw",
        "colab_type": "text"
      },
      "source": [
        "We add a dimension of size 1 to represent the timestep hence shape of **hidden** becomes (batch_size, time_step, hidden_size). Score variable stores attention weights which is a function of the hidden state immediately before that timestep(hidden) in the decoder and all the hidden states (encoder_out) of the encoder. Then we've applied softmax to score across the max_length dimension which is at axis=1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bJHJsq1qdK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(keras.models.Model):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "    \n",
        "        self.W1 = keras.layers.Dense(units)\n",
        "        self.W2 = keras.layers.Dense(units)\n",
        "        self.V = keras.layers.Dense(1)\n",
        "        \n",
        "    def call(self, encoder_out, hidden):\n",
        "        #shape of encoder_out : batch_size, seq_length, hidden_dim \n",
        "        #shape of encoder_hidden : batch_size, hidden_dim \n",
        "        \n",
        "        hidden = tf.expand_dims(hidden, axis=1) #out: (16, 1, 1024)\n",
        "        \n",
        "        score = self.V(tf.nn.tanh(self.W1(encoder_out) + \\\n",
        "                                  self.W2(hidden))) #out: (16, 10, 1)\n",
        "\n",
        "        \n",
        "        attn_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        context =  attn_weights * encoder_out  #out: ((16,10,1) * (16,10,1024))=16, 10, 1024\n",
        "        context = tf.reduce_sum(context, axis=1) #out: 16, 1024\n",
        "        return context, attn_weights\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwvMXJToz_qv",
        "colab_type": "text"
      },
      "source": [
        "The end product of the softmax function gives us the weights which we multiply with all the hidden states from the encoder. A hidden state at a particular timestep with a bigger weight value means more attention is being paid on the word at that timestep.\n",
        "\n",
        "reduce_sum will produce the context vector. After multiplying each hidden_state with its corresponding weight, we combine all resultant values through a summation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gRC60cCqdK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attn = BahdanauAttention(256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHU6rIRzqdLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context, attn_weights = attn(e_outputs, e_hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2XacZYkqdLK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2c7da419-c961-474d-f4ab-3b676f6893ff"
      },
      "source": [
        "attn_weights.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([16, 10, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4BnFHyDk1nx",
        "colab_type": "text"
      },
      "source": [
        "Under the Decoder class\n",
        "\n",
        "x: tensor of a single word\n",
        "x is passed through an embedding layer that maps the single integer token into a dense 256 dimensional vector. That vector is later concatenated with the context vector generated by attention layer. The resultant tensor becomes the input for the Gated Recurrent Network for a single timestep.\n",
        "\n",
        "Finally we pass the output of the GRU through a fully connected layer which outputs a vector of size (batch_size, number of english words).\n",
        "\n",
        "We also return hidden state to be fed into the next timestep and the attention weights for later visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsLImrCwqdLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(keras.models.Model):\n",
        "    def __init__(self, vocab_size, dec_dim=256, embedding_dim=256):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.attn = BahdanauAttention(dec_dim)\n",
        "        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = keras.layers.GRU(dec_dim, recurrent_initializer='glorot_uniform',\n",
        "                                   return_sequences=True, return_state=True)\n",
        "        self.fc = keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x, enc_hidden, enc_out):\n",
        "        x = self.embedding(x)\n",
        "        context, attn_weights = self.attn(enc_out, enc_hidden)\n",
        "        x = tf.concat((tf.expand_dims(context, 1), x), -1)\n",
        "        r_out, hidden = self.gru(x, initial_state=enc_hidden)\n",
        "        out = tf.reshape(r_out,shape=(-1, r_out.shape[2]))\n",
        "        return self.fc(out), hidden, attn_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItYHAqWJqdLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder = Decoder(output_lang.n_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnhNtfb9qdLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor, output_tensor = next(iter(dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7Y7QsykqdLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.expand_dims(output_tensor[:,1], -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At4TcH8NoX6c",
        "colab_type": "text"
      },
      "source": [
        "Next step is to build a training pipeline. \n",
        "\n",
        "Starting with our loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaWOV6fdpod-",
        "colab_type": "text"
      },
      "source": [
        "keras’s sparse categorical cross entropy module has been used since we have a large number of categories(number of english words). We create a mask that asserts that the padding tokens are not included in calculating the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DE7fnvAqdLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(real, pred):\n",
        "    criterion = keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                           reduction='none')\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0)) #mask asserts that the padding tokens are not included in calculating the loss.\n",
        "    _loss = criterion(real, pred)\n",
        "    mask = tf.cast(mask, dtype=_loss.dtype)\n",
        "    _loss *= mask\n",
        "    return tf.reduce_mean(_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMc7zD6WqdLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.optimizers.Adam()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM9heXPLrSFm",
        "colab_type": "text"
      },
      "source": [
        "**Building a training pipeline**\n",
        "\n",
        "\n",
        "The following snippet implements a single training step. In which we pass the input_tensor which represent the input sentence through the forward propagation pipeline of the Encoder. This return the enc_output(hidden_state of all timesteps) and enc_hidden(last hidden_state).\n",
        "\n",
        "In the decoding part, we use a technique called teacher forcing where instead of using the predicted word as input for the next timestep, we use the actual word.\n",
        "\n",
        "\n",
        "At the start of decoding, we feed the Start Of Sentence token as input and maximize the probability of the decoder predicting the first word in the output sequence as it output. We then take the actual first word and feed it into the second timestep and maximize the probability of the decoder predicting the second word in the output sequence as it output. This continues sequentially until we reach the End of Sentence token<EOS>. We accumulate all the losses, derive the gradients and train both networks end-to-end with the gradients. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiAmynEsqdLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(input_tensor, target_tensor, enc_hidden):\n",
        "    loss = 0.0\n",
        "    with tf.GradientTape() as tape:\n",
        "    \n",
        "        batch_size = input_tensor.shape[0]\n",
        "        enc_output, enc_hidden = encoder(input_tensor, enc_hidden)\n",
        "\n",
        "        SOS_tensor = np.array([SOS_token])\n",
        "        dec_input = tf.squeeze(tf.expand_dims([SOS_tensor]*batch_size, 1), -1)\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        for tx in range(target_tensor.shape[1]-1):\n",
        "          \n",
        "            dec_out, dec_hidden, _ = decoder(dec_input, dec_hidden,\n",
        "                                            enc_output)\n",
        "            loss += loss_fn(target_tensor[:, tx], dec_out)\n",
        "            dec_input = tf.expand_dims(target_tensor[:, tx], 1)\n",
        "\n",
        "    batch_loss = loss / target_tensor.shape[1]\n",
        "    t_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, t_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, t_variables))\n",
        "    return batch_loss\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kj6diJMqdL0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "40060f1d-4883-45d4-df4c-ce179bf3a3bd"
      },
      "source": [
        "hidden = tf.zeros(shape=(16, 256))\n",
        "loss = train_step(input_tensor, output_tensor, hidden)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(6.5667825, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw5Sib0zvN15",
        "colab_type": "text"
      },
      "source": [
        "**checkpoint** is a helper function to save our model at certain points during our training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk_OG391qdL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def checkpoint(model, name=None):\n",
        "    if name is not None:\n",
        "        model.save_weights('{}.h5'.format(name))\n",
        "    else:\n",
        "        raise NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuH_Zd23w1-S",
        "colab_type": "text"
      },
      "source": [
        "The final part of our training pipeline is a training loop. \n",
        "In each epoch, we iterate through our dataset and call the train_step function on each batch of the dataset. If-else statements just to log our training statistics on screen. Let’s see how."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZEbkwx9qdL-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "42d5fa9e-7c81-4a24-e1bb-3c6d908bcebd"
      },
      "source": [
        "EPOCHS = 10\n",
        "log_every = 50\n",
        "steps_per_epoch = len(pairs) // BATCH_SIZE\n",
        "loss_list = []\n",
        "\n",
        "for e in range(1, EPOCHS):\n",
        "    \n",
        "    total_loss = 0.0\n",
        "    enc_hidden = encoder.init_hidden()\n",
        "    \n",
        "    for idx, (input_tensor, target_tensor) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(input_tensor, target_tensor, hidden)\n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        if idx % log_every == 0:\n",
        "            loss_list.append(batch_loss)\n",
        "            print(\"Epochs: {} batch_loss: {:.4f}\".format(e, batch_loss))\n",
        "            checkpoint(encoder, 'encoder')\n",
        "            checkpoint(decoder, 'decoder')\n",
        "      "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epochs: 1 batch_loss: 7.0865\n",
            "Epochs: 1 batch_loss: 3.8397\n",
            "Epochs: 1 batch_loss: 3.7026\n",
            "Epochs: 1 batch_loss: 3.9938\n",
            "Epochs: 1 batch_loss: 3.8857\n",
            "Epochs: 1 batch_loss: 3.2963\n",
            "Epochs: 1 batch_loss: 3.4493\n",
            "Epochs: 1 batch_loss: 3.7155\n",
            "Epochs: 1 batch_loss: 3.1246\n",
            "Epochs: 1 batch_loss: 3.2690\n",
            "Epochs: 1 batch_loss: 3.5250\n",
            "Epochs: 1 batch_loss: 3.5497\n",
            "Epochs: 1 batch_loss: 3.1909\n",
            "Epochs: 1 batch_loss: 3.1432\n",
            "Epochs: 1 batch_loss: 3.3324\n",
            "Epochs: 1 batch_loss: 2.9870\n",
            "Epochs: 1 batch_loss: 3.0151\n",
            "Epochs: 1 batch_loss: 2.9846\n",
            "Epochs: 1 batch_loss: 2.7710\n",
            "Epochs: 1 batch_loss: 2.6845\n",
            "Epochs: 1 batch_loss: 3.0098\n",
            "Epochs: 1 batch_loss: 3.1071\n",
            "Epochs: 1 batch_loss: 2.7628\n",
            "Epochs: 1 batch_loss: 2.6159\n",
            "Epochs: 1 batch_loss: 2.5755\n",
            "Epochs: 1 batch_loss: 2.8085\n",
            "Epochs: 1 batch_loss: 2.3538\n",
            "Epochs: 1 batch_loss: 2.6461\n",
            "Epochs: 1 batch_loss: 2.6198\n",
            "Epochs: 1 batch_loss: 2.4945\n",
            "Epochs: 1 batch_loss: 2.1924\n",
            "Epochs: 1 batch_loss: 2.5782\n",
            "Epochs: 1 batch_loss: 2.2662\n",
            "Epochs: 1 batch_loss: 2.1683\n",
            "Epochs: 1 batch_loss: 2.1759\n",
            "Epochs: 1 batch_loss: 2.3755\n",
            "Epochs: 1 batch_loss: 2.6029\n",
            "Epochs: 1 batch_loss: 2.2704\n",
            "Epochs: 1 batch_loss: 2.5153\n",
            "Epochs: 1 batch_loss: 2.1038\n",
            "Epochs: 1 batch_loss: 2.1261\n",
            "Epochs: 1 batch_loss: 2.2918\n",
            "Epochs: 1 batch_loss: 1.9160\n",
            "Epochs: 1 batch_loss: 1.9667\n",
            "Epochs: 1 batch_loss: 2.0875\n",
            "Epochs: 1 batch_loss: 1.7474\n",
            "Epochs: 1 batch_loss: 2.0256\n",
            "Epochs: 1 batch_loss: 1.9095\n",
            "Epochs: 1 batch_loss: 1.7637\n",
            "Epochs: 1 batch_loss: 1.8081\n",
            "Epochs: 1 batch_loss: 1.3236\n",
            "Epochs: 1 batch_loss: 2.4397\n",
            "Epochs: 1 batch_loss: 1.5431\n",
            "Epochs: 1 batch_loss: 1.7825\n",
            "Epochs: 1 batch_loss: 1.7979\n",
            "Epochs: 1 batch_loss: 1.8669\n",
            "Epochs: 1 batch_loss: 1.9146\n",
            "Epochs: 1 batch_loss: 1.6814\n",
            "Epochs: 1 batch_loss: 1.6225\n",
            "Epochs: 1 batch_loss: 1.4473\n",
            "Epochs: 1 batch_loss: 1.9731\n",
            "Epochs: 1 batch_loss: 2.1054\n",
            "Epochs: 1 batch_loss: 2.2666\n",
            "Epochs: 1 batch_loss: 1.8019\n",
            "Epochs: 1 batch_loss: 1.7806\n",
            "Epochs: 1 batch_loss: 1.7758\n",
            "Epochs: 1 batch_loss: 1.6168\n",
            "Epochs: 1 batch_loss: 1.9487\n",
            "Epochs: 1 batch_loss: 1.7980\n",
            "Epochs: 1 batch_loss: 1.2899\n",
            "Epochs: 1 batch_loss: 1.9101\n",
            "Epochs: 1 batch_loss: 1.3372\n",
            "Epochs: 1 batch_loss: 1.8161\n",
            "Epochs: 1 batch_loss: 1.6494\n",
            "Epochs: 1 batch_loss: 1.7337\n",
            "Epochs: 1 batch_loss: 2.0917\n",
            "Epochs: 1 batch_loss: 1.3011\n",
            "Epochs: 1 batch_loss: 1.9302\n",
            "Epochs: 1 batch_loss: 1.4695\n",
            "Epochs: 1 batch_loss: 1.5638\n",
            "Epochs: 1 batch_loss: 1.2716\n",
            "Epochs: 1 batch_loss: 1.5786\n",
            "Epochs: 1 batch_loss: 1.2888\n",
            "Epochs: 1 batch_loss: 1.8293\n",
            "Epochs: 1 batch_loss: 1.5137\n",
            "Epochs: 1 batch_loss: 1.7292\n",
            "Epochs: 1 batch_loss: 1.7485\n",
            "Epochs: 1 batch_loss: 1.3287\n",
            "Epochs: 1 batch_loss: 1.5315\n",
            "Epochs: 1 batch_loss: 1.3849\n",
            "Epochs: 1 batch_loss: 1.2761\n",
            "Epochs: 1 batch_loss: 1.5645\n",
            "Epochs: 1 batch_loss: 1.1347\n",
            "Epochs: 1 batch_loss: 1.5629\n",
            "Epochs: 1 batch_loss: 1.4530\n",
            "Epochs: 1 batch_loss: 1.2343\n",
            "Epochs: 1 batch_loss: 1.3301\n",
            "Epochs: 1 batch_loss: 1.2498\n",
            "Epochs: 1 batch_loss: 1.5250\n",
            "Epochs: 1 batch_loss: 1.5391\n",
            "Epochs: 1 batch_loss: 1.5504\n",
            "Epochs: 1 batch_loss: 1.7726\n",
            "Epochs: 1 batch_loss: 1.4258\n",
            "Epochs: 1 batch_loss: 1.3335\n",
            "Epochs: 1 batch_loss: 1.4417\n",
            "Epochs: 1 batch_loss: 1.2948\n",
            "Epochs: 1 batch_loss: 1.5371\n",
            "Epochs: 1 batch_loss: 1.1637\n",
            "Epochs: 1 batch_loss: 1.3533\n",
            "Epochs: 1 batch_loss: 1.5463\n",
            "Epochs: 1 batch_loss: 1.1625\n",
            "Epochs: 1 batch_loss: 0.9064\n",
            "Epochs: 1 batch_loss: 1.6538\n",
            "Epochs: 1 batch_loss: 1.5012\n",
            "Epochs: 1 batch_loss: 1.1283\n",
            "Epochs: 1 batch_loss: 1.1929\n",
            "Epochs: 1 batch_loss: 1.3670\n",
            "Epochs: 1 batch_loss: 1.2396\n",
            "Epochs: 1 batch_loss: 1.2341\n",
            "Epochs: 1 batch_loss: 0.9859\n",
            "Epochs: 1 batch_loss: 0.8443\n",
            "Epochs: 1 batch_loss: 1.5188\n",
            "Epochs: 1 batch_loss: 1.5356\n",
            "Epochs: 1 batch_loss: 0.8413\n",
            "Epochs: 1 batch_loss: 0.9558\n",
            "Epochs: 1 batch_loss: 1.4217\n",
            "Epochs: 1 batch_loss: 0.9611\n",
            "Epochs: 1 batch_loss: 1.4710\n",
            "Epochs: 1 batch_loss: 1.3062\n",
            "Epochs: 1 batch_loss: 1.0906\n",
            "Epochs: 1 batch_loss: 1.4007\n",
            "Epochs: 1 batch_loss: 1.6722\n",
            "Epochs: 1 batch_loss: 1.2052\n",
            "Epochs: 1 batch_loss: 1.2129\n",
            "Epochs: 1 batch_loss: 1.0909\n",
            "Epochs: 1 batch_loss: 0.9621\n",
            "Epochs: 1 batch_loss: 1.4760\n",
            "Epochs: 1 batch_loss: 0.9795\n",
            "Epochs: 1 batch_loss: 1.1857\n",
            "Epochs: 1 batch_loss: 1.0439\n",
            "Epochs: 1 batch_loss: 1.1216\n",
            "Epochs: 1 batch_loss: 1.0963\n",
            "Epochs: 1 batch_loss: 1.2925\n",
            "Epochs: 1 batch_loss: 1.1186\n",
            "Epochs: 1 batch_loss: 0.9726\n",
            "Epochs: 1 batch_loss: 0.7743\n",
            "Epochs: 1 batch_loss: 1.0342\n",
            "Epochs: 1 batch_loss: 0.9544\n",
            "Epochs: 1 batch_loss: 1.1446\n",
            "Epochs: 1 batch_loss: 1.0962\n",
            "Epochs: 1 batch_loss: 1.4023\n",
            "Epochs: 1 batch_loss: 1.3241\n",
            "Epochs: 1 batch_loss: 1.2989\n",
            "Epochs: 1 batch_loss: 1.0457\n",
            "Epochs: 2 batch_loss: 0.7629\n",
            "Epochs: 2 batch_loss: 0.6724\n",
            "Epochs: 2 batch_loss: 1.0011\n",
            "Epochs: 2 batch_loss: 0.6319\n",
            "Epochs: 2 batch_loss: 1.1855\n",
            "Epochs: 2 batch_loss: 0.8157\n",
            "Epochs: 2 batch_loss: 0.5211\n",
            "Epochs: 2 batch_loss: 1.0272\n",
            "Epochs: 2 batch_loss: 0.8598\n",
            "Epochs: 2 batch_loss: 1.1366\n",
            "Epochs: 2 batch_loss: 0.7036\n",
            "Epochs: 2 batch_loss: 0.8302\n",
            "Epochs: 2 batch_loss: 0.9508\n",
            "Epochs: 2 batch_loss: 0.8150\n",
            "Epochs: 2 batch_loss: 1.0165\n",
            "Epochs: 2 batch_loss: 0.8861\n",
            "Epochs: 2 batch_loss: 0.9818\n",
            "Epochs: 2 batch_loss: 0.8416\n",
            "Epochs: 2 batch_loss: 0.8260\n",
            "Epochs: 2 batch_loss: 0.8700\n",
            "Epochs: 2 batch_loss: 0.6320\n",
            "Epochs: 2 batch_loss: 1.2917\n",
            "Epochs: 2 batch_loss: 0.7601\n",
            "Epochs: 2 batch_loss: 1.1558\n",
            "Epochs: 2 batch_loss: 0.8520\n",
            "Epochs: 2 batch_loss: 0.6135\n",
            "Epochs: 2 batch_loss: 0.8106\n",
            "Epochs: 2 batch_loss: 0.9447\n",
            "Epochs: 2 batch_loss: 0.8972\n",
            "Epochs: 2 batch_loss: 1.1767\n",
            "Epochs: 2 batch_loss: 0.7056\n",
            "Epochs: 2 batch_loss: 1.1695\n",
            "Epochs: 2 batch_loss: 1.3964\n",
            "Epochs: 2 batch_loss: 0.8862\n",
            "Epochs: 2 batch_loss: 1.0638\n",
            "Epochs: 2 batch_loss: 1.0971\n",
            "Epochs: 2 batch_loss: 0.9308\n",
            "Epochs: 2 batch_loss: 1.1779\n",
            "Epochs: 2 batch_loss: 0.8722\n",
            "Epochs: 2 batch_loss: 0.7984\n",
            "Epochs: 2 batch_loss: 1.1087\n",
            "Epochs: 2 batch_loss: 0.7885\n",
            "Epochs: 2 batch_loss: 1.3661\n",
            "Epochs: 2 batch_loss: 0.8994\n",
            "Epochs: 2 batch_loss: 0.7869\n",
            "Epochs: 2 batch_loss: 1.3317\n",
            "Epochs: 2 batch_loss: 0.8233\n",
            "Epochs: 2 batch_loss: 0.9630\n",
            "Epochs: 2 batch_loss: 1.0134\n",
            "Epochs: 2 batch_loss: 0.8718\n",
            "Epochs: 2 batch_loss: 0.8209\n",
            "Epochs: 2 batch_loss: 1.3027\n",
            "Epochs: 2 batch_loss: 1.1132\n",
            "Epochs: 2 batch_loss: 0.5862\n",
            "Epochs: 2 batch_loss: 0.7451\n",
            "Epochs: 2 batch_loss: 0.9488\n",
            "Epochs: 2 batch_loss: 0.8960\n",
            "Epochs: 2 batch_loss: 0.4971\n",
            "Epochs: 2 batch_loss: 0.8817\n",
            "Epochs: 2 batch_loss: 0.8518\n",
            "Epochs: 2 batch_loss: 0.8776\n",
            "Epochs: 2 batch_loss: 1.1574\n",
            "Epochs: 2 batch_loss: 0.6534\n",
            "Epochs: 2 batch_loss: 0.6094\n",
            "Epochs: 2 batch_loss: 0.6556\n",
            "Epochs: 2 batch_loss: 0.8045\n",
            "Epochs: 2 batch_loss: 0.6313\n",
            "Epochs: 2 batch_loss: 0.7138\n",
            "Epochs: 2 batch_loss: 0.6907\n",
            "Epochs: 2 batch_loss: 0.6092\n",
            "Epochs: 2 batch_loss: 0.7222\n",
            "Epochs: 2 batch_loss: 1.2795\n",
            "Epochs: 2 batch_loss: 0.5605\n",
            "Epochs: 2 batch_loss: 0.6559\n",
            "Epochs: 2 batch_loss: 0.7507\n",
            "Epochs: 2 batch_loss: 0.8216\n",
            "Epochs: 2 batch_loss: 1.0180\n",
            "Epochs: 2 batch_loss: 1.1954\n",
            "Epochs: 2 batch_loss: 0.7750\n",
            "Epochs: 2 batch_loss: 0.5682\n",
            "Epochs: 2 batch_loss: 1.1370\n",
            "Epochs: 2 batch_loss: 0.8324\n",
            "Epochs: 2 batch_loss: 0.8499\n",
            "Epochs: 2 batch_loss: 0.7554\n",
            "Epochs: 2 batch_loss: 0.7159\n",
            "Epochs: 2 batch_loss: 0.7647\n",
            "Epochs: 2 batch_loss: 0.7412\n",
            "Epochs: 2 batch_loss: 0.9758\n",
            "Epochs: 2 batch_loss: 1.2024\n",
            "Epochs: 2 batch_loss: 0.9947\n",
            "Epochs: 2 batch_loss: 0.9868\n",
            "Epochs: 2 batch_loss: 0.9614\n",
            "Epochs: 2 batch_loss: 0.6497\n",
            "Epochs: 2 batch_loss: 0.6628\n",
            "Epochs: 2 batch_loss: 0.6329\n",
            "Epochs: 2 batch_loss: 0.7027\n",
            "Epochs: 2 batch_loss: 0.8307\n",
            "Epochs: 2 batch_loss: 0.7490\n",
            "Epochs: 2 batch_loss: 0.7051\n",
            "Epochs: 2 batch_loss: 1.0370\n",
            "Epochs: 2 batch_loss: 0.5998\n",
            "Epochs: 2 batch_loss: 0.4475\n",
            "Epochs: 2 batch_loss: 0.7866\n",
            "Epochs: 2 batch_loss: 0.6830\n",
            "Epochs: 2 batch_loss: 1.0740\n",
            "Epochs: 2 batch_loss: 0.8400\n",
            "Epochs: 2 batch_loss: 1.1673\n",
            "Epochs: 2 batch_loss: 0.5872\n",
            "Epochs: 2 batch_loss: 0.7529\n",
            "Epochs: 2 batch_loss: 0.9052\n",
            "Epochs: 2 batch_loss: 0.5638\n",
            "Epochs: 2 batch_loss: 0.8550\n",
            "Epochs: 2 batch_loss: 0.5647\n",
            "Epochs: 2 batch_loss: 0.7601\n",
            "Epochs: 2 batch_loss: 0.7554\n",
            "Epochs: 2 batch_loss: 0.9425\n",
            "Epochs: 2 batch_loss: 0.7463\n",
            "Epochs: 2 batch_loss: 1.0200\n",
            "Epochs: 2 batch_loss: 0.7817\n",
            "Epochs: 2 batch_loss: 0.5138\n",
            "Epochs: 2 batch_loss: 0.7136\n",
            "Epochs: 2 batch_loss: 0.5896\n",
            "Epochs: 2 batch_loss: 0.8464\n",
            "Epochs: 2 batch_loss: 1.0315\n",
            "Epochs: 2 batch_loss: 0.8140\n",
            "Epochs: 2 batch_loss: 0.5969\n",
            "Epochs: 2 batch_loss: 0.8187\n",
            "Epochs: 2 batch_loss: 0.5993\n",
            "Epochs: 2 batch_loss: 0.7617\n",
            "Epochs: 2 batch_loss: 0.5905\n",
            "Epochs: 2 batch_loss: 1.1294\n",
            "Epochs: 2 batch_loss: 0.8841\n",
            "Epochs: 2 batch_loss: 0.5662\n",
            "Epochs: 2 batch_loss: 1.2283\n",
            "Epochs: 2 batch_loss: 0.7158\n",
            "Epochs: 2 batch_loss: 0.8589\n",
            "Epochs: 2 batch_loss: 0.3811\n",
            "Epochs: 2 batch_loss: 0.6063\n",
            "Epochs: 2 batch_loss: 0.7781\n",
            "Epochs: 2 batch_loss: 0.7814\n",
            "Epochs: 2 batch_loss: 0.9208\n",
            "Epochs: 2 batch_loss: 0.8029\n",
            "Epochs: 2 batch_loss: 0.6966\n",
            "Epochs: 2 batch_loss: 0.6909\n",
            "Epochs: 2 batch_loss: 0.5419\n",
            "Epochs: 2 batch_loss: 1.0799\n",
            "Epochs: 2 batch_loss: 0.7466\n",
            "Epochs: 2 batch_loss: 0.7528\n",
            "Epochs: 2 batch_loss: 0.5984\n",
            "Epochs: 2 batch_loss: 0.8372\n",
            "Epochs: 2 batch_loss: 0.4866\n",
            "Epochs: 2 batch_loss: 0.5248\n",
            "Epochs: 2 batch_loss: 0.7515\n",
            "Epochs: 2/10 total_loss: 0.8145\n",
            "Epochs: 3 batch_loss: 0.4011\n",
            "Epochs: 3 batch_loss: 0.7162\n",
            "Epochs: 3 batch_loss: 0.4499\n",
            "Epochs: 3 batch_loss: 0.4855\n",
            "Epochs: 3 batch_loss: 0.6339\n",
            "Epochs: 3 batch_loss: 0.4038\n",
            "Epochs: 3 batch_loss: 0.5578\n",
            "Epochs: 3 batch_loss: 0.5067\n",
            "Epochs: 3 batch_loss: 0.5076\n",
            "Epochs: 3 batch_loss: 0.6865\n",
            "Epochs: 3 batch_loss: 0.7436\n",
            "Epochs: 3 batch_loss: 0.6426\n",
            "Epochs: 3 batch_loss: 0.4831\n",
            "Epochs: 3 batch_loss: 0.4533\n",
            "Epochs: 3 batch_loss: 0.4464\n",
            "Epochs: 3 batch_loss: 0.4926\n",
            "Epochs: 3 batch_loss: 0.5921\n",
            "Epochs: 3 batch_loss: 0.5024\n",
            "Epochs: 3 batch_loss: 0.6453\n",
            "Epochs: 3 batch_loss: 0.6609\n",
            "Epochs: 3 batch_loss: 0.6230\n",
            "Epochs: 3 batch_loss: 0.5258\n",
            "Epochs: 3 batch_loss: 0.5158\n",
            "Epochs: 3 batch_loss: 0.5655\n",
            "Epochs: 3 batch_loss: 0.7934\n",
            "Epochs: 3 batch_loss: 0.2919\n",
            "Epochs: 3 batch_loss: 0.5669\n",
            "Epochs: 3 batch_loss: 0.5057\n",
            "Epochs: 3 batch_loss: 0.5606\n",
            "Epochs: 3 batch_loss: 0.7542\n",
            "Epochs: 3 batch_loss: 0.4953\n",
            "Epochs: 3 batch_loss: 0.8889\n",
            "Epochs: 3 batch_loss: 0.4057\n",
            "Epochs: 3 batch_loss: 1.1322\n",
            "Epochs: 3 batch_loss: 0.6672\n",
            "Epochs: 3 batch_loss: 0.6163\n",
            "Epochs: 3 batch_loss: 0.8580\n",
            "Epochs: 3 batch_loss: 0.6461\n",
            "Epochs: 3 batch_loss: 0.6656\n",
            "Epochs: 3 batch_loss: 0.6039\n",
            "Epochs: 3 batch_loss: 0.6470\n",
            "Epochs: 3 batch_loss: 0.6385\n",
            "Epochs: 3 batch_loss: 0.5270\n",
            "Epochs: 3 batch_loss: 0.6613\n",
            "Epochs: 3 batch_loss: 0.5161\n",
            "Epochs: 3 batch_loss: 0.5385\n",
            "Epochs: 3 batch_loss: 0.6536\n",
            "Epochs: 3 batch_loss: 0.5207\n",
            "Epochs: 3 batch_loss: 0.3911\n",
            "Epochs: 3 batch_loss: 0.5724\n",
            "Epochs: 3 batch_loss: 0.4724\n",
            "Epochs: 3 batch_loss: 0.3895\n",
            "Epochs: 3 batch_loss: 0.7339\n",
            "Epochs: 3 batch_loss: 0.6419\n",
            "Epochs: 3 batch_loss: 0.5595\n",
            "Epochs: 3 batch_loss: 0.7240\n",
            "Epochs: 3 batch_loss: 0.7455\n",
            "Epochs: 3 batch_loss: 0.4211\n",
            "Epochs: 3 batch_loss: 0.7123\n",
            "Epochs: 3 batch_loss: 0.6384\n",
            "Epochs: 3 batch_loss: 0.5558\n",
            "Epochs: 3 batch_loss: 0.5565\n",
            "Epochs: 3 batch_loss: 0.4893\n",
            "Epochs: 3 batch_loss: 0.9464\n",
            "Epochs: 3 batch_loss: 0.5200\n",
            "Epochs: 3 batch_loss: 0.7384\n",
            "Epochs: 3 batch_loss: 0.3854\n",
            "Epochs: 3 batch_loss: 0.5151\n",
            "Epochs: 3 batch_loss: 0.5320\n",
            "Epochs: 3 batch_loss: 0.4741\n",
            "Epochs: 3 batch_loss: 0.5913\n",
            "Epochs: 3 batch_loss: 0.4678\n",
            "Epochs: 3 batch_loss: 0.6333\n",
            "Epochs: 3 batch_loss: 0.9600\n",
            "Epochs: 3 batch_loss: 0.3673\n",
            "Epochs: 3 batch_loss: 0.6042\n",
            "Epochs: 3 batch_loss: 0.4900\n",
            "Epochs: 3 batch_loss: 0.8939\n",
            "Epochs: 3 batch_loss: 0.5815\n",
            "Epochs: 3 batch_loss: 0.5281\n",
            "Epochs: 3 batch_loss: 0.5824\n",
            "Epochs: 3 batch_loss: 0.5513\n",
            "Epochs: 3 batch_loss: 0.3426\n",
            "Epochs: 3 batch_loss: 0.4802\n",
            "Epochs: 3 batch_loss: 0.6618\n",
            "Epochs: 3 batch_loss: 0.6692\n",
            "Epochs: 3 batch_loss: 0.6590\n",
            "Epochs: 3 batch_loss: 0.4155\n",
            "Epochs: 3 batch_loss: 0.7509\n",
            "Epochs: 3 batch_loss: 0.5633\n",
            "Epochs: 3 batch_loss: 0.6737\n",
            "Epochs: 3 batch_loss: 0.5588\n",
            "Epochs: 3 batch_loss: 0.5316\n",
            "Epochs: 3 batch_loss: 0.4147\n",
            "Epochs: 3 batch_loss: 0.6836\n",
            "Epochs: 3 batch_loss: 0.5132\n",
            "Epochs: 3 batch_loss: 0.5925\n",
            "Epochs: 3 batch_loss: 0.6863\n",
            "Epochs: 3 batch_loss: 0.6635\n",
            "Epochs: 3 batch_loss: 0.5532\n",
            "Epochs: 3 batch_loss: 0.4784\n",
            "Epochs: 3 batch_loss: 0.4663\n",
            "Epochs: 3 batch_loss: 0.7470\n",
            "Epochs: 3 batch_loss: 0.6639\n",
            "Epochs: 3 batch_loss: 0.6070\n",
            "Epochs: 3 batch_loss: 0.2999\n",
            "Epochs: 3 batch_loss: 0.5752\n",
            "Epochs: 3 batch_loss: 0.5735\n",
            "Epochs: 3 batch_loss: 0.8247\n",
            "Epochs: 3 batch_loss: 0.4663\n",
            "Epochs: 3 batch_loss: 0.5469\n",
            "Epochs: 3 batch_loss: 0.5173\n",
            "Epochs: 3 batch_loss: 0.4768\n",
            "Epochs: 3 batch_loss: 0.8198\n",
            "Epochs: 3 batch_loss: 0.6282\n",
            "Epochs: 3 batch_loss: 0.4819\n",
            "Epochs: 3 batch_loss: 0.5455\n",
            "Epochs: 3 batch_loss: 0.7927\n",
            "Epochs: 3 batch_loss: 0.3879\n",
            "Epochs: 3 batch_loss: 0.4437\n",
            "Epochs: 3 batch_loss: 0.4197\n",
            "Epochs: 3 batch_loss: 0.6169\n",
            "Epochs: 3 batch_loss: 0.6566\n",
            "Epochs: 3 batch_loss: 0.3635\n",
            "Epochs: 3 batch_loss: 0.4762\n",
            "Epochs: 3 batch_loss: 0.3955\n",
            "Epochs: 3 batch_loss: 0.5568\n",
            "Epochs: 3 batch_loss: 0.6322\n",
            "Epochs: 3 batch_loss: 0.4337\n",
            "Epochs: 3 batch_loss: 0.6214\n",
            "Epochs: 3 batch_loss: 0.3791\n",
            "Epochs: 3 batch_loss: 0.5203\n",
            "Epochs: 3 batch_loss: 0.5763\n",
            "Epochs: 3 batch_loss: 0.5998\n",
            "Epochs: 3 batch_loss: 0.7539\n",
            "Epochs: 3 batch_loss: 0.4666\n",
            "Epochs: 3 batch_loss: 0.4081\n",
            "Epochs: 3 batch_loss: 0.6774\n",
            "Epochs: 3 batch_loss: 0.5322\n",
            "Epochs: 3 batch_loss: 0.5440\n",
            "Epochs: 3 batch_loss: 0.5451\n",
            "Epochs: 3 batch_loss: 0.5680\n",
            "Epochs: 3 batch_loss: 0.4331\n",
            "Epochs: 3 batch_loss: 0.5562\n",
            "Epochs: 3 batch_loss: 0.3375\n",
            "Epochs: 3 batch_loss: 0.5554\n",
            "Epochs: 3 batch_loss: 0.4165\n",
            "Epochs: 3 batch_loss: 0.9883\n",
            "Epochs: 3 batch_loss: 0.5669\n",
            "Epochs: 3 batch_loss: 0.7071\n",
            "Epochs: 3 batch_loss: 0.5199\n",
            "Epochs: 3 batch_loss: 0.4989\n",
            "Epochs: 3 batch_loss: 0.8109\n",
            "Epochs: 3 batch_loss: 0.5958\n",
            "Epochs: 3 batch_loss: 1.0341\n",
            "Epochs: 4 batch_loss: 0.3285\n",
            "Epochs: 4 batch_loss: 0.2521\n",
            "Epochs: 4 batch_loss: 0.2846\n",
            "Epochs: 4 batch_loss: 0.1990\n",
            "Epochs: 4 batch_loss: 0.3416\n",
            "Epochs: 4 batch_loss: 0.8418\n",
            "Epochs: 4 batch_loss: 0.3641\n",
            "Epochs: 4 batch_loss: 0.8667\n",
            "Epochs: 4 batch_loss: 0.2593\n",
            "Epochs: 4 batch_loss: 0.4471\n",
            "Epochs: 4 batch_loss: 0.5069\n",
            "Epochs: 4 batch_loss: 0.2743\n",
            "Epochs: 4 batch_loss: 0.5024\n",
            "Epochs: 4 batch_loss: 0.2620\n",
            "Epochs: 4 batch_loss: 0.6704\n",
            "Epochs: 4 batch_loss: 0.4609\n",
            "Epochs: 4 batch_loss: 0.3502\n",
            "Epochs: 4 batch_loss: 0.2342\n",
            "Epochs: 4 batch_loss: 0.2333\n",
            "Epochs: 4 batch_loss: 0.4314\n",
            "Epochs: 4 batch_loss: 0.5311\n",
            "Epochs: 4 batch_loss: 0.3957\n",
            "Epochs: 4 batch_loss: 0.4351\n",
            "Epochs: 4 batch_loss: 0.2684\n",
            "Epochs: 4 batch_loss: 0.4984\n",
            "Epochs: 4 batch_loss: 0.6599\n",
            "Epochs: 4 batch_loss: 0.4969\n",
            "Epochs: 4 batch_loss: 0.4014\n",
            "Epochs: 4 batch_loss: 0.5115\n",
            "Epochs: 4 batch_loss: 0.5731\n",
            "Epochs: 4 batch_loss: 0.3867\n",
            "Epochs: 4 batch_loss: 0.2882\n",
            "Epochs: 4 batch_loss: 0.4273\n",
            "Epochs: 4 batch_loss: 0.7231\n",
            "Epochs: 4 batch_loss: 0.2458\n",
            "Epochs: 4 batch_loss: 0.4566\n",
            "Epochs: 4 batch_loss: 0.5742\n",
            "Epochs: 4 batch_loss: 0.4318\n",
            "Epochs: 4 batch_loss: 0.3182\n",
            "Epochs: 4 batch_loss: 0.2236\n",
            "Epochs: 4 batch_loss: 0.7442\n",
            "Epochs: 4 batch_loss: 0.4197\n",
            "Epochs: 4 batch_loss: 0.4073\n",
            "Epochs: 4 batch_loss: 0.4243\n",
            "Epochs: 4 batch_loss: 0.4547\n",
            "Epochs: 4 batch_loss: 0.4331\n",
            "Epochs: 4 batch_loss: 0.3872\n",
            "Epochs: 4 batch_loss: 0.3260\n",
            "Epochs: 4 batch_loss: 0.3197\n",
            "Epochs: 4 batch_loss: 0.3196\n",
            "Epochs: 4 batch_loss: 0.6090\n",
            "Epochs: 4 batch_loss: 0.4381\n",
            "Epochs: 4 batch_loss: 0.2524\n",
            "Epochs: 4 batch_loss: 0.3504\n",
            "Epochs: 4 batch_loss: 0.7104\n",
            "Epochs: 4 batch_loss: 0.6354\n",
            "Epochs: 4 batch_loss: 0.2775\n",
            "Epochs: 4 batch_loss: 0.6234\n",
            "Epochs: 4 batch_loss: 0.5105\n",
            "Epochs: 4 batch_loss: 0.3419\n",
            "Epochs: 4 batch_loss: 0.3067\n",
            "Epochs: 4 batch_loss: 0.1851\n",
            "Epochs: 4 batch_loss: 0.5038\n",
            "Epochs: 4 batch_loss: 0.3737\n",
            "Epochs: 4 batch_loss: 0.3756\n",
            "Epochs: 4 batch_loss: 0.3402\n",
            "Epochs: 4 batch_loss: 0.6396\n",
            "Epochs: 4 batch_loss: 0.6518\n",
            "Epochs: 4 batch_loss: 0.4066\n",
            "Epochs: 4 batch_loss: 0.3713\n",
            "Epochs: 4 batch_loss: 0.2968\n",
            "Epochs: 4 batch_loss: 0.5429\n",
            "Epochs: 4 batch_loss: 0.6336\n",
            "Epochs: 4 batch_loss: 0.3603\n",
            "Epochs: 4 batch_loss: 0.4295\n",
            "Epochs: 4 batch_loss: 0.3117\n",
            "Epochs: 4 batch_loss: 0.3661\n",
            "Epochs: 4 batch_loss: 0.3516\n",
            "Epochs: 4 batch_loss: 0.3747\n",
            "Epochs: 4 batch_loss: 0.4312\n",
            "Epochs: 4 batch_loss: 0.4332\n",
            "Epochs: 4 batch_loss: 0.1903\n",
            "Epochs: 4 batch_loss: 0.3706\n",
            "Epochs: 4 batch_loss: 0.4559\n",
            "Epochs: 4 batch_loss: 0.1882\n",
            "Epochs: 4 batch_loss: 0.5380\n",
            "Epochs: 4 batch_loss: 0.4907\n",
            "Epochs: 4 batch_loss: 0.3318\n",
            "Epochs: 4 batch_loss: 0.4721\n",
            "Epochs: 4 batch_loss: 0.4018\n",
            "Epochs: 4 batch_loss: 0.4936\n",
            "Epochs: 4 batch_loss: 0.4494\n",
            "Epochs: 4 batch_loss: 0.3535\n",
            "Epochs: 4 batch_loss: 0.3376\n",
            "Epochs: 4 batch_loss: 0.4026\n",
            "Epochs: 4 batch_loss: 0.3990\n",
            "Epochs: 4 batch_loss: 0.5239\n",
            "Epochs: 4 batch_loss: 0.3839\n",
            "Epochs: 4 batch_loss: 0.4860\n",
            "Epochs: 4 batch_loss: 0.4250\n",
            "Epochs: 4 batch_loss: 0.2147\n",
            "Epochs: 4 batch_loss: 0.7001\n",
            "Epochs: 4 batch_loss: 0.3638\n",
            "Epochs: 4 batch_loss: 0.4563\n",
            "Epochs: 4 batch_loss: 0.4006\n",
            "Epochs: 4 batch_loss: 0.4383\n",
            "Epochs: 4 batch_loss: 0.4866\n",
            "Epochs: 4 batch_loss: 0.5986\n",
            "Epochs: 4 batch_loss: 0.5103\n",
            "Epochs: 4 batch_loss: 0.8339\n",
            "Epochs: 4 batch_loss: 0.5737\n",
            "Epochs: 4 batch_loss: 0.3607\n",
            "Epochs: 4 batch_loss: 0.3842\n",
            "Epochs: 4 batch_loss: 0.3198\n",
            "Epochs: 4 batch_loss: 0.6669\n",
            "Epochs: 4 batch_loss: 0.2959\n",
            "Epochs: 4 batch_loss: 0.5349\n",
            "Epochs: 4 batch_loss: 0.3718\n",
            "Epochs: 4 batch_loss: 0.2222\n",
            "Epochs: 4 batch_loss: 0.3112\n",
            "Epochs: 4 batch_loss: 0.7196\n",
            "Epochs: 4 batch_loss: 0.3162\n",
            "Epochs: 4 batch_loss: 0.4764\n",
            "Epochs: 4 batch_loss: 0.4650\n",
            "Epochs: 4 batch_loss: 0.5715\n",
            "Epochs: 4 batch_loss: 0.3048\n",
            "Epochs: 4 batch_loss: 0.3627\n",
            "Epochs: 4 batch_loss: 0.5080\n",
            "Epochs: 4 batch_loss: 0.5046\n",
            "Epochs: 4 batch_loss: 0.3668\n",
            "Epochs: 4 batch_loss: 0.6767\n",
            "Epochs: 4 batch_loss: 0.4639\n",
            "Epochs: 4 batch_loss: 0.3121\n",
            "Epochs: 4 batch_loss: 0.3562\n",
            "Epochs: 4 batch_loss: 0.7272\n",
            "Epochs: 4 batch_loss: 0.4015\n",
            "Epochs: 4 batch_loss: 0.1706\n",
            "Epochs: 4 batch_loss: 0.7875\n",
            "Epochs: 4 batch_loss: 0.4281\n",
            "Epochs: 4 batch_loss: 0.3271\n",
            "Epochs: 4 batch_loss: 0.3676\n",
            "Epochs: 4 batch_loss: 0.7584\n",
            "Epochs: 4 batch_loss: 0.4396\n",
            "Epochs: 4 batch_loss: 0.3582\n",
            "Epochs: 4 batch_loss: 0.2871\n",
            "Epochs: 4 batch_loss: 0.3751\n",
            "Epochs: 4 batch_loss: 0.7139\n",
            "Epochs: 4 batch_loss: 0.3731\n",
            "Epochs: 4 batch_loss: 0.6676\n",
            "Epochs: 4 batch_loss: 0.4555\n",
            "Epochs: 4 batch_loss: 0.5115\n",
            "Epochs: 4 batch_loss: 0.6396\n",
            "Epochs: 4 batch_loss: 0.4065\n",
            "Epochs: 4 batch_loss: 0.4953\n",
            "Epochs: 4 batch_loss: 0.6897\n",
            "Epochs: 4/10 total_loss: 0.4402\n",
            "Epochs: 5 batch_loss: 0.4047\n",
            "Epochs: 5 batch_loss: 0.3581\n",
            "Epochs: 5 batch_loss: 0.2072\n",
            "Epochs: 5 batch_loss: 0.2149\n",
            "Epochs: 5 batch_loss: 0.4027\n",
            "Epochs: 5 batch_loss: 0.1699\n",
            "Epochs: 5 batch_loss: 0.2131\n",
            "Epochs: 5 batch_loss: 0.1568\n",
            "Epochs: 5 batch_loss: 0.2828\n",
            "Epochs: 5 batch_loss: 0.3490\n",
            "Epochs: 5 batch_loss: 0.3264\n",
            "Epochs: 5 batch_loss: 0.1899\n",
            "Epochs: 5 batch_loss: 0.4290\n",
            "Epochs: 5 batch_loss: 0.4768\n",
            "Epochs: 5 batch_loss: 0.2617\n",
            "Epochs: 5 batch_loss: 0.2597\n",
            "Epochs: 5 batch_loss: 0.1758\n",
            "Epochs: 5 batch_loss: 0.5174\n",
            "Epochs: 5 batch_loss: 0.2642\n",
            "Epochs: 5 batch_loss: 0.2049\n",
            "Epochs: 5 batch_loss: 0.2307\n",
            "Epochs: 5 batch_loss: 0.2306\n",
            "Epochs: 5 batch_loss: 0.2745\n",
            "Epochs: 5 batch_loss: 0.4042\n",
            "Epochs: 5 batch_loss: 0.3323\n",
            "Epochs: 5 batch_loss: 0.2357\n",
            "Epochs: 5 batch_loss: 0.3924\n",
            "Epochs: 5 batch_loss: 0.3882\n",
            "Epochs: 5 batch_loss: 0.2995\n",
            "Epochs: 5 batch_loss: 0.3433\n",
            "Epochs: 5 batch_loss: 0.2754\n",
            "Epochs: 5 batch_loss: 0.2263\n",
            "Epochs: 5 batch_loss: 0.1988\n",
            "Epochs: 5 batch_loss: 0.3408\n",
            "Epochs: 5 batch_loss: 0.4248\n",
            "Epochs: 5 batch_loss: 0.1965\n",
            "Epochs: 5 batch_loss: 0.4830\n",
            "Epochs: 5 batch_loss: 0.2174\n",
            "Epochs: 5 batch_loss: 0.3031\n",
            "Epochs: 5 batch_loss: 0.4621\n",
            "Epochs: 5 batch_loss: 0.4586\n",
            "Epochs: 5 batch_loss: 0.3424\n",
            "Epochs: 5 batch_loss: 0.4009\n",
            "Epochs: 5 batch_loss: 0.5349\n",
            "Epochs: 5 batch_loss: 0.3274\n",
            "Epochs: 5 batch_loss: 0.3224\n",
            "Epochs: 5 batch_loss: 0.5871\n",
            "Epochs: 5 batch_loss: 0.6334\n",
            "Epochs: 5 batch_loss: 0.2752\n",
            "Epochs: 5 batch_loss: 0.6503\n",
            "Epochs: 5 batch_loss: 0.4624\n",
            "Epochs: 5 batch_loss: 0.4717\n",
            "Epochs: 5 batch_loss: 0.3696\n",
            "Epochs: 5 batch_loss: 0.2152\n",
            "Epochs: 5 batch_loss: 0.2320\n",
            "Epochs: 5 batch_loss: 0.2606\n",
            "Epochs: 5 batch_loss: 0.2140\n",
            "Epochs: 5 batch_loss: 0.5039\n",
            "Epochs: 5 batch_loss: 0.6507\n",
            "Epochs: 5 batch_loss: 0.3926\n",
            "Epochs: 5 batch_loss: 0.4452\n",
            "Epochs: 5 batch_loss: 0.3601\n",
            "Epochs: 5 batch_loss: 0.4995\n",
            "Epochs: 5 batch_loss: 0.5152\n",
            "Epochs: 5 batch_loss: 0.3619\n",
            "Epochs: 5 batch_loss: 0.5370\n",
            "Epochs: 5 batch_loss: 0.4803\n",
            "Epochs: 5 batch_loss: 0.2442\n",
            "Epochs: 5 batch_loss: 0.3028\n",
            "Epochs: 5 batch_loss: 0.2170\n",
            "Epochs: 5 batch_loss: 0.4675\n",
            "Epochs: 5 batch_loss: 0.5016\n",
            "Epochs: 5 batch_loss: 0.2282\n",
            "Epochs: 5 batch_loss: 0.4040\n",
            "Epochs: 5 batch_loss: 0.3083\n",
            "Epochs: 5 batch_loss: 0.3579\n",
            "Epochs: 5 batch_loss: 0.3321\n",
            "Epochs: 5 batch_loss: 0.3481\n",
            "Epochs: 5 batch_loss: 0.2677\n",
            "Epochs: 5 batch_loss: 0.2690\n",
            "Epochs: 5 batch_loss: 0.3557\n",
            "Epochs: 5 batch_loss: 0.3443\n",
            "Epochs: 5 batch_loss: 0.4154\n",
            "Epochs: 5 batch_loss: 0.3841\n",
            "Epochs: 5 batch_loss: 0.2909\n",
            "Epochs: 5 batch_loss: 0.6026\n",
            "Epochs: 5 batch_loss: 0.3576\n",
            "Epochs: 5 batch_loss: 0.3647\n",
            "Epochs: 5 batch_loss: 0.5275\n",
            "Epochs: 5 batch_loss: 0.5579\n",
            "Epochs: 5 batch_loss: 0.5456\n",
            "Epochs: 5 batch_loss: 0.3908\n",
            "Epochs: 5 batch_loss: 0.4964\n",
            "Epochs: 5 batch_loss: 0.5274\n",
            "Epochs: 5 batch_loss: 0.3134\n",
            "Epochs: 5 batch_loss: 0.3554\n",
            "Epochs: 5 batch_loss: 0.3958\n",
            "Epochs: 5 batch_loss: 0.2328\n",
            "Epochs: 5 batch_loss: 0.3866\n",
            "Epochs: 5 batch_loss: 0.4053\n",
            "Epochs: 5 batch_loss: 0.3604\n",
            "Epochs: 5 batch_loss: 0.4601\n",
            "Epochs: 5 batch_loss: 0.2589\n",
            "Epochs: 5 batch_loss: 0.4367\n",
            "Epochs: 5 batch_loss: 0.3759\n",
            "Epochs: 5 batch_loss: 0.4361\n",
            "Epochs: 5 batch_loss: 0.6376\n",
            "Epochs: 5 batch_loss: 0.4100\n",
            "Epochs: 5 batch_loss: 0.3922\n",
            "Epochs: 5 batch_loss: 0.2325\n",
            "Epochs: 5 batch_loss: 0.3257\n",
            "Epochs: 5 batch_loss: 0.2430\n",
            "Epochs: 5 batch_loss: 0.3019\n",
            "Epochs: 5 batch_loss: 0.4835\n",
            "Epochs: 5 batch_loss: 0.3553\n",
            "Epochs: 5 batch_loss: 0.2720\n",
            "Epochs: 5 batch_loss: 0.3771\n",
            "Epochs: 5 batch_loss: 0.3308\n",
            "Epochs: 5 batch_loss: 0.2621\n",
            "Epochs: 5 batch_loss: 0.2186\n",
            "Epochs: 5 batch_loss: 0.4308\n",
            "Epochs: 5 batch_loss: 0.3860\n",
            "Epochs: 5 batch_loss: 0.2356\n",
            "Epochs: 5 batch_loss: 0.4298\n",
            "Epochs: 5 batch_loss: 0.6717\n",
            "Epochs: 5 batch_loss: 0.2685\n",
            "Epochs: 5 batch_loss: 0.3463\n",
            "Epochs: 5 batch_loss: 0.5904\n",
            "Epochs: 5 batch_loss: 0.4361\n",
            "Epochs: 5 batch_loss: 0.2047\n",
            "Epochs: 5 batch_loss: 0.5122\n",
            "Epochs: 5 batch_loss: 0.3417\n",
            "Epochs: 5 batch_loss: 0.4136\n",
            "Epochs: 5 batch_loss: 0.3005\n",
            "Epochs: 5 batch_loss: 0.2528\n",
            "Epochs: 5 batch_loss: 0.2776\n",
            "Epochs: 5 batch_loss: 0.4202\n",
            "Epochs: 5 batch_loss: 0.2942\n",
            "Epochs: 5 batch_loss: 0.3700\n",
            "Epochs: 5 batch_loss: 0.3284\n",
            "Epochs: 5 batch_loss: 0.3871\n",
            "Epochs: 5 batch_loss: 0.3923\n",
            "Epochs: 5 batch_loss: 0.4944\n",
            "Epochs: 5 batch_loss: 0.2762\n",
            "Epochs: 5 batch_loss: 0.2192\n",
            "Epochs: 5 batch_loss: 0.6331\n",
            "Epochs: 5 batch_loss: 0.3961\n",
            "Epochs: 5 batch_loss: 0.4140\n",
            "Epochs: 5 batch_loss: 0.5712\n",
            "Epochs: 5 batch_loss: 0.5826\n",
            "Epochs: 5 batch_loss: 0.2119\n",
            "Epochs: 5 batch_loss: 0.5883\n",
            "Epochs: 5 batch_loss: 0.2820\n",
            "Epochs: 5 batch_loss: 0.2586\n",
            "Epochs: 5 batch_loss: 0.3528\n",
            "Epochs: 6 batch_loss: 0.2310\n",
            "Epochs: 6 batch_loss: 0.1763\n",
            "Epochs: 6 batch_loss: 0.2474\n",
            "Epochs: 6 batch_loss: 0.4260\n",
            "Epochs: 6 batch_loss: 0.1470\n",
            "Epochs: 6 batch_loss: 0.3508\n",
            "Epochs: 6 batch_loss: 0.2202\n",
            "Epochs: 6 batch_loss: 0.2117\n",
            "Epochs: 6 batch_loss: 0.2411\n",
            "Epochs: 6 batch_loss: 0.2053\n",
            "Epochs: 6 batch_loss: 0.2576\n",
            "Epochs: 6 batch_loss: 0.1566\n",
            "Epochs: 6 batch_loss: 0.3267\n",
            "Epochs: 6 batch_loss: 0.3135\n",
            "Epochs: 6 batch_loss: 0.3178\n",
            "Epochs: 6 batch_loss: 0.1574\n",
            "Epochs: 6 batch_loss: 0.1918\n",
            "Epochs: 6 batch_loss: 0.1813\n",
            "Epochs: 6 batch_loss: 0.1342\n",
            "Epochs: 6 batch_loss: 0.2390\n",
            "Epochs: 6 batch_loss: 0.2596\n",
            "Epochs: 6 batch_loss: 0.1444\n",
            "Epochs: 6 batch_loss: 0.2064\n",
            "Epochs: 6 batch_loss: 0.1310\n",
            "Epochs: 6 batch_loss: 0.1864\n",
            "Epochs: 6 batch_loss: 0.2401\n",
            "Epochs: 6 batch_loss: 0.2765\n",
            "Epochs: 6 batch_loss: 0.2416\n",
            "Epochs: 6 batch_loss: 0.1538\n",
            "Epochs: 6 batch_loss: 0.2304\n",
            "Epochs: 6 batch_loss: 0.4012\n",
            "Epochs: 6 batch_loss: 0.4250\n",
            "Epochs: 6 batch_loss: 0.3546\n",
            "Epochs: 6 batch_loss: 0.3103\n",
            "Epochs: 6 batch_loss: 0.3345\n",
            "Epochs: 6 batch_loss: 0.4435\n",
            "Epochs: 6 batch_loss: 0.3101\n",
            "Epochs: 6 batch_loss: 0.4264\n",
            "Epochs: 6 batch_loss: 0.1421\n",
            "Epochs: 6 batch_loss: 0.5456\n",
            "Epochs: 6 batch_loss: 0.3518\n",
            "Epochs: 6 batch_loss: 0.0982\n",
            "Epochs: 6 batch_loss: 0.2981\n",
            "Epochs: 6 batch_loss: 0.3959\n",
            "Epochs: 6 batch_loss: 0.2057\n",
            "Epochs: 6 batch_loss: 0.4327\n",
            "Epochs: 6 batch_loss: 0.3753\n",
            "Epochs: 6 batch_loss: 0.3405\n",
            "Epochs: 6 batch_loss: 0.3065\n",
            "Epochs: 6 batch_loss: 0.2945\n",
            "Epochs: 6 batch_loss: 0.2640\n",
            "Epochs: 6 batch_loss: 0.2560\n",
            "Epochs: 6 batch_loss: 0.3237\n",
            "Epochs: 6 batch_loss: 0.2624\n",
            "Epochs: 6 batch_loss: 0.2141\n",
            "Epochs: 6 batch_loss: 0.2638\n",
            "Epochs: 6 batch_loss: 0.3125\n",
            "Epochs: 6 batch_loss: 0.2549\n",
            "Epochs: 6 batch_loss: 0.3707\n",
            "Epochs: 6 batch_loss: 0.2947\n",
            "Epochs: 6 batch_loss: 0.1505\n",
            "Epochs: 6 batch_loss: 0.3530\n",
            "Epochs: 6 batch_loss: 0.2405\n",
            "Epochs: 6 batch_loss: 0.2653\n",
            "Epochs: 6 batch_loss: 0.2823\n",
            "Epochs: 6 batch_loss: 0.1971\n",
            "Epochs: 6 batch_loss: 0.2263\n",
            "Epochs: 6 batch_loss: 0.3296\n",
            "Epochs: 6 batch_loss: 0.4822\n",
            "Epochs: 6 batch_loss: 0.1344\n",
            "Epochs: 6 batch_loss: 0.2035\n",
            "Epochs: 6 batch_loss: 0.3762\n",
            "Epochs: 6 batch_loss: 0.2069\n",
            "Epochs: 6 batch_loss: 0.2686\n",
            "Epochs: 6 batch_loss: 0.2620\n",
            "Epochs: 6 batch_loss: 0.2135\n",
            "Epochs: 6 batch_loss: 0.2507\n",
            "Epochs: 6 batch_loss: 0.2973\n",
            "Epochs: 6 batch_loss: 0.3455\n",
            "Epochs: 6 batch_loss: 0.3750\n",
            "Epochs: 6 batch_loss: 0.2894\n",
            "Epochs: 6 batch_loss: 0.3215\n",
            "Epochs: 6 batch_loss: 0.5817\n",
            "Epochs: 6 batch_loss: 0.2214\n",
            "Epochs: 6 batch_loss: 0.2415\n",
            "Epochs: 6 batch_loss: 0.1939\n",
            "Epochs: 6 batch_loss: 0.2165\n",
            "Epochs: 6 batch_loss: 0.3199\n",
            "Epochs: 6 batch_loss: 0.1946\n",
            "Epochs: 6 batch_loss: 0.3076\n",
            "Epochs: 6 batch_loss: 0.2105\n",
            "Epochs: 6 batch_loss: 0.3272\n",
            "Epochs: 6 batch_loss: 0.2516\n",
            "Epochs: 6 batch_loss: 0.2863\n",
            "Epochs: 6 batch_loss: 0.3324\n",
            "Epochs: 6 batch_loss: 0.2754\n",
            "Epochs: 6 batch_loss: 0.2673\n",
            "Epochs: 6 batch_loss: 0.4894\n",
            "Epochs: 6 batch_loss: 0.2206\n",
            "Epochs: 6 batch_loss: 0.2298\n",
            "Epochs: 6 batch_loss: 0.2560\n",
            "Epochs: 6 batch_loss: 0.2271\n",
            "Epochs: 6 batch_loss: 0.2525\n",
            "Epochs: 6 batch_loss: 0.3064\n",
            "Epochs: 6 batch_loss: 0.3746\n",
            "Epochs: 6 batch_loss: 0.2349\n",
            "Epochs: 6 batch_loss: 0.2877\n",
            "Epochs: 6 batch_loss: 0.5839\n",
            "Epochs: 6 batch_loss: 0.2697\n",
            "Epochs: 6 batch_loss: 0.1909\n",
            "Epochs: 6 batch_loss: 0.3098\n",
            "Epochs: 6 batch_loss: 0.3688\n",
            "Epochs: 6 batch_loss: 0.3230\n",
            "Epochs: 6 batch_loss: 0.5071\n",
            "Epochs: 6 batch_loss: 0.2790\n",
            "Epochs: 6 batch_loss: 0.4868\n",
            "Epochs: 6 batch_loss: 0.2878\n",
            "Epochs: 6 batch_loss: 0.3093\n",
            "Epochs: 6 batch_loss: 0.2462\n",
            "Epochs: 6 batch_loss: 0.2770\n",
            "Epochs: 6 batch_loss: 0.2363\n",
            "Epochs: 6 batch_loss: 0.4894\n",
            "Epochs: 6 batch_loss: 0.3627\n",
            "Epochs: 6 batch_loss: 0.2810\n",
            "Epochs: 6 batch_loss: 0.2013\n",
            "Epochs: 6 batch_loss: 0.2615\n",
            "Epochs: 6 batch_loss: 0.4641\n",
            "Epochs: 6 batch_loss: 0.4695\n",
            "Epochs: 6 batch_loss: 0.3274\n",
            "Epochs: 6 batch_loss: 0.3349\n",
            "Epochs: 6 batch_loss: 0.3227\n",
            "Epochs: 6 batch_loss: 0.2232\n",
            "Epochs: 6 batch_loss: 0.4391\n",
            "Epochs: 6 batch_loss: 0.1536\n",
            "Epochs: 6 batch_loss: 0.3982\n",
            "Epochs: 6 batch_loss: 0.1963\n",
            "Epochs: 6 batch_loss: 0.2000\n",
            "Epochs: 6 batch_loss: 0.4122\n",
            "Epochs: 6 batch_loss: 0.3071\n",
            "Epochs: 6 batch_loss: 0.1931\n",
            "Epochs: 6 batch_loss: 0.3194\n",
            "Epochs: 6 batch_loss: 0.2979\n",
            "Epochs: 6 batch_loss: 0.2559\n",
            "Epochs: 6 batch_loss: 0.4758\n",
            "Epochs: 6 batch_loss: 0.5016\n",
            "Epochs: 6 batch_loss: 0.2781\n",
            "Epochs: 6 batch_loss: 0.5275\n",
            "Epochs: 6 batch_loss: 0.3619\n",
            "Epochs: 6 batch_loss: 0.2795\n",
            "Epochs: 6 batch_loss: 0.2353\n",
            "Epochs: 6 batch_loss: 0.2048\n",
            "Epochs: 6 batch_loss: 0.1578\n",
            "Epochs: 6 batch_loss: 0.5039\n",
            "Epochs: 6 batch_loss: 0.3148\n",
            "Epochs: 6 batch_loss: 0.3943\n",
            "Epochs: 6/10 total_loss: 0.3051\n",
            "Epochs: 7 batch_loss: 0.3114\n",
            "Epochs: 7 batch_loss: 0.1277\n",
            "Epochs: 7 batch_loss: 0.3321\n",
            "Epochs: 7 batch_loss: 0.2225\n",
            "Epochs: 7 batch_loss: 0.1559\n",
            "Epochs: 7 batch_loss: 0.1793\n",
            "Epochs: 7 batch_loss: 0.1105\n",
            "Epochs: 7 batch_loss: 0.1109\n",
            "Epochs: 7 batch_loss: 0.1994\n",
            "Epochs: 7 batch_loss: 0.1710\n",
            "Epochs: 7 batch_loss: 0.2392\n",
            "Epochs: 7 batch_loss: 0.2753\n",
            "Epochs: 7 batch_loss: 0.2003\n",
            "Epochs: 7 batch_loss: 0.2816\n",
            "Epochs: 7 batch_loss: 0.2592\n",
            "Epochs: 7 batch_loss: 0.2611\n",
            "Epochs: 7 batch_loss: 0.1184\n",
            "Epochs: 7 batch_loss: 0.2375\n",
            "Epochs: 7 batch_loss: 0.1286\n",
            "Epochs: 7 batch_loss: 0.2696\n",
            "Epochs: 7 batch_loss: 0.2837\n",
            "Epochs: 7 batch_loss: 0.2384\n",
            "Epochs: 7 batch_loss: 0.2215\n",
            "Epochs: 7 batch_loss: 0.3067\n",
            "Epochs: 7 batch_loss: 0.2644\n",
            "Epochs: 7 batch_loss: 0.4646\n",
            "Epochs: 7 batch_loss: 0.1300\n",
            "Epochs: 7 batch_loss: 0.1125\n",
            "Epochs: 7 batch_loss: 0.2227\n",
            "Epochs: 7 batch_loss: 0.1723\n",
            "Epochs: 7 batch_loss: 0.1346\n",
            "Epochs: 7 batch_loss: 0.2044\n",
            "Epochs: 7 batch_loss: 0.2245\n",
            "Epochs: 7 batch_loss: 0.2519\n",
            "Epochs: 7 batch_loss: 0.3303\n",
            "Epochs: 7 batch_loss: 0.2186\n",
            "Epochs: 7 batch_loss: 0.2140\n",
            "Epochs: 7 batch_loss: 0.1702\n",
            "Epochs: 7 batch_loss: 0.3605\n",
            "Epochs: 7 batch_loss: 0.2133\n",
            "Epochs: 7 batch_loss: 0.1977\n",
            "Epochs: 7 batch_loss: 0.1232\n",
            "Epochs: 7 batch_loss: 0.1356\n",
            "Epochs: 7 batch_loss: 0.2864\n",
            "Epochs: 7 batch_loss: 0.2113\n",
            "Epochs: 7 batch_loss: 0.2631\n",
            "Epochs: 7 batch_loss: 0.3090\n",
            "Epochs: 7 batch_loss: 0.1360\n",
            "Epochs: 7 batch_loss: 0.4163\n",
            "Epochs: 7 batch_loss: 0.2385\n",
            "Epochs: 7 batch_loss: 0.1667\n",
            "Epochs: 7 batch_loss: 0.3780\n",
            "Epochs: 7 batch_loss: 0.2802\n",
            "Epochs: 7 batch_loss: 0.4300\n",
            "Epochs: 7 batch_loss: 0.2002\n",
            "Epochs: 7 batch_loss: 0.3206\n",
            "Epochs: 7 batch_loss: 0.2493\n",
            "Epochs: 7 batch_loss: 0.2868\n",
            "Epochs: 7 batch_loss: 0.4085\n",
            "Epochs: 7 batch_loss: 0.2362\n",
            "Epochs: 7 batch_loss: 0.1983\n",
            "Epochs: 7 batch_loss: 0.2514\n",
            "Epochs: 7 batch_loss: 0.1639\n",
            "Epochs: 7 batch_loss: 0.4043\n",
            "Epochs: 7 batch_loss: 0.4197\n",
            "Epochs: 7 batch_loss: 0.2956\n",
            "Epochs: 7 batch_loss: 0.1466\n",
            "Epochs: 7 batch_loss: 0.3261\n",
            "Epochs: 7 batch_loss: 0.2832\n",
            "Epochs: 7 batch_loss: 0.2122\n",
            "Epochs: 7 batch_loss: 0.2895\n",
            "Epochs: 7 batch_loss: 0.2219\n",
            "Epochs: 7 batch_loss: 0.3636\n",
            "Epochs: 7 batch_loss: 0.3457\n",
            "Epochs: 7 batch_loss: 0.1944\n",
            "Epochs: 7 batch_loss: 0.1956\n",
            "Epochs: 7 batch_loss: 0.4046\n",
            "Epochs: 7 batch_loss: 0.1660\n",
            "Epochs: 7 batch_loss: 0.2558\n",
            "Epochs: 7 batch_loss: 0.2965\n",
            "Epochs: 7 batch_loss: 0.3285\n",
            "Epochs: 7 batch_loss: 0.2333\n",
            "Epochs: 7 batch_loss: 0.0584\n",
            "Epochs: 7 batch_loss: 0.4087\n",
            "Epochs: 7 batch_loss: 0.1870\n",
            "Epochs: 7 batch_loss: 0.2692\n",
            "Epochs: 7 batch_loss: 0.2212\n",
            "Epochs: 7 batch_loss: 0.2617\n",
            "Epochs: 7 batch_loss: 0.3944\n",
            "Epochs: 7 batch_loss: 0.2051\n",
            "Epochs: 7 batch_loss: 0.1579\n",
            "Epochs: 7 batch_loss: 0.3631\n",
            "Epochs: 7 batch_loss: 0.4662\n",
            "Epochs: 7 batch_loss: 0.3407\n",
            "Epochs: 7 batch_loss: 0.2423\n",
            "Epochs: 7 batch_loss: 0.4138\n",
            "Epochs: 7 batch_loss: 0.2301\n",
            "Epochs: 7 batch_loss: 0.1818\n",
            "Epochs: 7 batch_loss: 0.2695\n",
            "Epochs: 7 batch_loss: 0.1479\n",
            "Epochs: 7 batch_loss: 0.2044\n",
            "Epochs: 7 batch_loss: 0.6000\n",
            "Epochs: 7 batch_loss: 0.4562\n",
            "Epochs: 7 batch_loss: 0.2435\n",
            "Epochs: 7 batch_loss: 0.3826\n",
            "Epochs: 7 batch_loss: 0.2552\n",
            "Epochs: 7 batch_loss: 0.2808\n",
            "Epochs: 7 batch_loss: 0.2395\n",
            "Epochs: 7 batch_loss: 0.3306\n",
            "Epochs: 7 batch_loss: 0.3268\n",
            "Epochs: 7 batch_loss: 0.1883\n",
            "Epochs: 7 batch_loss: 0.4443\n",
            "Epochs: 7 batch_loss: 0.4832\n",
            "Epochs: 7 batch_loss: 0.2684\n",
            "Epochs: 7 batch_loss: 0.2002\n",
            "Epochs: 7 batch_loss: 0.3316\n",
            "Epochs: 7 batch_loss: 0.2323\n",
            "Epochs: 7 batch_loss: 0.1261\n",
            "Epochs: 7 batch_loss: 0.3057\n",
            "Epochs: 7 batch_loss: 0.3296\n",
            "Epochs: 7 batch_loss: 0.5057\n",
            "Epochs: 7 batch_loss: 0.2843\n",
            "Epochs: 7 batch_loss: 0.3212\n",
            "Epochs: 7 batch_loss: 0.2625\n",
            "Epochs: 7 batch_loss: 0.2872\n",
            "Epochs: 7 batch_loss: 0.1966\n",
            "Epochs: 7 batch_loss: 0.5768\n",
            "Epochs: 7 batch_loss: 0.4405\n",
            "Epochs: 7 batch_loss: 0.2923\n",
            "Epochs: 7 batch_loss: 0.2561\n",
            "Epochs: 7 batch_loss: 0.1585\n",
            "Epochs: 7 batch_loss: 0.3258\n",
            "Epochs: 7 batch_loss: 0.4287\n",
            "Epochs: 7 batch_loss: 0.2441\n",
            "Epochs: 7 batch_loss: 0.3842\n",
            "Epochs: 7 batch_loss: 0.1460\n",
            "Epochs: 7 batch_loss: 0.1846\n",
            "Epochs: 7 batch_loss: 0.3517\n",
            "Epochs: 7 batch_loss: 0.3408\n",
            "Epochs: 7 batch_loss: 0.3697\n",
            "Epochs: 7 batch_loss: 0.4146\n",
            "Epochs: 7 batch_loss: 0.2394\n",
            "Epochs: 7 batch_loss: 0.1686\n",
            "Epochs: 7 batch_loss: 0.1442\n",
            "Epochs: 7 batch_loss: 0.4917\n",
            "Epochs: 7 batch_loss: 0.2175\n",
            "Epochs: 7 batch_loss: 0.2513\n",
            "Epochs: 7 batch_loss: 0.3916\n",
            "Epochs: 7 batch_loss: 0.4123\n",
            "Epochs: 7 batch_loss: 0.2004\n",
            "Epochs: 7 batch_loss: 0.3853\n",
            "Epochs: 7 batch_loss: 0.4439\n",
            "Epochs: 7 batch_loss: 0.3495\n",
            "Epochs: 7 batch_loss: 0.2805\n",
            "Epochs: 7 batch_loss: 0.2261\n",
            "Epochs: 8 batch_loss: 0.2080\n",
            "Epochs: 8 batch_loss: 0.1591\n",
            "Epochs: 8 batch_loss: 0.2142\n",
            "Epochs: 8 batch_loss: 0.1838\n",
            "Epochs: 8 batch_loss: 0.2575\n",
            "Epochs: 8 batch_loss: 0.2240\n",
            "Epochs: 8 batch_loss: 0.1726\n",
            "Epochs: 8 batch_loss: 0.2049\n",
            "Epochs: 8 batch_loss: 0.1181\n",
            "Epochs: 8 batch_loss: 0.2062\n",
            "Epochs: 8 batch_loss: 0.3042\n",
            "Epochs: 8 batch_loss: 0.2393\n",
            "Epochs: 8 batch_loss: 0.1014\n",
            "Epochs: 8 batch_loss: 0.1590\n",
            "Epochs: 8 batch_loss: 0.1454\n",
            "Epochs: 8 batch_loss: 0.1550\n",
            "Epochs: 8 batch_loss: 0.3725\n",
            "Epochs: 8 batch_loss: 0.2424\n",
            "Epochs: 8 batch_loss: 0.0815\n",
            "Epochs: 8 batch_loss: 0.1612\n",
            "Epochs: 8 batch_loss: 0.3857\n",
            "Epochs: 8 batch_loss: 0.2236\n",
            "Epochs: 8 batch_loss: 0.1580\n",
            "Epochs: 8 batch_loss: 0.1743\n",
            "Epochs: 8 batch_loss: 0.3433\n",
            "Epochs: 8 batch_loss: 0.2100\n",
            "Epochs: 8 batch_loss: 0.1493\n",
            "Epochs: 8 batch_loss: 0.2946\n",
            "Epochs: 8 batch_loss: 0.3301\n",
            "Epochs: 8 batch_loss: 0.2071\n",
            "Epochs: 8 batch_loss: 0.2475\n",
            "Epochs: 8 batch_loss: 0.1858\n",
            "Epochs: 8 batch_loss: 0.1957\n",
            "Epochs: 8 batch_loss: 0.2111\n",
            "Epochs: 8 batch_loss: 0.2152\n",
            "Epochs: 8 batch_loss: 0.2174\n",
            "Epochs: 8 batch_loss: 0.1984\n",
            "Epochs: 8 batch_loss: 0.1874\n",
            "Epochs: 8 batch_loss: 0.1917\n",
            "Epochs: 8 batch_loss: 0.2622\n",
            "Epochs: 8 batch_loss: 0.3130\n",
            "Epochs: 8 batch_loss: 0.1272\n",
            "Epochs: 8 batch_loss: 0.1094\n",
            "Epochs: 8 batch_loss: 0.1957\n",
            "Epochs: 8 batch_loss: 0.1691\n",
            "Epochs: 8 batch_loss: 0.1463\n",
            "Epochs: 8 batch_loss: 0.1583\n",
            "Epochs: 8 batch_loss: 0.2968\n",
            "Epochs: 8 batch_loss: 0.2802\n",
            "Epochs: 8 batch_loss: 0.2381\n",
            "Epochs: 8 batch_loss: 0.3417\n",
            "Epochs: 8 batch_loss: 0.2091\n",
            "Epochs: 8 batch_loss: 0.3609\n",
            "Epochs: 8 batch_loss: 0.4636\n",
            "Epochs: 8 batch_loss: 0.2659\n",
            "Epochs: 8 batch_loss: 0.1826\n",
            "Epochs: 8 batch_loss: 0.3746\n",
            "Epochs: 8 batch_loss: 0.1577\n",
            "Epochs: 8 batch_loss: 0.2352\n",
            "Epochs: 8 batch_loss: 0.1893\n",
            "Epochs: 8 batch_loss: 0.3030\n",
            "Epochs: 8 batch_loss: 0.2614\n",
            "Epochs: 8 batch_loss: 0.2573\n",
            "Epochs: 8 batch_loss: 0.2671\n",
            "Epochs: 8 batch_loss: 0.1097\n",
            "Epochs: 8 batch_loss: 0.3101\n",
            "Epochs: 8 batch_loss: 0.3680\n",
            "Epochs: 8 batch_loss: 0.3152\n",
            "Epochs: 8 batch_loss: 0.1646\n",
            "Epochs: 8 batch_loss: 0.2379\n",
            "Epochs: 8 batch_loss: 0.2106\n",
            "Epochs: 8 batch_loss: 0.1433\n",
            "Epochs: 8 batch_loss: 0.1667\n",
            "Epochs: 8 batch_loss: 0.4669\n",
            "Epochs: 8 batch_loss: 0.2377\n",
            "Epochs: 8 batch_loss: 0.2246\n",
            "Epochs: 8 batch_loss: 0.2480\n",
            "Epochs: 8 batch_loss: 0.1416\n",
            "Epochs: 8 batch_loss: 0.1471\n",
            "Epochs: 8 batch_loss: 0.3256\n",
            "Epochs: 8 batch_loss: 0.2285\n",
            "Epochs: 8 batch_loss: 0.1878\n",
            "Epochs: 8 batch_loss: 0.2423\n",
            "Epochs: 8 batch_loss: 0.2318\n",
            "Epochs: 8 batch_loss: 0.2375\n",
            "Epochs: 8 batch_loss: 0.3408\n",
            "Epochs: 8 batch_loss: 0.1235\n",
            "Epochs: 8 batch_loss: 0.3793\n",
            "Epochs: 8 batch_loss: 0.2627\n",
            "Epochs: 8 batch_loss: 0.2660\n",
            "Epochs: 8 batch_loss: 0.3776\n",
            "Epochs: 8 batch_loss: 0.1487\n",
            "Epochs: 8 batch_loss: 0.2061\n",
            "Epochs: 8 batch_loss: 0.3206\n",
            "Epochs: 8 batch_loss: 0.1820\n",
            "Epochs: 8 batch_loss: 0.2885\n",
            "Epochs: 8 batch_loss: 0.3760\n",
            "Epochs: 8 batch_loss: 0.3126\n",
            "Epochs: 8 batch_loss: 0.2141\n",
            "Epochs: 8 batch_loss: 0.2457\n",
            "Epochs: 8 batch_loss: 0.3735\n",
            "Epochs: 8 batch_loss: 0.1789\n",
            "Epochs: 8 batch_loss: 0.1752\n",
            "Epochs: 8 batch_loss: 0.1234\n",
            "Epochs: 8 batch_loss: 0.2449\n",
            "Epochs: 8 batch_loss: 0.3376\n",
            "Epochs: 8 batch_loss: 0.2398\n",
            "Epochs: 8 batch_loss: 0.3147\n",
            "Epochs: 8 batch_loss: 0.1584\n",
            "Epochs: 8 batch_loss: 0.1386\n",
            "Epochs: 8 batch_loss: 0.2056\n",
            "Epochs: 8 batch_loss: 0.3163\n",
            "Epochs: 8 batch_loss: 0.4543\n",
            "Epochs: 8 batch_loss: 0.3453\n",
            "Epochs: 8 batch_loss: 0.1753\n",
            "Epochs: 8 batch_loss: 0.2803\n",
            "Epochs: 8 batch_loss: 0.1929\n",
            "Epochs: 8 batch_loss: 0.3626\n",
            "Epochs: 8 batch_loss: 0.2837\n",
            "Epochs: 8 batch_loss: 0.2620\n",
            "Epochs: 8 batch_loss: 0.2581\n",
            "Epochs: 8 batch_loss: 0.2062\n",
            "Epochs: 8 batch_loss: 0.4252\n",
            "Epochs: 8 batch_loss: 0.2075\n",
            "Epochs: 8 batch_loss: 0.2917\n",
            "Epochs: 8 batch_loss: 0.1741\n",
            "Epochs: 8 batch_loss: 0.2796\n",
            "Epochs: 8 batch_loss: 0.2457\n",
            "Epochs: 8 batch_loss: 0.2821\n",
            "Epochs: 8 batch_loss: 0.2504\n",
            "Epochs: 8 batch_loss: 0.2631\n",
            "Epochs: 8 batch_loss: 0.3051\n",
            "Epochs: 8 batch_loss: 0.3144\n",
            "Epochs: 8 batch_loss: 0.3373\n",
            "Epochs: 8 batch_loss: 0.1589\n",
            "Epochs: 8 batch_loss: 0.2671\n",
            "Epochs: 8 batch_loss: 0.3248\n",
            "Epochs: 8 batch_loss: 0.2349\n",
            "Epochs: 8 batch_loss: 0.3432\n",
            "Epochs: 8 batch_loss: 0.3433\n",
            "Epochs: 8 batch_loss: 0.2433\n",
            "Epochs: 8 batch_loss: 0.2259\n",
            "Epochs: 8 batch_loss: 0.2515\n",
            "Epochs: 8 batch_loss: 0.2462\n",
            "Epochs: 8 batch_loss: 0.2429\n",
            "Epochs: 8 batch_loss: 0.2409\n",
            "Epochs: 8 batch_loss: 0.3272\n",
            "Epochs: 8 batch_loss: 0.1194\n",
            "Epochs: 8 batch_loss: 0.3426\n",
            "Epochs: 8 batch_loss: 0.1957\n",
            "Epochs: 8 batch_loss: 0.4685\n",
            "Epochs: 8 batch_loss: 0.2378\n",
            "Epochs: 8 batch_loss: 0.2711\n",
            "Epochs: 8 batch_loss: 0.2904\n",
            "Epochs: 8 batch_loss: 0.2912\n",
            "Epochs: 8/10 total_loss: 0.2431\n",
            "Epochs: 9 batch_loss: 0.3547\n",
            "Epochs: 9 batch_loss: 0.1398\n",
            "Epochs: 9 batch_loss: 0.1738\n",
            "Epochs: 9 batch_loss: 0.1363\n",
            "Epochs: 9 batch_loss: 0.2514\n",
            "Epochs: 9 batch_loss: 0.2666\n",
            "Epochs: 9 batch_loss: 0.1593\n",
            "Epochs: 9 batch_loss: 0.1387\n",
            "Epochs: 9 batch_loss: 0.2994\n",
            "Epochs: 9 batch_loss: 0.1500\n",
            "Epochs: 9 batch_loss: 0.2130\n",
            "Epochs: 9 batch_loss: 0.3108\n",
            "Epochs: 9 batch_loss: 0.2208\n",
            "Epochs: 9 batch_loss: 0.1505\n",
            "Epochs: 9 batch_loss: 0.2567\n",
            "Epochs: 9 batch_loss: 0.1695\n",
            "Epochs: 9 batch_loss: 0.1687\n",
            "Epochs: 9 batch_loss: 0.1486\n",
            "Epochs: 9 batch_loss: 0.1879\n",
            "Epochs: 9 batch_loss: 0.1999\n",
            "Epochs: 9 batch_loss: 0.1833\n",
            "Epochs: 9 batch_loss: 0.1172\n",
            "Epochs: 9 batch_loss: 0.4272\n",
            "Epochs: 9 batch_loss: 0.2994\n",
            "Epochs: 9 batch_loss: 0.1954\n",
            "Epochs: 9 batch_loss: 0.2178\n",
            "Epochs: 9 batch_loss: 0.3554\n",
            "Epochs: 9 batch_loss: 0.1485\n",
            "Epochs: 9 batch_loss: 0.1242\n",
            "Epochs: 9 batch_loss: 0.1145\n",
            "Epochs: 9 batch_loss: 0.0918\n",
            "Epochs: 9 batch_loss: 0.1469\n",
            "Epochs: 9 batch_loss: 0.2672\n",
            "Epochs: 9 batch_loss: 0.1636\n",
            "Epochs: 9 batch_loss: 0.1873\n",
            "Epochs: 9 batch_loss: 0.1351\n",
            "Epochs: 9 batch_loss: 0.3160\n",
            "Epochs: 9 batch_loss: 0.2482\n",
            "Epochs: 9 batch_loss: 0.1157\n",
            "Epochs: 9 batch_loss: 0.1346\n",
            "Epochs: 9 batch_loss: 0.1523\n",
            "Epochs: 9 batch_loss: 0.1403\n",
            "Epochs: 9 batch_loss: 0.2786\n",
            "Epochs: 9 batch_loss: 0.3182\n",
            "Epochs: 9 batch_loss: 0.1322\n",
            "Epochs: 9 batch_loss: 0.1606\n",
            "Epochs: 9 batch_loss: 0.1139\n",
            "Epochs: 9 batch_loss: 0.1646\n",
            "Epochs: 9 batch_loss: 0.2676\n",
            "Epochs: 9 batch_loss: 0.2205\n",
            "Epochs: 9 batch_loss: 0.1649\n",
            "Epochs: 9 batch_loss: 0.2837\n",
            "Epochs: 9 batch_loss: 0.1780\n",
            "Epochs: 9 batch_loss: 0.2136\n",
            "Epochs: 9 batch_loss: 0.1873\n",
            "Epochs: 9 batch_loss: 0.1292\n",
            "Epochs: 9 batch_loss: 0.0928\n",
            "Epochs: 9 batch_loss: 0.1099\n",
            "Epochs: 9 batch_loss: 0.1446\n",
            "Epochs: 9 batch_loss: 0.4684\n",
            "Epochs: 9 batch_loss: 0.2409\n",
            "Epochs: 9 batch_loss: 0.1506\n",
            "Epochs: 9 batch_loss: 0.2889\n",
            "Epochs: 9 batch_loss: 0.1516\n",
            "Epochs: 9 batch_loss: 0.1874\n",
            "Epochs: 9 batch_loss: 0.2609\n",
            "Epochs: 9 batch_loss: 0.3219\n",
            "Epochs: 9 batch_loss: 0.2287\n",
            "Epochs: 9 batch_loss: 0.2505\n",
            "Epochs: 9 batch_loss: 0.2142\n",
            "Epochs: 9 batch_loss: 0.3142\n",
            "Epochs: 9 batch_loss: 0.2811\n",
            "Epochs: 9 batch_loss: 0.2922\n",
            "Epochs: 9 batch_loss: 0.2848\n",
            "Epochs: 9 batch_loss: 0.1641\n",
            "Epochs: 9 batch_loss: 0.2123\n",
            "Epochs: 9 batch_loss: 0.2393\n",
            "Epochs: 9 batch_loss: 0.4063\n",
            "Epochs: 9 batch_loss: 0.1562\n",
            "Epochs: 9 batch_loss: 0.3368\n",
            "Epochs: 9 batch_loss: 0.2587\n",
            "Epochs: 9 batch_loss: 0.4518\n",
            "Epochs: 9 batch_loss: 0.2933\n",
            "Epochs: 9 batch_loss: 0.1742\n",
            "Epochs: 9 batch_loss: 0.2518\n",
            "Epochs: 9 batch_loss: 0.2295\n",
            "Epochs: 9 batch_loss: 0.2462\n",
            "Epochs: 9 batch_loss: 0.2812\n",
            "Epochs: 9 batch_loss: 0.3541\n",
            "Epochs: 9 batch_loss: 0.3343\n",
            "Epochs: 9 batch_loss: 0.2164\n",
            "Epochs: 9 batch_loss: 0.2186\n",
            "Epochs: 9 batch_loss: 0.2371\n",
            "Epochs: 9 batch_loss: 0.1764\n",
            "Epochs: 9 batch_loss: 0.1643\n",
            "Epochs: 9 batch_loss: 0.3686\n",
            "Epochs: 9 batch_loss: 0.1884\n",
            "Epochs: 9 batch_loss: 0.3683\n",
            "Epochs: 9 batch_loss: 0.2764\n",
            "Epochs: 9 batch_loss: 0.2904\n",
            "Epochs: 9 batch_loss: 0.3212\n",
            "Epochs: 9 batch_loss: 0.2698\n",
            "Epochs: 9 batch_loss: 0.2112\n",
            "Epochs: 9 batch_loss: 0.2012\n",
            "Epochs: 9 batch_loss: 0.3034\n",
            "Epochs: 9 batch_loss: 0.3245\n",
            "Epochs: 9 batch_loss: 0.1517\n",
            "Epochs: 9 batch_loss: 0.3935\n",
            "Epochs: 9 batch_loss: 0.1469\n",
            "Epochs: 9 batch_loss: 0.3004\n",
            "Epochs: 9 batch_loss: 0.2396\n",
            "Epochs: 9 batch_loss: 0.3124\n",
            "Epochs: 9 batch_loss: 0.2267\n",
            "Epochs: 9 batch_loss: 0.3860\n",
            "Epochs: 9 batch_loss: 0.3227\n",
            "Epochs: 9 batch_loss: 0.1940\n",
            "Epochs: 9 batch_loss: 0.1794\n",
            "Epochs: 9 batch_loss: 0.3631\n",
            "Epochs: 9 batch_loss: 0.3402\n",
            "Epochs: 9 batch_loss: 0.2482\n",
            "Epochs: 9 batch_loss: 0.2107\n",
            "Epochs: 9 batch_loss: 0.2600\n",
            "Epochs: 9 batch_loss: 0.3398\n",
            "Epochs: 9 batch_loss: 0.3283\n",
            "Epochs: 9 batch_loss: 0.1387\n",
            "Epochs: 9 batch_loss: 0.2668\n",
            "Epochs: 9 batch_loss: 0.2328\n",
            "Epochs: 9 batch_loss: 0.4165\n",
            "Epochs: 9 batch_loss: 0.2265\n",
            "Epochs: 9 batch_loss: 0.1993\n",
            "Epochs: 9 batch_loss: 0.3533\n",
            "Epochs: 9 batch_loss: 0.3696\n",
            "Epochs: 9 batch_loss: 0.2413\n",
            "Epochs: 9 batch_loss: 0.2044\n",
            "Epochs: 9 batch_loss: 0.1733\n",
            "Epochs: 9 batch_loss: 0.1531\n",
            "Epochs: 9 batch_loss: 0.2435\n",
            "Epochs: 9 batch_loss: 0.2302\n",
            "Epochs: 9 batch_loss: 0.1878\n",
            "Epochs: 9 batch_loss: 0.2172\n",
            "Epochs: 9 batch_loss: 0.4160\n",
            "Epochs: 9 batch_loss: 0.1584\n",
            "Epochs: 9 batch_loss: 0.3678\n",
            "Epochs: 9 batch_loss: 0.3030\n",
            "Epochs: 9 batch_loss: 0.2509\n",
            "Epochs: 9 batch_loss: 0.1847\n",
            "Epochs: 9 batch_loss: 0.2112\n",
            "Epochs: 9 batch_loss: 0.2862\n",
            "Epochs: 9 batch_loss: 0.2193\n",
            "Epochs: 9 batch_loss: 0.2310\n",
            "Epochs: 9 batch_loss: 0.2771\n",
            "Epochs: 9 batch_loss: 0.3108\n",
            "Epochs: 9 batch_loss: 0.3319\n",
            "Epochs: 9 batch_loss: 0.2375\n",
            "Epochs: 9 batch_loss: 0.2395\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjVwV_waXH0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to reload wights\n",
        "\n",
        "# encoder.load_weights('/content/gdrive/My Drive/encoder.h5')\n",
        "# decoder.load_weights('/content/gdrive/My Drive/decoder.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfB2kwYLx4Fe",
        "colab_type": "text"
      },
      "source": [
        "In order to perform the translation, we need to write a function much like what we did in the train_step function but instead of feeding in the actual word at a prior time step into the next time step, we feed in the word predicted by our network. This algorithm is known as Greedy search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuswAWMYqdME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence, max_length=10):\n",
        "    result = ''\n",
        "    attention_plot = np.zeros((10,10))\n",
        "    sentence = normalizeString(sentence)\n",
        "    sentence = sentencetoIndexes(sentence, input_lang)\n",
        "    sentence = keras.preprocessing.sequence.pad_sequences([sentence],padding='post',\n",
        "                                                      maxlen=max_length, truncating='post')\n",
        "    \n",
        "    encoder_hidden = hidden = [tf.zeros((1, 256))]\n",
        "    \n",
        "    enc_out, enc_hidden = encoder(sentence, encoder_hidden)\n",
        "    \n",
        "    dec_hidden = enc_hidden\n",
        "    SOS_tensor = np.array([SOS_token])\n",
        "    dec_input = tf.squeeze(tf.expand_dims([SOS_tensor], 1), -1)\n",
        "    \n",
        "    for tx in range(max_length):\n",
        "        dec_out, dec_hidden, attn_weights = decoder(dec_input,\n",
        "                                                   dec_hidden, enc_out)\n",
        "        attn_weights = tf.reshape(attn_weights, (-1, ))\n",
        "        attention_plot[tx] = attn_weights.numpy()\n",
        "        pred = tf.argmax(dec_out, axis=1).numpy()\n",
        "        result += output_lang.int2word[pred[0]] + \" \"\n",
        "        if output_lang.int2word[pred[0]] == \"EOS\":\n",
        "            break\n",
        "        dec_input = tf.expand_dims(pred, axis=1)\n",
        "    return result, attention_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7mmzYUIqdMI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1e142863-3bc9-4451-d659-b844a5ca19a1"
      },
      "source": [
        "sentence = \"j'ai besoin de quelqu'un pour m'aider ?\"\n",
        "pred, attn_weights = translate(sentence)\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i need somebody to help me . EOS \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYBWcIpCWOKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence = \"ou vas tu ?\"\n",
        "pred, attn_weights = translate(sentence)\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efRbcnkXWaA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence= \"quand tu vas au marche ?\"\n",
        "pred, attn_weights = translate(sentence)\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s9cnSvUxNIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    sentence = normalizeString(sentence)\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9T2GrGdSZ1z3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yROfA7newSua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder.load_weights('/content/gdrive/My Drive/encoder.h5')\n",
        "decoder.load_weights('/content/gdrive/My Drive/decoder.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}